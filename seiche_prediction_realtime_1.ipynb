{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebookbfffb4db2a",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c00c1ebe5754d399fcd21fc700cb947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_607e74f515e1446d889c6de334f94068",
              "IPY_MODEL_cad9d60f06824fd8b678e9f69a19fc6f",
              "IPY_MODEL_784dec59ef5b4859afdf42845b4fce96"
            ],
            "layout": "IPY_MODEL_f5d6c135e0dc4f0b9be0d897f7fede83"
          }
        },
        "607e74f515e1446d889c6de334f94068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2467e95fcd174d5c809a1bba6ab803d1",
            "placeholder": "​",
            "style": "IPY_MODEL_863a8033f90c49b3b88fb2cb2d33db5b",
            "value": "STDMET years:  28%"
          }
        },
        "cad9d60f06824fd8b678e9f69a19fc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fbefb43c8b4a63acf2a2274164e0d2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a53cfce0e3f5472284b3ab207a4bba97",
            "value": 5
          }
        },
        "784dec59ef5b4859afdf42845b4fce96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e343d0787d4f6c968556740c62f1b3",
            "placeholder": "​",
            "style": "IPY_MODEL_c8547b3f555747ff86175661af305788",
            "value": " 5/18 [00:05&lt;00:13,  1.05s/yr]"
          }
        },
        "f5d6c135e0dc4f0b9be0d897f7fede83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2467e95fcd174d5c809a1bba6ab803d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "863a8033f90c49b3b88fb2cb2d33db5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78fbefb43c8b4a63acf2a2274164e0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53cfce0e3f5472284b3ab207a4bba97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55e343d0787d4f6c968556740c62f1b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8547b3f555747ff86175661af305788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaysmerrill/Yaquina_Bay_Seiching/blob/main/seiche_prediction_realtime_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# Seiche Pipeline — Full Rebuild (with all requested updates)\n",
        "# ============================================================\n",
        "import io, gzip, requests, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "from contextlib import nullcontext\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Progress bars\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    def tqdm(x, *a, **k):  # fallback if tqdm unavailable\n",
        "        return x\n",
        "\n",
        "# -------------------\n",
        "# USER KNOBS\n",
        "# -------------------\n",
        "YEARS               = list(range(2008, 2026))   # swden/stdmet years\n",
        "DT_MINUTES          = 6                         # base grid step (minutes)\n",
        "\n",
        "TFT_TARGET_LOG1P = True   # train TFT on log1p(var1h); invert with expm1 for outputs\n",
        "\n",
        "# --- Baro & XGB selection controls ---\n",
        "BARO_FORCE_IN_XGB   = True            # always include baro features (best lag)\n",
        "BARO_FORCE_LAGS_MIN = [0, 60, 120, 240]  # minutes to try for baro/hpa & baro_var\n",
        "BARO_SPIKE_Q        = 0.90            # spike threshold quantile in transformed target space\n",
        "BARO_SPIKE_LOSS_W   = 2.0             # extra loss weight when baro_var is spiking\n",
        "\n",
        "# ---- Response processing knobs ----\n",
        "VAR_WIN_HOURS       = 2.0                       # centered rolling-variance window (hours)\n",
        "BANDPASS_MIN_MIN    = 20.0                      # lower period (minutes)\n",
        "BANDPASS_MAX_MIN    = 30.0                      # upper period (minutes)\n",
        "BP_ORDER            = 6                         # Butterworth order\n",
        "\n",
        "MAX_GAP_HOURS       = 5\n",
        "USE_UNIFIED_GAP_MASK= False\n",
        "\n",
        "# ---- Windowing / horizons ----\n",
        "HISTORY_HOURS       = 12\n",
        "HORIZONS_MIN        = [60, 120, 240]\n",
        "PRIMARY_HORIZON     = 60\n",
        "\n",
        "# If you already loaded df_wl6 yourself (with columns ['time','wl']), set False.\n",
        "WL_USE_FETCH        = True\n",
        "\n",
        "# NOAA wind (CO-OPS) fetch window (chunked)\n",
        "NOAA_BEGIN_DATE     = \"20080101\"\n",
        "NOAA_END_DATE       = \"20251001\"\n",
        "NOAA_WIND_STATION   = \"9435380\"   # Newport / South Beach\n",
        "\n",
        "# Water level (response) station and date range — default to 9435380\n",
        "NOAA_WL_STATION     = \"9435380\"\n",
        "WL_BEGIN_DATE       = \"20080101\"\n",
        "WL_END_DATE         = \"20251001\"\n",
        "NOAA_CHUNK_DAYS     = 365\n",
        "\n",
        "# HMSC monthly archive (backfill for wind & baro)\n",
        "HMSC_BASE_URL       = \"http://weather.hmsc.oregonstate.edu/weather/weatherproject/archive/{yyyy}/HMSC_{yyyymm}.dat\"\n",
        "HMSC_UTC_OFFSET_H   = +8  # PST → UTC\n",
        "\n",
        "# ---- Lag feature caps ----\n",
        "NON_EQ_MAX_LAG_MIN  = 4 * 60\n",
        "EQ_MAX_LAG_MIN      = 24 * 60\n",
        "TOP_LAG_FEATURES    = 12\n",
        "PER_BASE_CAP        = 1\n",
        "\n",
        "# ---- Spike weighting on target ----\n",
        "WEIGHT_SPIKES       = True\n",
        "SPIKE_P90           = 0.90\n",
        "SPIKE_WEIGHT        = 2.0\n",
        "\n",
        "# ---- Time splits ----\n",
        "TRAIN_END           = pd.Timestamp(\"2018-12-31 23:59:59\")\n",
        "VAL_END             = pd.Timestamp(\"2022-12-31 23:59:59\")\n",
        "MAX_SAMPLES_SPLIT   = 60_000\n",
        "\n",
        "# ---- Plotting controls ----\n",
        "PRED_SPLIT          = \"test\"     # \"test\" or \"val\"\n",
        "PRED_START          = pd.Timestamp(\"2022-01-01 00:00:00\")\n",
        "PRED_END            = pd.Timestamp(\"2022-01-31 23:59:59\")\n",
        "PLOT_LAST_DAYS      = 40\n",
        "PLOT_TOP_INPUTS     = 6          # max number of input drivers to show per horizon\n",
        "\n",
        "# ===================\n",
        "#  MODEL KNOBS\n",
        "# ===================\n",
        "\n",
        "# XGBoost knobs (used for lag ranking / feature selection)\n",
        "XGB_N_ESTIMATORS    = 800\n",
        "XGB_LEARNING_RATE   = 0.03\n",
        "XGB_MAX_DEPTH       = 5\n",
        "XGB_SUBSAMPLE       = 0.8\n",
        "XGB_COLSAMPLE       = 0.8\n",
        "XGB_L2              = 1.0                        # reg_lambda\n",
        "XGB_VERBOSE         = 0                          # 0=quiet\n",
        "\n",
        "# Mini-TFT knobs\n",
        "TFT_D_MODEL         = 128\n",
        "TFT_NHEAD           = 4\n",
        "TFT_LSTM_LAYERS     = 1\n",
        "TFT_FF_DIM          = 256\n",
        "TFT_DROPOUT         = 0.2\n",
        "TFT_LR              = 2e-3\n",
        "TFT_EPOCHS          = 12\n",
        "TFT_PATIENCE        = 3\n",
        "TFT_BATCH           = 512\n",
        "\n",
        "# TFT output constraints (variance ≥ 0)\n",
        "TFT_NONNEG_OUTPUT   = True\n",
        "TFT_VAR_FLOOR       = 1e-8\n",
        "\n",
        "# ---- HMSC / wind weighting / baro variance knobs ----\n",
        "WIND_WEIGHT                 = 0.35      # scale applied to wind speed (0=no wind influence, 1=original)\n",
        "BARO_COL_KEYS               = [\"bp\", \"baro\", \"barometer\", \"press\", \"pressure\", \"slp\"]\n",
        "BARO_VAR_HIGHPASS_HOURS     = 4.0       # high-pass cutoff period (hours) before variance\n",
        "BARO_VAR_WIN_HOURS          = 4.0       # centered rolling variance window length (hours)\n",
        "BARO_CLIP_MIN_HPA           = 900.0\n",
        "BARO_CLIP_MAX_HPA           = 1050.0\n",
        "\n",
        "# -------------------\n",
        "# Robust HTTP session\n",
        "# -------------------\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def _make_session():\n",
        "    s = requests.Session()\n",
        "    retry = Retry(total=4, backoff_factor=0.6, status_forcelist=(429,500,502,503,504),\n",
        "                  allowed_methods=[\"HEAD\",\"GET\",\"OPTIONS\"], raise_on_status=False)\n",
        "    s.headers.update({\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) SeicheFetcher/1.0\", \"Accept\":\"*/*\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
        "    return s\n",
        "\n",
        "session = _make_session()\n",
        "\n",
        "# -------------------\n",
        "# Helpers\n",
        "# -------------------\n",
        "def _ndbc_to_float(tok:str) -> float:\n",
        "    s = str(tok).strip()\n",
        "    if s == \"\" or s.upper() == \"MM\":\n",
        "        return np.nan\n",
        "    if re.fullmatch(r\"-?9+(\\.0+)?\", s):  # 99, 9999, 999.00, etc. -> NaN\n",
        "        return np.nan\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _datestr(dt) -> str:\n",
        "    return pd.Timestamp(dt).strftime(\"%Y%m%d\")\n",
        "\n",
        "# -------------------\n",
        "# NDBC 46050 STDMET (Hs/DPD/APD/MWD)\n",
        "# -------------------\n",
        "STDMET_DIRECT_PAT = \"https://www.ndbc.noaa.gov/data/historical/stdmet/46050h{year}.txt.gz\"\n",
        "STDMET_PHP_PAT    = \"https://www.ndbc.noaa.gov/view_text_file.php?filename=46050h{year}.txt.gz&dir=data/historical/stdmet/\"\n",
        "\n",
        "def fetch_stdmet_year(year:int)->str|None:\n",
        "    for url in (STDMET_DIRECT_PAT.format(year=year), STDMET_PHP_PAT.format(year=year)):\n",
        "        try:\n",
        "            r = session.get(url, timeout=30)\n",
        "            if r.status_code != 200 or not r.content: continue\n",
        "            data = r.content\n",
        "            try:\n",
        "                with gzip.GzipFile(fileobj=io.BytesIO(data)) as gz:\n",
        "                    return gz.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "            except OSError:\n",
        "                try:\n",
        "                    return data.decode(\"utf-8\", errors=\"ignore\")\n",
        "                except Exception:\n",
        "                    continue\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def parse_stdmet_text(txt:str)->pd.DataFrame:\n",
        "    rows=[]\n",
        "    for ln in txt.splitlines():\n",
        "        if not ln or ln.lstrip().startswith(\"#\"): continue\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 6: continue\n",
        "        y,mo,dy,hh,mi = parts[:5]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh); mi=int(mi)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, minute=mi, tz=\"UTC\").tz_localize(None)\n",
        "        except Exception:\n",
        "            continue\n",
        "        vals = parts[5:]\n",
        "        if len(vals) < 13: vals = (vals + [\"MM\"]*13)[:13]\n",
        "        vals = [_ndbc_to_float(v) for v in vals]\n",
        "        rows.append([ts] + vals)\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "    cols = [\"time\",\"WDIR\",\"WSPD\",\"GST\",\"WVHT\",\"DPD\",\"APD\",\"MWD\",\"PRES\",\"ATMP\",\"WTMP\",\"DEWP\",\"VIS\",\"TIDE\"]\n",
        "    df = pd.DataFrame(rows, columns=cols).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    df = df.rename(columns={\"WVHT\":\"Hs\"}).reset_index(drop=True)\n",
        "    for c,thr in [(\"Hs\",50.0),(\"DPD\",50.0),(\"APD\",50.0)]:\n",
        "        if c in df.columns:\n",
        "            m = df[c] > thr\n",
        "            if m.any(): df.loc[m,c]=np.nan\n",
        "    if \"MWD\" in df.columns:\n",
        "        df.loc[(df[\"MWD\"]<0)|(df[\"MWD\"]>360),\"MWD\"]=np.nan\n",
        "    keep = [\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"]\n",
        "    for k in keep:\n",
        "        if k not in df.columns: df[k]=np.nan\n",
        "    return df[keep]\n",
        "\n",
        "def build_stdmet(years: List[int]) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for y in tqdm(years, desc=\"STDMET years\", unit=\"yr\"):\n",
        "        txt = fetch_stdmet_year(y)\n",
        "        if not txt: continue\n",
        "        dfy = parse_stdmet_text(txt)\n",
        "        if dfy.empty: continue\n",
        "        frames.append(dfy)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "df_stdmet = build_stdmet(YEARS)\n",
        "\n",
        "# -------------------\n",
        "# NDBC 46050 SWDEN → 3-band Hm0\n",
        "# -------------------\n",
        "SWDEN_PHP_PATS = [\n",
        "    \"https://www.ndbc.noaa.gov/view_text_file.php?filename=46050w{year}.txt.gz&dir=data/historical/swden/\",\n",
        "    \"https://www.ndbc.noaa.gov/view_text_file.php?filename=46050w{year}.txt.gz&dir=data/swden/46050/\",\n",
        "]\n",
        "\n",
        "def fetch_swden_year(year:int)->str|None:\n",
        "    for url in SWDEN_PHP_PATS:\n",
        "        try:\n",
        "            r = session.get(url.format(year=year), timeout=40)\n",
        "            if r.status_code==200 and r.text and \"Page Not Found\" not in r.text:\n",
        "                return r.text\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def split_swden_blocks(lines:List[str]):\n",
        "    header=[]; i=0\n",
        "    while i<len(lines) and lines[i].startswith(\"#\"):\n",
        "        header.append(lines[i]); i+=1\n",
        "    while i<len(lines) and not lines[i].strip():\n",
        "        i+=1\n",
        "    freq_line = lines[i].rstrip(\"\\n\") if i<len(lines) else None\n",
        "    i += 1 if i<len(lines) else 0\n",
        "    data=[ln for ln in lines[i:] if ln.strip()]\n",
        "    return header, freq_line, data\n",
        "\n",
        "def parse_swden_year(txt:str)->pd.DataFrame:\n",
        "    lines = txt.splitlines()\n",
        "    _, freq_line, data_lines = split_swden_blocks(lines)\n",
        "    if not freq_line: return pd.DataFrame()\n",
        "    freq_tokens = [t for t in re.split(r\"\\s+\", freq_line.strip()) if t]\n",
        "    if any(k in freq_tokens[:6] for k in (\"YY\",\"MM\",\"DD\",\"hh\")):\n",
        "        idx = lines.index(freq_line)\n",
        "        j=idx+1\n",
        "        while j<len(lines) and not lines[j].strip():\n",
        "            j+=1\n",
        "        if j<len(lines):\n",
        "            freq_tokens = [t for t in re.split(r\"\\s+\", lines[j].strip()) if t]\n",
        "            data_lines  = [ln for ln in lines[j+1:] if ln.strip()]\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _num(tok):\n",
        "        try: return float(tok)\n",
        "        except: return np.nan\n",
        "    freqs = np.array([_num(t) for t in freq_tokens], dtype=float)\n",
        "    freqs = freqs[np.isfinite(freqs)]\n",
        "    if freqs.size==0: return pd.DataFrame()\n",
        "\n",
        "    dfw = np.empty_like(freqs)\n",
        "    if freqs.size==1:\n",
        "        dfw[:] = 0.0\n",
        "    else:\n",
        "        dfw[1:-1] = 0.5*(freqs[2:] - freqs[:-2])\n",
        "        dfw[0]    = freqs[1] - freqs[0]\n",
        "        dfw[-1]   = freqs[-1] - freqs[-2]\n",
        "        dfw = np.where(dfw<0, np.nan, dfw)\n",
        "\n",
        "    mask_hig   = (freqs < 0.05)\n",
        "    mask_swell = (freqs >= 0.05) & (freqs <= 0.14)\n",
        "    mask_sea   = (freqs > 0.14) & (freqs <= 0.30)\n",
        "\n",
        "    rows=[]\n",
        "    for ln in data_lines:\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 4: continue\n",
        "        y,mo,dy,hh = parts[:4]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, tz=\"UTC\").tz_localize(None)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        vals = np.array([_ndbc_to_float(v) for v in parts[4:]], dtype=float)\n",
        "        if vals.size != freqs.size:\n",
        "            if vals.size < freqs.size:\n",
        "                vals = np.pad(vals, (0, freqs.size - vals.size), constant_values=np.nan)\n",
        "            else:\n",
        "                vals = vals[:freqs.size]\n",
        "\n",
        "        def band_m0(mask):\n",
        "            return np.nansum(vals[mask] * dfw[mask]) if mask.any() else np.nan\n",
        "\n",
        "        m0_hig   = band_m0(mask_hig)\n",
        "        m0_swell = band_m0(mask_swell)\n",
        "        m0_sea   = band_m0(mask_sea)\n",
        "\n",
        "        high_HIG = 4.0*np.sqrt(m0_hig)   if np.isfinite(m0_hig)   and m0_hig>=0   else np.nan\n",
        "        Hswell   = 4.0*np.sqrt(m0_swell) if np.isfinite(m0_swell) and m0_swell>=0 else np.nan\n",
        "        Hsea     = 4.0*np.sqrt(m0_sea)   if np.isfinite(m0_sea)   and m0_sea>=0   else np.nan\n",
        "\n",
        "        rows.append((ts, high_HIG, Hswell, Hsea))\n",
        "\n",
        "    if not rows: return pd.DataFrame()\n",
        "    df_out = (\n",
        "        pd.DataFrame(rows, columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "        .sort_values(\"time\")\n",
        "        .drop_duplicates(\"time\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return df_out\n",
        "\n",
        "def build_swden(years: List[int]) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for y in tqdm(years, desc=\"SWDEN years\", unit=\"yr\"):\n",
        "        txt = fetch_swden_year(y)\n",
        "        if not txt: continue\n",
        "        dfy = parse_swden_year(txt)\n",
        "        if dfy.empty: continue\n",
        "        frames.append(dfy)\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "df_swden = build_swden(YEARS)\n",
        "\n",
        "# -------------------\n",
        "# NOAA CO-OPS 9435380 wind (hourly) + HMSC fallback (+ baro)\n",
        "# -------------------\n",
        "def parse_noaa_wind_csv(txt:str)->pd.DataFrame:\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    time_cols = [c for c in df.columns if str(c).strip().lower() in (\"t\",\"time\",\"date time\",\"date_time\",\"date\")]\n",
        "    time_col = time_cols[0] if time_cols else df.columns[0]\n",
        "    df[\"time\"] = pd.to_datetime(df[time_col], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    def _find(colnames, keys):\n",
        "        keys = [k.lower() for k in keys]\n",
        "        for c in colnames:\n",
        "            s = str(c).lower()\n",
        "            if any(k == s or k in s for k in keys): return c\n",
        "        return None\n",
        "    sp_col = _find(df.columns, [\"s\",\"speed\"])\n",
        "    dir_col= _find(df.columns, [\"d\",\"dir\",\"direction\"])\n",
        "    out = pd.DataFrame({\"time\":df[\"time\"]})\n",
        "    out[\"wind9435380_speed\"] = pd.to_numeric(df[sp_col], errors=\"coerce\") if sp_col else np.nan\n",
        "    out[\"wind9435380_dir\"]   = pd.to_numeric(df[dir_col], errors=\"coerce\") if dir_col else np.nan\n",
        "    out.loc[(out[\"wind9435380_dir\"]<0)|(out[\"wind9435380_dir\"]>360),\"wind9435380_dir\"]=np.nan\n",
        "    out.loc[(out[\"wind9435380_speed\"]<0)|(out[\"wind9435380_speed\"]>100),\"wind9435380_speed\"]=np.nan\n",
        "    out = out.sort_values(\"time\").dropna(subset=[\"time\"]).drop_duplicates(\"time\")\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def fetch_noaa_wind_dataframe(begin_date:str, end_date:str, station:str, days_per_chunk:int=365) -> pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\")\n",
        "    end   = pd.to_datetime(end_date,   format=\"%Y%m%d\")\n",
        "    chunks = []\n",
        "    cur = start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        chunks.append((cur, chunk_end))\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "    frames=[]\n",
        "    for a, b in tqdm(chunks, desc=f\"NOAA wind {station}\", unit=\"chunk\"):\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(a)}&end_date={_datestr(b)}\"\n",
        "               f\"&station={station}&product=wind&time_zone=gmt&interval=h&units=metric\"\n",
        "               \"&application=SeichePipeline&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=60)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wind_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"time\",\"wind9435380_speed\",\"wind9435380_dir\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def fetch_hmsc_month_text(year:int, month:int) -> str|None:\n",
        "    yyyymm = f\"{year}{month:02d}\"; url = HMSC_BASE_URL.format(yyyy=year, yyyymm=yyyymm)\n",
        "    try:\n",
        "        r = session.get(url, timeout=60)\n",
        "        if r.status_code==200 and r.content:\n",
        "            return r.content.decode(\"utf-8\",\"ignore\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def parse_hmsc_dat(txt:str) -> pd.DataFrame:\n",
        "    lines = txt.splitlines(); start_idx=None\n",
        "    for i,ln in enumerate(lines):\n",
        "        s = ln.strip().strip('\"')\n",
        "        if s.startswith(\"TIMESTAMP\"): start_idx=i; break\n",
        "    if start_idx is None: return pd.DataFrame()\n",
        "    content = \"\\n\".join(lines[start_idx:])\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "\n",
        "    def _try_time(x):\n",
        "        try: return pd.to_datetime(x)\n",
        "        except: return pd.NaT\n",
        "    tt = df.iloc[:,0].apply(_try_time)\n",
        "    df = df.loc[tt.notna()].copy()\n",
        "    df.rename(columns={df.columns[0]:\"TIMESTAMP\"}, inplace=True)\n",
        "\n",
        "    cols_lower = {str(c).strip().lower(): c for c in df.columns}\n",
        "\n",
        "    AWS_col = None; AWD_col = None\n",
        "    for k in cols_lower:\n",
        "        if k in (\"aws\",): AWS_col = cols_lower[k]\n",
        "        if k in (\"awd\",): AWD_col = cols_lower[k]\n",
        "\n",
        "    BARO_col = None\n",
        "    for key in BARO_COL_KEYS:\n",
        "        for k in cols_lower:\n",
        "            if key == k or key in k:\n",
        "                BARO_col = cols_lower[k]; break\n",
        "        if BARO_col is not None: break\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    out[\"time\"] = pd.to_datetime(df[\"TIMESTAMP\"], errors=\"coerce\") + pd.Timedelta(hours=HMSC_UTC_OFFSET_H)\n",
        "\n",
        "    if AWS_col is not None and AWD_col is not None:\n",
        "        mph = pd.to_numeric(df[AWS_col], errors=\"coerce\")\n",
        "        spd_ms = mph * 0.44704\n",
        "        dir_deg = pd.to_numeric(df[AWD_col], errors=\"coerce\")\n",
        "        dir_deg[(dir_deg<0)|(dir_deg>360)] = np.nan\n",
        "        out[\"hmsc_speed_ms\"]=spd_ms.astype(\"float32\")\n",
        "        out[\"hmsc_dir_deg\"]=dir_deg.astype(\"float32\")\n",
        "\n",
        "    if BARO_col is not None:\n",
        "        bp = pd.to_numeric(df[BARO_col], errors=\"coerce\").astype(\"float32\")\n",
        "        bp[(bp < BARO_CLIP_MIN_HPA) | (bp > BARO_CLIP_MAX_HPA)] = np.nan\n",
        "        out[\"hmsc_baro_hpa\"] = bp\n",
        "\n",
        "    out = out.sort_values(\"time\").dropna(subset=[\"time\"]).drop_duplicates(\"time\")\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def month_iter(start:pd.Timestamp, end:pd.Timestamp):\n",
        "    y,m = start.year, start.month\n",
        "    while pd.Timestamp(year=y, month=m, day=1) <= end:\n",
        "        yield (y,m)\n",
        "        if m==12: y,m=y+1,1\n",
        "        else: m+=1\n",
        "\n",
        "def fetch_hmsc_range(start:pd.Timestamp, end:pd.Timestamp)->pd.DataFrame:\n",
        "    months = list(month_iter(start.normalize(), end.normalize()))\n",
        "    frames=[]\n",
        "    for y,m in tqdm(months, desc=\"HMSC months\", unit=\"mo\"):\n",
        "        txt = fetch_hmsc_month_text(y,m)\n",
        "        if not txt: continue\n",
        "        dfm = parse_hmsc_dat(txt)\n",
        "        if not dfm.empty:\n",
        "            dfm = dfm[(dfm[\"time\"]>=start-pd.Timedelta(days=1))&(dfm[\"time\"]<=end+pd.Timedelta(days=1))]\n",
        "            frames.append(dfm)\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"time\",\"hmsc_speed_ms\",\"hmsc_dir_deg\",\"hmsc_baro_hpa\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def resample_hmsc_hourly(df_hmsc:pd.DataFrame)->pd.DataFrame:\n",
        "    if df_hmsc.empty: return df_hmsc\n",
        "    g = df_hmsc.set_index(\"time\").sort_index()\n",
        "    hourly = pd.DataFrame(index=g.index.floor(\"h\").unique())\n",
        "    for col in [\"hmsc_speed_ms\",\"hmsc_dir_deg\",\"hmsc_baro_hpa\"]:\n",
        "        if col in g.columns:\n",
        "            hourly[col] = g[col].resample(\"h\").mean()\n",
        "    out = hourly.reset_index().rename(columns={\"index\":\"time\"}).dropna(subset=[\"time\"])\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def baro_highpass_variance(df_hmsc_hour:pd.DataFrame,\n",
        "                           hp_hours:float=BARO_VAR_HIGHPASS_HOURS,\n",
        "                           var_hours:float=BARO_VAR_WIN_HOURS)->pd.DataFrame:\n",
        "    from scipy.signal import butter, filtfilt\n",
        "    if df_hmsc_hour.empty or \"hmsc_baro_hpa\" not in df_hmsc_hour.columns:\n",
        "        return pd.DataFrame(columns=[\"time\",\"baro_var4h\"])\n",
        "    df = df_hmsc_hour.dropna(subset=[\"time\",\"hmsc_baro_hpa\"]).copy().sort_values(\"time\")\n",
        "    s = df.set_index(\"time\")[\"hmsc_baro_hpa\"].astype(float)\n",
        "    fs = 1.0  # hourly\n",
        "    fc = 1.0 / max(hp_hours, 0.5)\n",
        "    wn = min(max(fc / (fs/2.0), 1e-4), 0.999)\n",
        "    b,a = butter(4, wn, btype=\"highpass\")\n",
        "    x = s.to_numpy(dtype=float)\n",
        "    if np.isnan(x).any():\n",
        "        idx = np.arange(len(x)); good = np.isfinite(x)\n",
        "        x[~good] = np.interp(idx[~good], idx[good], x[good])\n",
        "    xf = filtfilt(b, a, x, method=\"gust\")\n",
        "    win = max(3, int(round(var_hours)))\n",
        "    minp = max(2, int(0.8 * win))\n",
        "    var_series = pd.Series(xf, index=s.index).rolling(window=win, center=True, min_periods=minp).var()\n",
        "    out = pd.DataFrame({\"time\": var_series.index, \"baro_var4h\": var_series.astype(\"float32\")}).dropna(subset=[\"time\"])\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "# Build wind + baro with fill\n",
        "df_noaa_wind = fetch_noaa_wind_dataframe(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_WIND_STATION, days_per_chunk=NOAA_CHUNK_DAYS)\n",
        "if len(df_noaa_wind):\n",
        "    fill_start, fill_end = df_noaa_wind[\"time\"].min(), df_noaa_wind[\"time\"].max()\n",
        "else:\n",
        "    candidates=[]\n",
        "    if len(df_swden): candidates.append((df_swden[\"time\"].min(), df_swden[\"time\"].max()))\n",
        "    if len(df_stdmet): candidates.append((df_stdmet[\"time\"].min(), df_stdmet[\"time\"].max()))\n",
        "    if candidates:\n",
        "        fill_start = min(a for a,_ in candidates); fill_end = max(b for _,b in candidates)\n",
        "    else:\n",
        "        fill_start = pd.Timestamp(\"2008-01-01\"); fill_end=pd.Timestamp(\"2025-10-01\")\n",
        "\n",
        "df_hmsc_raw  = fetch_hmsc_range(fill_start, fill_end)\n",
        "df_hmsc_hour = resample_hmsc_hourly(df_hmsc_raw)\n",
        "df_baro_var  = baro_highpass_variance(df_hmsc_hour, hp_hours=BARO_VAR_HIGHPASS_HOURS, var_hours=BARO_VAR_WIN_HOURS)\n",
        "\n",
        "# ---- NOAA/HMSC hourly merge with per-column fill (NOAA priority), speed/dir only ----\n",
        "hourly_grid = pd.date_range(fill_start.floor(\"h\"), fill_end.ceil(\"h\"), freq=\"h\")\n",
        "\n",
        "noaa = (df_noaa_wind.set_index(\"time\").reindex(hourly_grid).rename_axis(\"time\").reset_index()) \\\n",
        "        if len(df_noaa_wind) else pd.DataFrame({\"time\":hourly_grid})\n",
        "noaa.rename(columns={\n",
        "    \"wind9435380_speed\": \"speed\",\n",
        "    \"wind9435380_dir\":   \"dir\",\n",
        "}, inplace=True)\n",
        "\n",
        "if len(df_hmsc_hour):\n",
        "    hmsc = (df_hmsc_hour.set_index(\"time\").reindex(hourly_grid).rename_axis(\"time\").reset_index())\n",
        "else:\n",
        "    hmsc = pd.DataFrame({\n",
        "        \"time\": hourly_grid,\n",
        "        \"hmsc_speed_ms\": np.nan,\n",
        "        \"hmsc_dir_deg\":  np.nan,\n",
        "        \"hmsc_baro_hpa\": np.nan,\n",
        "    })\n",
        "\n",
        "for out_col, src_col in [(\"speed\", \"hmsc_speed_ms\"), (\"dir\", \"hmsc_dir_deg\")]:\n",
        "    if out_col not in noaa.columns:\n",
        "        noaa[out_col] = np.nan\n",
        "    if src_col in hmsc.columns:\n",
        "        m = noaa[out_col].isna() & hmsc[src_col].notna()\n",
        "        noaa.loc[m, out_col] = hmsc.loc[m, src_col].values\n",
        "\n",
        "df_wind_filled = pd.DataFrame({\n",
        "    \"time\": noaa[\"time\"],\n",
        "    \"wind9435380_speed\": pd.to_numeric(noaa[\"speed\"], errors=\"coerce\").astype(\"float32\"),\n",
        "    \"wind9435380_dir\":   pd.to_numeric(noaa[\"dir\"],   errors=\"coerce\").astype(\"float32\"),\n",
        "}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "# -------------------\n",
        "# NOAA 9435380 water level (6-min) → bandpass(20–30 min) → centered variance(2h)\n",
        "# -------------------\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def parse_noaa_wl_csv(txt:str)->pd.DataFrame:\n",
        "    if txt.strip().lower().startswith(\"error\"):\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    if df.shape[1] < 2:\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df.columns = [str(c).strip().lower().replace(\"  \", \" \") for c in df.columns]\n",
        "    tcol = None\n",
        "    for k in (\"date time\",\"date_time\",\"time\",\"t\",\"date\"):\n",
        "        if k in df.columns: tcol=k; break\n",
        "    if tcol is None:\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df[\"time\"] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    vcol = None\n",
        "    for k in (\"water level\",\"water_level\",\"wl\",\"v\"):\n",
        "        if k in df.columns: vcol=k; break\n",
        "    if vcol is None:\n",
        "        vcol = df.columns[1]\n",
        "    df[\"wl\"] = pd.to_numeric(df[vcol], errors=\"coerce\")\n",
        "    out = df[[\"time\",\"wl\"]].dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    out.loc[(out[\"wl\"]<-50)|(out[\"wl\"]>50),\"wl\"]=np.nan\n",
        "    if out[\"wl\"].notna().sum() == 0:\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def fetch_noaa_wl(begin_date:str, end_date:str, station:str, days_per_chunk:int=31)->pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\")\n",
        "    end   = pd.to_datetime(end_date,   format=\"%Y%m%d\")\n",
        "    chunks=[]; cur=start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        chunks.append((cur, chunk_end))\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "    frames=[]\n",
        "    for a,b in tqdm(chunks, desc=f\"NOAA WL {station}\", unit=\"chunk\"):\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(a)}&end_date={_datestr(b)}\"\n",
        "               f\"&station={station}&product=water_level&datum=MLLW&time_zone=gmt&units=metric\"\n",
        "               f\"&interval=6&application=SeichePipeline&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=90)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wl_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty:\n",
        "                    frames.append(dfc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not frames:\n",
        "        print(f\"[WL] Station {station} returned 0 rows between {begin_date} and {end_date}.\")\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    print(f\"[WL] Station {station} rows: {len(df):,}  range: {df['time'].min()} → {df['time'].max()}\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "if WL_USE_FETCH:\n",
        "    df_wl6 = fetch_noaa_wl(WL_BEGIN_DATE, WL_END_DATE, NOAA_WL_STATION, days_per_chunk=31)\n",
        "else:\n",
        "    try:\n",
        "        df_wl6\n",
        "    except NameError:\n",
        "        df_wl6 = pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "\n",
        "def bandpass_wl_var(df_wl:pd.DataFrame,\n",
        "                    dt_minutes:int,\n",
        "                    var_win_hours:float) -> pd.DataFrame:\n",
        "    \"\"\"Bandpass BANDPASS_MAX_MIN–BANDPASS_MIN_MIN minutes (order BP_ORDER), zero-phase,\n",
        "       then centered rolling variance with window = var_win_hours.\"\"\"\n",
        "    if df_wl.empty:\n",
        "        return pd.DataFrame(columns=[\"time\",\"var1h\"])\n",
        "    t0, t1 = df_wl[\"time\"].min(), df_wl[\"time\"].max()\n",
        "    grid = pd.date_range(t0.floor(f\"{dt_minutes}min\"), t1.ceil(f\"{dt_minutes}min\"), freq=f\"{dt_minutes}min\")\n",
        "    s = df_wl.set_index(\"time\")[\"wl\"].reindex(grid)\n",
        "    s = s.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    dt_sec = dt_minutes * 60.0\n",
        "    fs = 1.0 / dt_sec\n",
        "    f_lo = 1.0 / (BANDPASS_MAX_MIN * 60.0)   # e.g., 30 min\n",
        "    f_hi = 1.0 / (BANDPASS_MIN_MIN * 60.0)   # e.g., 20 min\n",
        "    wn = [f_lo/(fs/2.0), f_hi/(fs/2.0)]\n",
        "    wn = [max(1e-6, min(0.999, w)) for w in wn]\n",
        "    b,a = butter(BP_ORDER, wn, btype=\"bandpass\")\n",
        "    x = s.to_numpy(dtype=float)\n",
        "    if np.all(np.isnan(x)):\n",
        "        return pd.DataFrame(columns=[\"time\",\"var1h\"])\n",
        "    if np.isnan(x).any():\n",
        "        idx = np.arange(len(x)); good = np.isfinite(x)\n",
        "        x[~good] = np.interp(idx[~good], idx[good], x[good])\n",
        "    x_bp = filtfilt(b, a, x, method=\"gust\")\n",
        "    win = max(3, int(round((var_win_hours * 60.0) / dt_minutes)))\n",
        "    minp = max(2, int(0.8 * win))\n",
        "    var_series = pd.Series(x_bp, index=grid).rolling(window=win, center=True, min_periods=minp).var()\n",
        "    out = pd.DataFrame({\"time\": grid, \"var1h\": var_series.astype(\"float32\")}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "df_var = bandpass_wl_var(df_wl6, DT_MINUTES, VAR_WIN_HOURS)\n",
        "if df_var.empty:\n",
        "    raise RuntimeError(\"var1h is empty — WL series could not be fetched/parsed or contained no finite values.\")\n",
        "\n",
        "# -------------------\n",
        "# Build common grid; interpolate inputs to that grid\n",
        "# -------------------\n",
        "def interp_timegrid(t_src:pd.Series, x_src:np.ndarray, t_grid:pd.DatetimeIndex)->np.ndarray:\n",
        "    s = pd.Series(x_src, index=pd.to_datetime(t_src))\n",
        "    u = s.reindex(pd.to_datetime(sorted(set(s.index).union(set(t_grid)))))\n",
        "    u = u.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    return u.reindex(pd.to_datetime(t_grid)).to_numpy(dtype=float)\n",
        "\n",
        "def interp_dir_circular(t_src, dir_deg, t_grid):\n",
        "    \"\"\"Interpolate directions on a circle via sin/cos then atan2; returns 0–360 deg.\"\"\"\n",
        "    t_src = pd.to_datetime(t_src)\n",
        "    dir_arr = np.asarray(dir_deg, dtype=float)\n",
        "    theta = np.deg2rad(dir_arr)\n",
        "    s = np.sin(theta); c = np.cos(theta)\n",
        "    s_ser = pd.Series(s, index=t_src).sort_index()\n",
        "    c_ser = pd.Series(c, index=t_src).sort_index()\n",
        "    union_idx = pd.to_datetime(sorted(set(s_ser.index).union(set(t_grid))))\n",
        "    s_u = s_ser.reindex(union_idx).interpolate(\"time\", limit_area=\"inside\")\n",
        "    c_u = c_ser.reindex(union_idx).interpolate(\"time\", limit_area=\"inside\")\n",
        "    s_i = s_u.reindex(t_grid).to_numpy(dtype=float)\n",
        "    c_i = c_u.reindex(t_grid).to_numpy(dtype=float)\n",
        "    r = np.hypot(c_i, s_i); m = r>0\n",
        "    c_i[m] /= r[m]; s_i[m] /= r[m]\n",
        "    return (np.degrees(np.arctan2(s_i, c_i)) % 360.0).astype(float)\n",
        "\n",
        "# Overlap window across response & inputs\n",
        "t0_candidates = [df_var[\"time\"].min()]\n",
        "t1_candidates = [df_var[\"time\"].max()]\n",
        "if len(df_swden):\n",
        "    t0_candidates.append(df_swden[\"time\"].min()); t1_candidates.append(df_swden[\"time\"].max())\n",
        "if len(df_stdmet):\n",
        "    t0_candidates.append(df_stdmet[\"time\"].min()); t1_candidates.append(df_stdmet[\"time\"].max())\n",
        "if len(df_wind_filled):\n",
        "    t0_candidates.append(df_wind_filled[\"time\"].min()); t1_candidates.append(df_wind_filled[\"time\"].max())\n",
        "if len(df_hmsc_hour):\n",
        "    t0_candidates.append(df_hmsc_hour[\"time\"].min()); t1_candidates.append(df_hmsc_hour[\"time\"].max())\n",
        "if len(df_baro_var):\n",
        "    t0_candidates.append(df_baro_var[\"time\"].min()); t1_candidates.append(df_baro_var[\"time\"].max())\n",
        "\n",
        "t0 = max(t0_candidates); t1 = min(t1_candidates)\n",
        "t_grid = pd.date_range(t0, t1, freq=f\"{DT_MINUTES}min\")\n",
        "\n",
        "seq_mat = {}\n",
        "# Response on grid\n",
        "var_on_grid = interp_timegrid(df_var[\"time\"], df_var[\"var1h\"].values, t_grid)\n",
        "\n",
        "# SWDEN bands\n",
        "if len(df_swden):\n",
        "    if \"Hswell\" in df_swden.columns:\n",
        "        seq_mat[\"Hswell\"] = interp_timegrid(df_swden[\"time\"], df_swden[\"Hswell\"].values, t_grid)\n",
        "    elif \"Hm0_swell\" in df_swden.columns:\n",
        "        seq_mat[\"Hswell\"] = interp_timegrid(df_swden[\"time\"], df_swden[\"Hm0_swell\"].values, t_grid)\n",
        "    else:\n",
        "        seq_mat[\"Hswell\"] = np.full(len(t_grid), np.nan)\n",
        "    if \"Hsea\" in df_swden.columns:\n",
        "        seq_mat[\"Hsea\"] = interp_timegrid(df_swden[\"time\"], df_swden[\"Hsea\"].values, t_grid)\n",
        "    elif \"Hm0_sea\" in df_swden.columns:\n",
        "        seq_mat[\"Hsea\"] = interp_timegrid(df_swden[\"time\"], df_swden[\"Hm0_sea\"].values, t_grid)\n",
        "    else:\n",
        "        seq_mat[\"Hsea\"] = np.full(len(t_grid), np.nan)\n",
        "    if \"high_HIG\" in df_swden.columns:\n",
        "        seq_mat[\"high_HIG\"] = interp_timegrid(df_swden[\"time\"], df_swden[\"high_HIG\"].values, t_grid)\n",
        "else:\n",
        "    seq_mat[\"Hswell\"]   = np.full(len(t_grid), np.nan)\n",
        "    seq_mat[\"Hsea\"]     = np.full(len(t_grid), np.nan)\n",
        "    seq_mat[\"high_HIG\"] = np.full(len(t_grid), np.nan)\n",
        "\n",
        "# STDMET waves\n",
        "for c in [\"Hs\",\"DPD\",\"APD\",\"MWD\"]:\n",
        "    if len(df_stdmet):\n",
        "        seq_mat[c] = interp_timegrid(df_stdmet[\"time\"], df_stdmet[c].values, t_grid)\n",
        "    else:\n",
        "        seq_mat[c] = np.full(len(t_grid), np.nan)\n",
        "\n",
        "# Wind (hourly → DT): circular interpolate direction; scale speed by WIND_WEIGHT\n",
        "if len(df_wind_filled):\n",
        "    sp6  = interp_timegrid(df_wind_filled[\"time\"], df_wind_filled[\"wind9435380_speed\"].values, t_grid)\n",
        "    dir6 = interp_dir_circular(df_wind_filled[\"time\"], df_wind_filled[\"wind9435380_dir\"].values, t_grid)\n",
        "    seq_mat[\"wind9435380_speed\"] = (sp6 * float(WIND_WEIGHT)).astype(float)\n",
        "    seq_mat[\"wind9435380_dir\"]   = dir6.astype(float)\n",
        "\n",
        "# HMSC baro + variance\n",
        "if len(df_hmsc_hour) and \"hmsc_baro_hpa\" in df_hmsc_hour.columns:\n",
        "    seq_mat[\"hmsc_baro_hpa\"] = interp_timegrid(df_hmsc_hour[\"time\"], df_hmsc_hour[\"hmsc_baro_hpa\"].values, t_grid)\n",
        "else:\n",
        "    seq_mat[\"hmsc_baro_hpa\"] = np.full(len(t_grid), np.nan)\n",
        "if len(df_baro_var):\n",
        "    seq_mat[\"baro_var4h\"] = interp_timegrid(df_baro_var[\"time\"], df_baro_var[\"baro_var4h\"].values, t_grid)\n",
        "else:\n",
        "    seq_mat[\"baro_var4h\"] = np.full(len(t_grid), np.nan)\n",
        "\n",
        "# ---- Unified gap mask (optional) ----\n",
        "def gap_intervals(t:pd.Series, x:np.ndarray, min_gap_hours:float)->List[Tuple[pd.Timestamp,pd.Timestamp]]:\n",
        "    t = pd.to_datetime(t)\n",
        "    dt = np.diff(t.values).astype(\"timedelta64[s]\").astype(int)\n",
        "    jumps = np.where(dt > min_gap_hours*3600)[0]\n",
        "    iv = [(t[j], t[j+1]) for j in jumps]\n",
        "    s = pd.Series(x, index=t)\n",
        "    isn = s.isna().to_numpy()\n",
        "    if isn.any():\n",
        "        starts = np.where(np.diff(np.r_[False, isn])==1)[0]\n",
        "        ends   = np.where(np.diff(np.r_[isn, False])==-1)[0]\n",
        "        for a,b in zip(starts, ends):\n",
        "            if (t[b]-t[a]) >= pd.Timedelta(hours=min_gap_hours):\n",
        "                iv.append((t[a], t[b]))\n",
        "    return iv\n",
        "\n",
        "def merge_intervals(intervals):\n",
        "    if not intervals: return []\n",
        "    z = sorted(intervals, key=lambda k: k[0]); out = [list(z[0])]\n",
        "    for s,e in z[1:]:\n",
        "        if s <= out[-1][1]: out[-1][1] = max(out[-1][1], e)\n",
        "        else: out.append([s,e])\n",
        "    return [(pd.to_datetime(a), pd.to_datetime(b)) for a,b in out]\n",
        "\n",
        "if USE_UNIFIED_GAP_MASK:\n",
        "    all_intervals=[]\n",
        "    if len(df_swden):\n",
        "        for c in [\"Hswell\",\"Hsea\",\"high_HIG\"]:\n",
        "            if c in df_swden.columns:\n",
        "                all_intervals += gap_intervals(df_swden[\"time\"], df_swden[c].values, MAX_GAP_HOURS)\n",
        "    if len(df_stdmet):\n",
        "        for c in [\"Hs\",\"DPD\",\"APD\",\"MWD\"]:\n",
        "            all_intervals += gap_intervals(df_stdmet[\"time\"], df_stdmet[c].values, MAX_GAP_HOURS)\n",
        "    if len(df_wind_filled):\n",
        "        for c in [\"wind9435380_speed\",\"wind9435380_dir\"]:\n",
        "            all_intervals += gap_intervals(df_wind_filled[\"time\"], df_wind_filled[c].values, MAX_GAP_HOURS)\n",
        "    if len(df_hmsc_hour):\n",
        "        for c in [\"hmsc_baro_hpa\"]:\n",
        "            all_intervals += gap_intervals(df_hmsc_hour[\"time\"], df_hmsc_hour[c].values, MAX_GAP_HOURS)\n",
        "    merged_gaps = merge_intervals(all_intervals)\n",
        "    gap_mask = np.zeros(len(t_grid), dtype=bool)\n",
        "    for s,e in merged_gaps:\n",
        "        gap_mask |= (t_grid>=s) & (t_grid<=e)\n",
        "else:\n",
        "    gap_mask = np.zeros(len(t_grid), dtype=bool)\n",
        "\n",
        "# Assemble df_all\n",
        "df_all = pd.DataFrame({\"time\": t_grid, \"var1h\": var_on_grid})\n",
        "for c,v in seq_mat.items():\n",
        "    df_all[c] = v\n",
        "df_all = df_all.loc[~gap_mask].reset_index(drop=True)\n",
        "\n",
        "# Clean/Interpolate some inputs post-align\n",
        "for col, thresh in [(\"Hs\",50.0),(\"DPD\",50.0),(\"APD\",50.0)]:\n",
        "    if col in df_all.columns:\n",
        "        m = df_all[col] > thresh\n",
        "        if m.any(): df_all.loc[m, col] = np.nan\n",
        "        df_all[col] = pd.Series(df_all[col].values, index=df_all[\"time\"]).interpolate(\"time\", limit_area=\"inside\").values\n",
        "\n",
        "# Feature placeholder\n",
        "df_all[\"wl_gradient\"] = np.nan\n",
        "\n",
        "# -------------------\n",
        "# Lag candidate helpers\n",
        "# -------------------\n",
        "def is_eq_base(name: str) -> bool:\n",
        "    n = name.lower()\n",
        "    return n.startswith(\"eq\") or (\"eq_\" in n)\n",
        "\n",
        "def build_lag_candidates(df: pd.DataFrame,\n",
        "                         bases: List[str],\n",
        "                         dt_minutes: int,\n",
        "                         step_minutes: int = 30) -> Tuple[pd.DataFrame, List[str], Dict[str, str], Dict[str, int]]:\n",
        "    lag_cols, series = [], {}\n",
        "    base_map, lag_minutes_map = {}, {}\n",
        "    for b in bases:\n",
        "        x = df[b].astype(\"float32\").to_numpy()\n",
        "        if not np.isfinite(x).any():\n",
        "            continue\n",
        "        max_lag_min = EQ_MAX_LAG_MIN if is_eq_base(b) else NON_EQ_MAX_LAG_MIN\n",
        "        grid_min = list(range(0, max_lag_min + 1, int(step_minutes)))\n",
        "        for L in grid_min:\n",
        "            steps = int(round(L / dt_minutes))\n",
        "            nm = f\"{b}__lag_{L}min\"\n",
        "            lag_minutes_map[nm] = L\n",
        "            base_map[nm] = b\n",
        "            if steps == 0:\n",
        "                series[nm] = x\n",
        "            else:\n",
        "                s = pd.Series(x)\n",
        "                series[nm] = s.shift(steps).to_numpy(dtype=\"float32\")\n",
        "            lag_cols.append(nm)\n",
        "    lag_df = pd.DataFrame(series).reindex(range(len(df))).reset_index(drop=True)\n",
        "    return lag_df, lag_cols, base_map, lag_minutes_map\n",
        "\n",
        "# Build lags from df_all\n",
        "exclude_cols = set([\"time\", \"var1h\", \"var1h_log1p\", \"wsp\", \"wdir\", \"gust\", \"atmvar1_boost\"])\n",
        "cand_bases = [c for c in df_all.columns if (c not in exclude_cols and df_all[c].dtype != object)]\n",
        "exclude_cols = set([\"time\",\"var1h\",\"var1h_log1p\",\"wsp\",\"wdir\",\"gust\",\"atmvar1_boost\",\"Hs\"])  # <— add Hs here\n",
        "cand_bases = [c for c in df_all.columns if (c not in exclude_cols and df_all[c].dtype != object)]\n",
        "lag_df, lag_cols, base_map, lag_minutes_map = build_lag_candidates(df_all, cand_bases, DT_MINUTES)\n",
        "\n",
        "# -------------------\n",
        "# XGBoost ranking: pick ONE best lag per base\n",
        "# -------------------\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "h_steps = max(1, int(round(PRIMARY_HORIZON / DT_MINUTES)))\n",
        "tgt = df_all[\"var1h\"].shift(-h_steps).reset_index(drop=True).astype(\"float32\")\n",
        "rank_data = pd.concat([lag_df, tgt.rename(\"target\")], axis=1).dropna().reset_index(drop=True)\n",
        "\n",
        "if rank_data.empty:\n",
        "    raise RuntimeError(\"No data for XGB ranking after lagging. Check df_all coverage and PRIMARY_HORIZON.\")\n",
        "\n",
        "X_rank = rank_data[lag_cols].to_numpy(dtype=np.float32)\n",
        "y_rank = rank_data[\"target\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "xgb_imp = XGBRegressor(\n",
        "    n_estimators=min(300, XGB_N_ESTIMATORS//2), learning_rate=XGB_LEARNING_RATE,\n",
        "    max_depth=XGB_MAX_DEPTH, subsample=XGB_SUBSAMPLE, colsample_bytree=XGB_COLSAMPLE,\n",
        "    reg_lambda=XGB_L2, objective=\"reg:squarederror\", n_jobs=-1, random_state=42, verbosity=XGB_VERBOSE\n",
        ")\n",
        "split = max(1, int(0.8 * len(X_rank)))\n",
        "xgb_imp.fit(X_rank[:split], y_rank[:split], verbose=False)\n",
        "importances = dict(zip(lag_cols, xgb_imp.feature_importances_))\n",
        "\n",
        "best_lag_per_base = {}\n",
        "for nm, imp in importances.items():\n",
        "    b = base_map[nm]\n",
        "    prev = best_lag_per_base.get(b)\n",
        "    if (prev is None) or (imp > prev[1]):\n",
        "        best_lag_per_base[b] = (nm, float(imp))\n",
        "\n",
        "best_list = [(b, nm, imp) for b, (nm, imp) in best_lag_per_base.items()]\n",
        "best_list.sort(key=lambda t: t[2], reverse=True)\n",
        "best_list = best_list[:TOP_LAG_FEATURES]\n",
        "selected_lag_cols = [nm for _, nm, _ in best_list]\n",
        "selected_bases = [b for b, _, _ in best_list]\n",
        "\n",
        "print(\"\\nSelected ONE best lag per base:\")\n",
        "for b, nm, imp in best_list:\n",
        "    print(f\"  {nm:<36s} (base={b}, importance={imp:.6f}, lag={lag_minutes_map[nm]} min)\")\n",
        "\n",
        "df_lag_all = pd.concat(\n",
        "    [df_all[[\"time\", \"var1h\"]].reset_index(drop=True),\n",
        "     lag_df[selected_lag_cols].reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep your best_list logic... then:\n",
        "selected_lag_cols = [nm for _, nm, _ in best_list]\n",
        "selected_bases    = [b  for b, _, _ in best_list]\n",
        "\n",
        "def _best_existing_lag_for(base_name:str, candidate_lags_min:list[int])->str|None:\n",
        "    # prefer specific lags if available\n",
        "    for L in candidate_lags_min:\n",
        "        nm = f\"{base_name}__lag_{L}min\"\n",
        "        if nm in lag_cols:\n",
        "            return nm\n",
        "    # otherwise, find any lag for this base with max importance\n",
        "    cands = [nm for nm in lag_cols if base_map.get(nm)==base_name]\n",
        "    if not cands: return None\n",
        "    return max(cands, key=lambda z: importances.get(z, 0.0))\n",
        "\n",
        "if BARO_FORCE_IN_XGB:\n",
        "    for base in [\"baro_var4h\", \"hmsc_baro_hpa\"]:\n",
        "        nm = _best_existing_lag_for(base, BARO_FORCE_LAGS_MIN)\n",
        "        if nm and nm not in selected_lag_cols:\n",
        "            selected_lag_cols.append(nm)\n",
        "            selected_bases.append(base)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Mini-TFT (seq-to-one) — compact training loop with nonneg output\n",
        "# ============================================================\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def _amp_ctx():\n",
        "    return nullcontext()\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.v = nn.Linear(d, d)\n",
        "        self.g = nn.Linear(d, d)\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.g(x)) * self.v(x)\n",
        "\n",
        "class MiniTFT(nn.Module):\n",
        "    def __init__(self, num_features:int, d_model:int=TFT_D_MODEL, nhead:int=TFT_NHEAD,\n",
        "                 lstm_layers:int=TFT_LSTM_LAYERS, ff_dim:int=TFT_FF_DIM, dropout:float=TFT_DROPOUT,\n",
        "                 nonneg:bool=TFT_NONNEG_OUTPUT, var_floor:float=TFT_VAR_FLOOR):\n",
        "        super().__init__()\n",
        "        self.F = num_features\n",
        "        self.d = d_model\n",
        "        self.nonneg = nonneg\n",
        "        self.var_floor = float(var_floor)\n",
        "        self.feat_emb = nn.ModuleList([nn.Linear(1, d_model) for _ in range(num_features)])\n",
        "        self.vsn_w = nn.Linear(num_features, num_features)\n",
        "        self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model,\n",
        "                            num_layers=lstm_layers, batch_first=True, dropout=0.0, bidirectional=False)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(2*d_model, ff_dim), nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, d_model), GLU(d_model),\n",
        "            nn.Linear(d_model, 1)\n",
        "        )\n",
        "        self.out_act = nn.Softplus() if self.nonneg else nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.last_alpha = None\n",
        "    def forward(self, x):  # (B,T,F)\n",
        "        B,T,F = x.shape\n",
        "        x4 = x.unsqueeze(-1)\n",
        "        embs = [ self.feat_emb[f](x4[:,:,f,:]) for f in range(self.F) ]\n",
        "        E = torch.stack(embs, dim=2)            # (B,T,F,d)\n",
        "        logits = self.vsn_w(x)                  # (B,T,F)\n",
        "        alpha = torch.softmax(logits, dim=2).unsqueeze(-1)\n",
        "        self.last_alpha = alpha.detach()\n",
        "        Z = (alpha * E).sum(dim=2)              # (B,T,d)\n",
        "        Z = self.dropout(Z)\n",
        "        H, _ = self.lstm(Z)\n",
        "        last = H[:, -1:, :]\n",
        "        ctx, _ = self.attn(query=last, key=H, value=H, need_weights=False)\n",
        "        cat = torch.cat([last, ctx], dim=-1).squeeze(1)\n",
        "        out = self.proj(cat).squeeze(-1)\n",
        "        out = self.out_act(out)\n",
        "        if self.nonneg and self.var_floor > 0:\n",
        "            out = torch.clamp(out, min=self.var_floor)\n",
        "        return out\n",
        "\n",
        "def _safe_robust_params(X: np.ndarray, q_low=25.0, q_high=75.0, eps=1e-6):\n",
        "    Xf = X.reshape(-1, X.shape[2])\n",
        "    med = np.nanmedian(Xf, axis=0)\n",
        "    q1  = np.nanpercentile(Xf, q_low, axis=0)\n",
        "    q3  = np.nanpercentile(Xf, q_high, axis=0)\n",
        "    iqr = q3 - q1\n",
        "    scale = np.where(iqr <= eps, 1.0, iqr)\n",
        "    return med.astype(np.float32), scale.astype(np.float32)\n",
        "\n",
        "def fit_scaler_from_windows_safe(X_tr, X_va=None, X_te=None):\n",
        "    for X in (X_tr, X_va, X_te):\n",
        "        if X is not None and len(X) > 0:\n",
        "            med, scale = _safe_robust_params(X)\n",
        "            return {\"center\": med, \"scale\": scale}\n",
        "    F = X_tr.shape[2] if (X_tr is not None and X_tr.size) else (\n",
        "        X_va.shape[2] if (X_va is not None and X_va.size) else (\n",
        "        X_te.shape[2] if (X_te is not None and X_te.size) else 1))\n",
        "    return {\"center\": np.zeros(F, np.float32), \"scale\": np.ones(F, np.float32)}\n",
        "\n",
        "def norm_windows(X, rs, clip=20.0):\n",
        "    if len(X) == 0:\n",
        "        return X\n",
        "    F = X.shape[2]\n",
        "    C = rs[\"center\"].reshape(1, 1, F)\n",
        "    S = rs[\"scale\" ].reshape(1, 1, F)\n",
        "    Z = (X - C) / S\n",
        "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
        "    if clip is not None:\n",
        "        Z = np.clip(Z, -clip, clip, out=Z)\n",
        "    return Z\n",
        "\n",
        "def make_windows_from_lagged(df:pd.DataFrame, lag_cols:List[str],\n",
        "                             history_hours:int, horizon_min:int, dt_minutes:int,\n",
        "                             stride:int=1):\n",
        "    steps_hist  = int(round(history_hours*60/dt_minutes))\n",
        "    steps_ahead = int(round(horizon_min/dt_minutes))\n",
        "    tab = df[[\"time\",\"var1h\"] + lag_cols].copy()\n",
        "    tab = tab.dropna(subset=lag_cols + [\"var1h\"]).reset_index(drop=True)\n",
        "    if len(tab) < steps_hist + steps_ahead + 1:\n",
        "        return np.empty((0,steps_hist,len(lag_cols))), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    X_list, y_list, t_list = [], [], []\n",
        "    F = len(lag_cols); Ttot = len(tab)\n",
        "    for end_idx in range(steps_hist, Ttot - steps_ahead + 1, stride):\n",
        "        s = end_idx - steps_hist\n",
        "        k = end_idx - 1 + steps_ahead\n",
        "        xb = tab.iloc[s:end_idx][lag_cols].values\n",
        "        yv = tab.iloc[k][\"var1h\"]\n",
        "        if np.any(~np.isfinite(xb)) or not np.isfinite(yv):\n",
        "            continue\n",
        "        X_list.append(xb.astype(np.float32))\n",
        "        y_list.append(float(yv))\n",
        "        t_list.append(pd.to_datetime(tab.iloc[k][\"time\"]))\n",
        "    if not X_list:\n",
        "        return np.empty((0,steps_hist,F)), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    return np.stack(X_list), np.array(y_list, dtype=np.float32), pd.DatetimeIndex(t_list)\n",
        "\n",
        "def adaptive_index_split(X, y, t_idx, train_frac=0.70, val_frac=0.15):\n",
        "    n = len(X)\n",
        "    if n < 10:\n",
        "        return (X[:0], y[:0], t_idx[:0]), (X[:0], y[:0], t_idx[:0]), (X, y, t_idx)\n",
        "    a = max(1, int(round(train_frac * n)))\n",
        "    b = max(a+1, int(round((train_frac + val_frac) * n)))\n",
        "    b = min(b, n-1)\n",
        "    return (X[:a], y[:a], t_idx[:a]), (X[a:b], y[a:b], t_idx[a:b]), (X[b:], y[b:], t_idx[b:])\n",
        "\n",
        "def cap_split(X_, y_, t_, cap=MAX_SAMPLES_SPLIT):\n",
        "    if len(X_) <= cap:\n",
        "        return X_, y_, t_\n",
        "    idx = np.linspace(0, len(X_) - 1, cap, dtype=int)\n",
        "    return X_[idx], y_[idx], t_[idx]\n",
        "\n",
        "def _fmt(ts):\n",
        "    return ts.strftime(\"%Y-%m-%d %H:%M\") if (ts is not None and pd.notna(ts)) else \"—\"\n",
        "\n",
        "def print_split_summary(horizon, t_tr, t_va, t_te):\n",
        "    def rng(t):\n",
        "        if len(t)==0:\n",
        "            return (None,None,0)\n",
        "        return (t.min(), t.max(), len(t))\n",
        "    tr0,tr1,ntr = rng(t_tr); va0,va1,nva = rng(t_va); te0,te1,nte = rng(t_te)\n",
        "    print(f\"[{horizon}m] Split ranges:\")\n",
        "    print(f\"  Train: {_fmt(tr0)} → {_fmt(tr1)}  (n={ntr})\")\n",
        "    print(f\"  Valid: {_fmt(va0)} → {_fmt(va1)}  (n={nva})\")\n",
        "    print(f\"  Test : {_fmt(te0)} → {_fmt(te1)}  (n={nte})\")\n",
        "    return (tr0,tr1),(va0,va1),(te0,te1)\n",
        "\n",
        "# ---- Train loop (per horizon) ----\n",
        "results = {}\n",
        "\n",
        "for horizon in HORIZONS_MIN:\n",
        "    print(f\"\\n=== Horizon {horizon} min ===\")\n",
        "    X, y, t_idx = make_windows_from_lagged(df_lag_all, selected_lag_cols,\n",
        "                                           HISTORY_HOURS, horizon, DT_MINUTES, stride=1)\n",
        "    if len(t_idx):\n",
        "        print(f\"[{horizon}m] Windows available:\", t_idx.min(), \"→\", t_idx.max(), f\"(n={len(t_idx):,})\")\n",
        "    else:\n",
        "        print(f\"[{horizon}m] No windows after lagging/masking.\")\n",
        "        continue\n",
        "\n",
        "    # Time split\n",
        "    mtr = t_idx <= TRAIN_END\n",
        "    mva = (t_idx > TRAIN_END) & (t_idx <= VAL_END)\n",
        "    mte = t_idx > VAL_END\n",
        "    X_tr, y_tr, t_tr = X[mtr], y[mtr], t_idx[mtr]\n",
        "    X_va, y_va, t_va = X[mva], y[mva], t_idx[mva]\n",
        "    X_te, y_te, t_te = X[mte], y[mte], t_idx[mte]\n",
        "    if len(X_tr)==0 or len(X_va)==0 or len(X_te)==0:\n",
        "        print(\"  Time split empty; using adaptive 70/15/15.\")\n",
        "        (X_tr, y_tr, t_tr), (X_va, y_va, t_va), (X_te, y_te, t_te) = adaptive_index_split(X, y, t_idx)\n",
        "    if len(X_te)==0:\n",
        "        n=len(X); cut=max(1,int(0.85*n))\n",
        "        X_tr,y_tr,t_tr = X[:cut],y[:cut],t_idx[:cut]\n",
        "        X_va,y_va,t_va = X[:0],y[:0],t_idx[:0]\n",
        "        X_te,y_te,t_te = X[cut:],y[cut:],t_idx[cut:]\n",
        "\n",
        "    X_tr, y_tr, t_tr = cap_split(X_tr, y_tr, t_tr, cap=MAX_SAMPLES_SPLIT)\n",
        "    X_va, y_va, t_va = cap_split(X_va, y_va, t_va, cap=MAX_SAMPLES_SPLIT)\n",
        "    X_te, y_te, t_te = cap_split(X_te, y_te, t_te, cap=MAX_SAMPLES_SPLIT)\n",
        "\n",
        "    _ = print_split_summary(horizon, t_tr, t_va, t_te)\n",
        "\n",
        "        # --- Target transform for TFT ---\n",
        "    if TFT_TARGET_LOG1P:\n",
        "        y_tr_t = np.log1p(np.maximum(y_tr, 0.0))\n",
        "        y_va_t = np.log1p(np.maximum(y_va, 0.0))\n",
        "        y_te_t = np.log1p(np.maximum(y_te, 0.0))\n",
        "    else:\n",
        "        y_tr_t, y_va_t, y_te_t = y_tr, y_va, y_te\n",
        "\n",
        "\n",
        "    # Normalize windows for TFT\n",
        "    rs     = fit_scaler_from_windows_safe(X_tr, X_va, X_te)\n",
        "    X_tr_n = norm_windows(X_tr, rs); X_va_n = norm_windows(X_va, rs); X_te_n = norm_windows(X_te, rs)\n",
        "\n",
        "    # --- Datasets / Loaders (use transformed targets y_*_t) ---\n",
        "    device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "    model = MiniTFT(\n",
        "        num_features=X_tr_n.shape[2],\n",
        "        d_model=TFT_D_MODEL, nhead=TFT_NHEAD,\n",
        "        lstm_layers=TFT_LSTM_LAYERS, ff_dim=TFT_FF_DIM, dropout=TFT_DROPOUT,\n",
        "        nonneg=TFT_NONNEG_OUTPUT, var_floor=TFT_VAR_FLOOR\n",
        "    ).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=TFT_LR)\n",
        "\n",
        "    ds_tr = TensorDataset(torch.from_numpy(X_tr_n), torch.from_numpy(y_tr_t))\n",
        "    ds_va = TensorDataset(torch.from_numpy(X_va_n), torch.from_numpy(y_va_t))\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=min(TFT_BATCH, len(ds_tr)), shuffle=True,  drop_last=False)\n",
        "    dl_va = DataLoader(ds_va, batch_size=min(TFT_BATCH, len(ds_va)), shuffle=False, drop_last=False)\n",
        "\n",
        "    best = np.inf; best_state=None; bad=0\n",
        "    train_curve=[]; val_curve=[]\n",
        "\n",
        "    # Spike threshold for weighting computed in TRANSFORMED space\n",
        "    q = np.quantile(y_tr_t, BARO_SPIKE_Q) if (WEIGHT_SPIKES and len(y_tr_t)>0) else None\n",
        "\n",
        "\n",
        "    for ep in range(TFT_EPOCHS):\n",
        "        model.train(); tr_loss=0.0\n",
        "        for xb,yb in dl_tr:\n",
        "            xb,yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with _amp_ctx():\n",
        "                pred = model(xb)                 # pred in transformed space\n",
        "                l = (pred - yb) ** 2\n",
        "                if q is not None:\n",
        "                    w = torch.where(\n",
        "                        yb >= torch.tensor(q, device=yb.device, dtype=yb.dtype),\n",
        "                        torch.tensor(BARO_SPIKE_LOSS_W, device=yb.device, dtype=yb.dtype),\n",
        "                        torch.tensor(1.0,               device=yb.device, dtype=yb.dtype)\n",
        "                    )\n",
        "                    l = l * w\n",
        "                loss = l.mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            opt.step()\n",
        "            tr_loss += loss.item() * len(xb)\n",
        "        tr_loss /= max(1, len(dl_tr.dataset))\n",
        "\n",
        "        model.eval(); va_loss=0.0\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for xb,yb in dl_va:\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb)\n",
        "                va_loss += nn.functional.mse_loss(out, yb, reduction=\"sum\").item()\n",
        "        va_loss /= max(1, len(dl_va.dataset))\n",
        "        train_curve.append(tr_loss); val_curve.append(va_loss)\n",
        "        print(f\"[TFT {horizon}m] epoch {ep+1:02d}  trainMSE={tr_loss:.5f}  valMSE={va_loss:.5f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best:\n",
        "            best = va_loss; best_state = model.state_dict(); bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= TFT_PATIENCE:\n",
        "                print(\"[TFT] early stop.\"); break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "\n",
        "    def predict_tft(model, X, device):\n",
        "        ds = TensorDataset(torch.from_numpy(X))\n",
        "        dl = DataLoader(ds, batch_size=min(TFT_BATCH, len(ds)), shuffle=False, drop_last=False)\n",
        "        out=[]; model.eval()\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for (xb,) in dl:\n",
        "                xb = xb.to(device)\n",
        "                out.append(model(xb).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0) if len(out)>0 else np.array([])\n",
        "\n",
        "    yp_tr = predict_tft(model, X_tr_n, device) if len(X_tr_n)>0 else np.array([])\n",
        "    yp_va = predict_tft(model, X_va_n, device) if len(X_va_n)>0 else np.array([])\n",
        "    yp_te = predict_tft(model, X_te_n, device) if len(X_te_n)>0 else np.array([])\n",
        "\n",
        "    # --- invert transform back to variance units ---\n",
        "    if TFT_TARGET_LOG1P:\n",
        "        if yp_tr.size: yp_tr = np.expm1(yp_tr)\n",
        "        if yp_va.size: yp_va = np.expm1(yp_va)\n",
        "        if yp_te.size: yp_te = np.expm1(yp_te)\n",
        "\n",
        "    # Safety clamp (numeric)\n",
        "    if yp_tr.size: yp_tr = np.maximum(yp_tr, TFT_VAR_FLOOR)\n",
        "    if yp_va.size: yp_va = np.maximum(yp_va, TFT_VAR_FLOOR)\n",
        "    if yp_te.size: yp_te = np.maximum(yp_te, TFT_VAR_FLOOR)\n",
        "\n",
        "\n",
        "    def _rmse(a,b):\n",
        "        return float(np.sqrt(np.mean((np.asarray(a)-np.asarray(b))**2))) if len(a)==len(b) and len(a)>0 else float(\"nan\")\n",
        "\n",
        "    R2   = r2_score(y_te, yp_te) if len(yp_te)>0 and len(y_te)>0 else float(\"nan\")\n",
        "    RMSE = _rmse(y_te, yp_te)\n",
        "    tr_RMSE = _rmse(y_tr, yp_tr) if len(yp_tr)>0 else float(\"nan\")\n",
        "    va_RMSE = _rmse(y_va, yp_va) if len(yp_va)>0 else float(\"nan\")\n",
        "    print(f\"[TFT] {horizon}m  Train RMSE={tr_RMSE:.3f}  Val RMSE={va_RMSE:.3f}  Test R²={R2:.3f}  RMSE={RMSE:.3f}\")\n",
        "\n",
        "    if np.isfinite(tr_RMSE) and np.isfinite(va_RMSE):\n",
        "        gap = va_RMSE - tr_RMSE\n",
        "        if gap > max(0.15*va_RMSE, 0.02):\n",
        "            print(\"  ↪ Likely OVERFITTING: try ↑TFT_DROPOUT, ↓TFT_D_MODEL/FF_DIM, fewer TFT_EPOCHS, ↑HISTORY_HOURS.\")\n",
        "        elif (tr_RMSE > va_RMSE*1.2):\n",
        "            print(\"  ↪ Likely UNDERFITTING: try ↑TFT_D_MODEL/FF_DIM/LSTM_LAYERS/EPOCHS, add features, extend HISTORY_HOURS.\")\n",
        "\n",
        "    results[horizon] = dict(\n",
        "        t_val=t_va, y_val=y_va, yp_val=yp_va,\n",
        "        t=t_te, y=y_te, yp=yp_te, r2=R2, rmse=RMSE,\n",
        "        scaler=rs, train_curve=np.array(train_curve), val_curve=np.array(val_curve),\n",
        "        t_tr=t_tr, y_tr=y_tr, yp_tr=yp_tr\n",
        "    )\n",
        "\n",
        "    # Learning curves\n",
        "    if len(train_curve) and len(val_curve):\n",
        "        plt.figure(figsize=(8,3.2))\n",
        "        plt.plot(train_curve, label=\"Train MSE\")\n",
        "        plt.plot(val_curve, label=\"Val MSE\")\n",
        "        plt.title(f\"TFT learning curves — {horizon} min\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\"); plt.grid(alpha=0.3); plt.legend(); plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---- Per-horizon visualization: TFT vs actual + inputs during forecast window ----\n",
        "def _get_base_series(name:str):\n",
        "    if name in df_all.columns:\n",
        "        return df_all[\"time\"].to_numpy(), df_all[name].to_numpy()\n",
        "    return None, None\n",
        "\n",
        "for horizon in HORIZONS_MIN:\n",
        "    rec = results.get(horizon)\n",
        "    if not rec:\n",
        "        continue\n",
        "    if PRED_SPLIT.lower() == \"test\" and len(rec[\"t\"])>0:\n",
        "        t_plot, y_plot, yhat_plot = rec[\"t\"], rec[\"y\"], rec[\"yp\"]\n",
        "        split_name = \"Test\"\n",
        "    else:\n",
        "        t_plot, y_plot, yhat_plot = rec[\"t_val\"], rec[\"y_val\"], rec[\"yp_val\"]\n",
        "        split_name = \"Validation\"\n",
        "    if len(t_plot)==0:\n",
        "        continue\n",
        "\n",
        "    mwin = (t_plot >= PRED_START) & (t_plot <= PRED_END)\n",
        "    if not np.any(mwin):\n",
        "        end = t_plot[-1]; start = end - pd.Timedelta(days=PLOT_LAST_DAYS)\n",
        "        mwin = (t_plot >= start) & (t_plot <= end)\n",
        "    if not np.any(mwin):\n",
        "        continue\n",
        "\n",
        "    ts, yt, ypp = t_plot[mwin], y_plot[mwin], yhat_plot[mwin]\n",
        "\n",
        "    # Figure 1: TFT predictions vs actual\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.plot(ts, yt, label=\"Measured var1h\", lw=1.2)\n",
        "    plt.plot(ts, ypp, \"--\", label=f\"TFT Pred ({split_name})\", lw=1.2)\n",
        "    plt.title(f\"Mini-TFT — {horizon} min ({split_name})\")\n",
        "    plt.ylabel(\"var1h\"); plt.xlabel(\"Time\"); plt.grid(alpha=0.3); plt.legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # Figure 2: Input drivers (top K bases)\n",
        "    bases_to_plot = selected_bases[:PLOT_TOP_INPUTS]\n",
        "    nK = len(bases_to_plot)\n",
        "    if nK > 0:\n",
        "        fig, axs = plt.subplots(nK, 1, figsize=(14, 1.8*nK), sharex=True)\n",
        "        if nK == 1: axs = [axs]\n",
        "        for ax, bn in zip(axs, bases_to_plot):\n",
        "            tb, xb = _get_base_series(bn)\n",
        "            if tb is None:\n",
        "                ax.text(0.02, 0.6, f\"{bn} (no series)\", transform=ax.transAxes); continue\n",
        "            tb = pd.to_datetime(tb)\n",
        "            m = (tb >= ts.min()) & (tb <= ts.max())\n",
        "            ax.plot(tb[m], np.asarray(xb)[m], lw=1.0)\n",
        "            ax.set_ylabel(bn); ax.grid(alpha=0.25)\n",
        "        axs[-1].set_xlabel(\"Time\")\n",
        "        fig.suptitle(f\"Driver inputs during forecast window — horizon {horizon} min\", y=0.98)\n",
        "        plt.tight_layout(rect=[0,0,1,0.96]); plt.show()\n",
        "\n",
        "print(\"\\nFinal selected lag channels (one per base):\")\n",
        "for nm in selected_lag_cols:\n",
        "    print(\"  \", nm)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZirWtk0PqWX6",
        "outputId": "f1623d3d-01a1-4dd9-866b-b75a15091cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "1c00c1ebe5754d399fcd21fc700cb947",
            "607e74f515e1446d889c6de334f94068",
            "cad9d60f06824fd8b678e9f69a19fc6f",
            "784dec59ef5b4859afdf42845b4fce96",
            "f5d6c135e0dc4f0b9be0d897f7fede83",
            "2467e95fcd174d5c809a1bba6ab803d1",
            "863a8033f90c49b3b88fb2cb2d33db5b",
            "78fbefb43c8b4a63acf2a2274164e0d2",
            "a53cfce0e3f5472284b3ab207a4bba97",
            "55e343d0787d4f6c968556740c62f1b3",
            "c8547b3f555747ff86175661af305788"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Patch failed: 'ipynb'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "STDMET years:   0%|          | 0/18 [00:00<?, ?yr/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c00c1ebe5754d399fcd21fc700cb947"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4011695380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m \u001b[0mdf_stdmet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_stdmet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYEARS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;31m# -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4011695380.py\u001b[0m in \u001b[0;36mbuild_stdmet\u001b[0;34m(years)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_stdmet_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mdfy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_stdmet_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdfy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4011695380.py\u001b[0m in \u001b[0;36mparse_stdmet_text\u001b[0;34m(txt)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mhh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UTC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP (Colab): ensure metadata.widgets.state exists so GitHub can render\n",
        "# Run once before \"Save a copy in GitHub…\", then comment out.\n",
        "import json, os, time\n",
        "patched_path = \"/content/notebook_patched.ipynb\"\n",
        "\n",
        "def _ensure_state(nb):\n",
        "    nb.setdefault(\"metadata\", {}).setdefault(\"widgets\", {}).setdefault(\"state\", {})\n",
        "    return nb\n",
        "\n",
        "try:\n",
        "    # 1) Ask Colab for the live notebook JSON\n",
        "    from google.colab import _message, notebook as colab_notebook\n",
        "    # Force-save first so frontend state is current\n",
        "    try:\n",
        "        colab_notebook.save(\"\")\n",
        "        time.sleep(0.5)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    resp = _message.blocking_request(\"get_ipynb\", {})  # typical key: 'ipynb'\n",
        "    nb = resp.get(\"ipynb\") or resp.get(\"notebook\") or resp  # fallback for edge cases\n",
        "    if not isinstance(nb, dict):\n",
        "        raise RuntimeError(\"Colab did not return a notebook dict\")\n",
        "\n",
        "    _ensure_state(nb)\n",
        "\n",
        "    # 2) Try to push patched JSON back into the running doc (so the GitHub saver uses it)\n",
        "    _message.blocking_request(\"set_ipynb\", {\"ipynb\": nb})\n",
        "    print(\"✅ Patched in-memory notebook metadata. Now do: File → Save a copy in GitHub…\")\n",
        "\n",
        "    # 3) Also write a patched copy to disk as a fallback\n",
        "    with open(patched_path, \"w\") as f:\n",
        "        json.dump(nb, f, ensure_ascii=False)\n",
        "    print(f\"💾 Fallback copy written to {patched_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    # If Colab API is locked down, at least give you a patched file to upload manually\n",
        "    print(\"⚠️ Could not patch in-memory notebook. Writing a patched local copy instead.\")\n",
        "    try:\n",
        "        # Minimal skeleton with just the metadata fix if we can't read live JSON\n",
        "        # (still preserves GitHub rendering; cells won't be included in this extreme fallback)\n",
        "        nb = {\"cells\": [], \"metadata\": {\"widgets\": {\"state\": {}}}, \"nbformat\": 4, \"nbformat_minor\": 5}\n",
        "        with open(patched_path, \"w\") as f:\n",
        "            json.dump(nb, f, ensure_ascii=False)\n",
        "        print(f\"💾 Wrote {patched_path}. If needed, upload this file to GitHub manually.\")\n",
        "    except Exception as ee:\n",
        "        print(\"❌ Patch failed:\", ee)\n"
      ],
      "metadata": {
        "id": "jY_ECK5CfzFj",
        "outputId": "0c66a625-f285-48f9-fba0-63b2b5605e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not patch in-memory notebook. Writing a patched local copy instead.\n",
            "💾 Wrote /content/notebook_patched.ipynb. If needed, upload this file to GitHub manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PBI_5stflpeA"
      }
    }
  ]
}