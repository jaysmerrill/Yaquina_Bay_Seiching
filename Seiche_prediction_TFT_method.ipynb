{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyONkhJLZm6ChV3oHVNd6Qb/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaysmerrill/Yaquina_Bay_Seiching/blob/main/Seiche_prediction_TFT_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88PIUgNk6u5R",
        "outputId": "1e3a56ce-a6d3-4558-b91d-a5eef8c9c125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_seq coverage: 2008-03-05 17:00:00 → 2023-04-17 04:36:00 (n=1,269,412)\n",
            "46050 raw coverage: 2008-03-05 17:50:00 → 2024-12-31 23:50:00 (n=470,547)\n",
            "46050 yearly fetch status (tail): [(2015, 'OK 8472'), (2016, 'OK 8688'), (2017, 'OK 28478'), (2018, 'OK 52403'), (2019, 'OK 52055'), (2020, 'OK 52211'), (2021, 'OK 51561'), (2022, 'OK 52531'), (2023, 'OK 52486'), (2024, 'OK 52695')]\n",
            "9435380 NOAA coverage: 2008-01-01 00:00:00 → 2025-01-01 23:00:00 (n=138,914)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 46050 ingest -> align -> (optional) unified mask -> XGB top-10 (1/base) -> Mini-TFT\n",
        "# New in this drop-in:\n",
        "#   • Fill NOAA 9435380 hourly wind gaps using HMSC monthly .dat files\n",
        "#     - Parse HMSC (skip preamble, read from TIMESTAMP row)\n",
        "#     - Convert PST(UTC-8) -> UTC by +8h\n",
        "#     - Convert AWS mph -> m/s; AWD deg\n",
        "#     - Build u/v (meteorological from-direction), resample 5-min -> hourly (vector mean)\n",
        "#     - Fill ONLY missing 9435380 hourly values with HMSC hourly values\n",
        "#   • No gust anywhere (neither NOAA nor HMSC)\n",
        "# ============================================================\n",
        "import io, gzip, requests, re, calendar\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "from contextlib import nullcontext\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# -------------------\n",
        "# USER KNOBS\n",
        "# -------------------\n",
        "DATA_PATH           = \"/content/input_matrix_for_ML_AI_applications.parquet\"\n",
        "YEARS               = list(range(2008, 2025))\n",
        "DT_MINUTES          = 6\n",
        "MAX_GAP_HOURS       = 5\n",
        "USE_UNIFIED_GAP_MASK= False   # keep False to avoid over-masking\n",
        "HISTORY_HOURS       = 12\n",
        "HORIZONS_MIN        = [60, 120, 240]\n",
        "PRIMARY_HORIZON     = 60\n",
        "\n",
        "# NOAA wind (CO-OPS) fetch window (chunked) — baseline to be filled by HMSC\n",
        "NOAA_BEGIN_DATE     = \"20080101\"\n",
        "NOAA_END_DATE       = \"20250101\"\n",
        "NOAA_STATION        = \"9435380\"\n",
        "NOAA_CHUNK_DAYS     = 365\n",
        "\n",
        "# HMSC monthly archive (used to backfill gaps)\n",
        "HMSC_BASE_URL       = \"http://weather.hmsc.oregonstate.edu/weather/weatherproject/archive/{yyyy}/HMSC_{yyyymm}.dat\"\n",
        "HMSC_UTC_OFFSET_H   = +8  # PST → UTC = +8h (per user)\n",
        "\n",
        "# lag caps (minutes)\n",
        "NON_EQ_MAX_LAG_MIN  = 4 * 60\n",
        "EQ_MAX_LAG_MIN      = 24 * 60\n",
        "\n",
        "# top-k lagged channels: ONE per base\n",
        "TOP_LAG_FEATURES    = 12\n",
        "PER_BASE_CAP        = 1\n",
        "\n",
        "# spike emphasis on target\n",
        "WEIGHT_SPIKES       = True\n",
        "SPIKE_P90           = 0.90\n",
        "SPIKE_WEIGHT        = 2.0\n",
        "\n",
        "# time splits\n",
        "TRAIN_END           = pd.Timestamp(\"2018-12-31 23:59:59\")\n",
        "VAL_END             = pd.Timestamp(\"2022-12-31 23:59:59\")\n",
        "MAX_SAMPLES_SPLIT   = 60_000\n",
        "\n",
        "# ---- Plotting controls ----\n",
        "PRED_SPLIT          = \"test\"     # \"test\" or \"val\"\n",
        "PRED_START          = pd.Timestamp(\"2022-01-01 00:00:00\")\n",
        "PRED_END            = pd.Timestamp(\"2022-01-31 23:59:59\")\n",
        "PLOT_LAST_DAYS      = 40\n",
        "\n",
        "# TFT hyperparams\n",
        "BATCH_SIZE          = 512\n",
        "EPOCHS              = 6\n",
        "LR                  = 2e-3\n",
        "DROPOUT             = 0.2\n",
        "D_MODEL             = 128\n",
        "NHEAD               = 4\n",
        "LSTM_LAYERS         = 1\n",
        "FF_DIM              = 256\n",
        "\n",
        "# -------------------\n",
        "# Load df_seq if needed\n",
        "# -------------------\n",
        "if DATA_PATH is not None and \"df_seq\" not in globals():\n",
        "    if DATA_PATH.lower().endswith(\".parquet\"):\n",
        "        df_seq = pd.read_parquet(DATA_PATH)\n",
        "    else:\n",
        "        df_seq = pd.read_csv(DATA_PATH)\n",
        "\n",
        "assert \"df_seq\" in globals(), \"Please define df_seq or set DATA_PATH to your parquet/csv.\"\n",
        "assert {\"time\",\"var1h\"}.issubset(df_seq.columns), \"df_seq must include 'time' and 'var1h'.\"\n",
        "\n",
        "df_seq[\"time\"] = pd.to_datetime(df_seq[\"time\"])\n",
        "df_seq = df_seq.sort_values(\"time\").reset_index(drop=True)\n",
        "for c in df_seq.columns:\n",
        "    if c != \"time\":\n",
        "        df_seq[c] = pd.to_numeric(df_seq[c], errors=\"coerce\").astype(\"float32\")\n",
        "\n",
        "# -------------------\n",
        "# Robust HTTP session\n",
        "# -------------------\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def _make_session():\n",
        "    s = requests.Session()\n",
        "    retry = Retry(total=4, backoff_factor=0.5, status_forcelist=(429,500,502,503,504),\n",
        "                  allowed_methods=[\"HEAD\",\"GET\",\"OPTIONS\"], raise_on_status=False)\n",
        "    s.headers.update({\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) ColabFetcher/1.0\", \"Accept\":\"*/*\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
        "    return s\n",
        "\n",
        "session = _make_session()\n",
        "\n",
        "# -------------------\n",
        "# NDBC 46050 downloader + robust parser  (Hs/DPD/APD/MWD only)\n",
        "# -------------------\n",
        "YEARS = [y for y in YEARS if y < pd.Timestamp.utcnow().year]\n",
        "DIRECT_PAT = \"https://www.ndbc.noaa.gov/data/historical/stdmet/46050h{year}.txt.gz\"\n",
        "PHP_PAT    = \"https://www.ndbc.noaa.gov/view_text_file.php?filename=46050h{year}.txt.gz&dir=data/historical/stdmet/\"\n",
        "\n",
        "def fetch_year_text(year:int, session:requests.Session)->str|None:\n",
        "    for url in (DIRECT_PAT.format(year=year), PHP_PAT.format(year=year)):\n",
        "        try:\n",
        "            r = session.get(url, timeout=30)\n",
        "            if r.status_code != 200 or not r.content: continue\n",
        "            ctype = r.headers.get(\"Content-Type\",\"\").lower()\n",
        "            data = r.content\n",
        "            try:\n",
        "                with gzip.GzipFile(fileobj=io.BytesIO(data)) as gz:\n",
        "                    return gz.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "            except OSError:\n",
        "                if \"html\" in ctype or data[:15].lower().startswith(b\"<!doctype html\"):\n",
        "                    continue\n",
        "                try:\n",
        "                    return data.decode(\"utf-8\", errors=\"ignore\")\n",
        "                except Exception:\n",
        "                    continue\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def _ndbc_to_float(tok:str) -> float:\n",
        "    s = str(tok).strip()\n",
        "    if s == \"\" or s.upper() == \"MM\":\n",
        "        return np.nan\n",
        "    if re.fullmatch(r\"-?9+(\\.0+)?\", s):  # 99, 9999, 99.0, etc.\n",
        "        return np.nan\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def parse_ndbc_stdmet_text(txt:str)->pd.DataFrame:\n",
        "    rows = []\n",
        "    for ln in txt.splitlines():\n",
        "        if not ln or ln.lstrip().startswith(\"#\"): continue\n",
        "        if not re.match(r\"^\\s*(\\d{2}|\\d{4})\\s+\\d{1,2}\\s+\\d{1,2}\\s+\\d{1,2}\\s+\\d{1,2}\\s\", ln):\n",
        "            continue\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        y,mo,dy,hh,mi = parts[:5]\n",
        "        try:\n",
        "            y = int(y); mo=int(mo); dy=int(dy); hh=int(hh); mi=int(mi)\n",
        "            year = 2000 + y if y < 100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, minute=mi, tz=\"UTC\").tz_localize(None)\n",
        "        except Exception:\n",
        "            continue\n",
        "        vals = parts[5:]\n",
        "        if len(vals) < 13: vals = (vals + [\"MM\"]*13)[:13]\n",
        "        vals = [_ndbc_to_float(v) for v in vals]\n",
        "        rows.append([ts] + vals)\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "    cols = [\"time\",\"WDIR\",\"WSPD\",\"GST\",\"WVHT\",\"DPD\",\"APD\",\"MWD\",\"PRES\",\"ATMP\",\"WTMP\",\"DEWP\",\"VIS\",\"TIDE\"]\n",
        "    df = pd.DataFrame(rows, columns=cols).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    df = df.rename(columns={\"WVHT\":\"Hs\"})\n",
        "    keep = [\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"]\n",
        "    for k in keep:\n",
        "        if k not in df.columns: df[k] = np.nan\n",
        "    return df[keep].reset_index(drop=True)\n",
        "\n",
        "frames, attempt_log = [], []\n",
        "for y in YEARS:\n",
        "    txt = fetch_year_text(y, session)\n",
        "    if not txt: attempt_log.append((y, \"Missing/HTML/404\")); continue\n",
        "    dfy = parse_ndbc_stdmet_text(txt)\n",
        "    if dfy.empty: attempt_log.append((y, \"Parsed 0 rows\")); continue\n",
        "    frames.append(dfy); attempt_log.append((y, f\"OK {len(dfy)}\"))\n",
        "if not frames:\n",
        "    print(\"No 46050 data parsed. Year statuses:\", attempt_log)\n",
        "    raise RuntimeError(\"46050 download failed.\")\n",
        "df_46050 = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "# Clean buoy values pre-align\n",
        "for col, thresh in [(\"Hs\", 50.0), (\"DPD\", 50.0), (\"APD\", 50.0)]:\n",
        "    if col in df_46050.columns:\n",
        "        m = df_46050[col] > thresh\n",
        "        if m.any():\n",
        "            print(f\"Clean[46050 pre]: set {int(m.sum())} '{col}' values >{thresh} to NaN (e.g., 99 sentinel).\")\n",
        "            df_46050.loc[m, col] = np.nan\n",
        "if \"MWD\" in df_46050.columns:\n",
        "    m = df_46050[\"MWD\"] > 360\n",
        "    if m.any():\n",
        "        print(f\"Clean[46050 pre]: set {int(m.sum())} 'MWD' values >360 to NaN.\")\n",
        "        df_46050.loc[m, \"MWD\"] = np.nan\n",
        "\n",
        "print(\"df_seq coverage:\", df_seq[\"time\"].min(), \"→\", df_seq[\"time\"].max(), f\"(n={len(df_seq):,})\")\n",
        "print(\"46050 raw coverage:\", df_46050[\"time\"].min(), \"→\", df_46050[\"time\"].max(), f\"(n={len(df_46050):,})\")\n",
        "print(\"46050 yearly fetch status (tail):\", attempt_log[-10:])\n",
        "\n",
        "# -------------------\n",
        "# NOAA CO-OPS 9435380 (hourly wind) — chunked\n",
        "# -------------------\n",
        "def parse_noaa_wind_csv(txt:str)->pd.DataFrame:\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    time_cols = [c for c in df.columns if str(c).strip().lower() in (\"t\",\"time\",\"date time\",\"date_time\",\"date\")]\n",
        "    time_col = time_cols[0] if time_cols else df.columns[0]\n",
        "    df[\"time\"] = pd.to_datetime(df[time_col], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "\n",
        "    def _find(colnames, keys):\n",
        "        keys = [k.lower() for k in keys]\n",
        "        for c in colnames:\n",
        "            s = str(c).lower()\n",
        "            if any(k == s or k in s for k in keys):\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    sp_col = _find(df.columns, [\"s\",\"speed\"])   # m/s\n",
        "    dir_col = _find(df.columns, [\"d\",\"dir\",\"direction\"])\n",
        "\n",
        "    out = pd.DataFrame({\"time\": df[\"time\"]})\n",
        "    out[\"wind9435380_speed\"] = pd.to_numeric(df[sp_col], errors=\"coerce\") if sp_col else np.nan\n",
        "    out[\"wind9435380_dir\"]   = pd.to_numeric(df[dir_col], errors=\"coerce\") if dir_col else np.nan\n",
        "    # Sanity\n",
        "    out.loc[(out[\"wind9435380_dir\"] < 0) | (out[\"wind9435380_dir\"] > 360), \"wind9435380_dir\"] = np.nan\n",
        "    out.loc[(out[\"wind9435380_speed\"] < 0) | (out[\"wind9435380_speed\"] > 100), \"wind9435380_speed\"] = np.nan\n",
        "\n",
        "    th = np.deg2rad(out[\"wind9435380_dir\"].to_numpy(dtype=float))\n",
        "    s  = out[\"wind9435380_speed\"].to_numpy(dtype=float)\n",
        "    u = -s * np.sin(th); v = -s * np.cos(th)\n",
        "    out[\"wind9435380_u\"] = u.astype(np.float32)\n",
        "    out[\"wind9435380_v\"] = v.astype(np.float32)\n",
        "\n",
        "    out = out.sort_values(\"time\").dropna(subset=[\"time\"]).drop_duplicates(\"time\")\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def _datestr(dt) -> str:\n",
        "    return pd.Timestamp(dt).strftime(\"%Y%m%d\")\n",
        "\n",
        "def fetch_noaa_wind_dataframe(begin_date:str, end_date:str, station:str, days_per_chunk:int=365) -> pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\")\n",
        "    end   = pd.to_datetime(end_date,   format=\"%Y%m%d\")\n",
        "    frames = []\n",
        "    cur = start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        url = (\n",
        "            \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "            f\"?begin_date={_datestr(cur)}&end_date={_datestr(chunk_end)}\"\n",
        "            f\"&station={station}&product=wind&time_zone=gmt&interval=h&units=metric\"\n",
        "            \"&application=DataAPI_Sample&format=csv\"\n",
        "        )\n",
        "        try:\n",
        "            r = session.get(url, timeout=60)\n",
        "            if r.status_code == 200 and r.content:\n",
        "                dfc = parse_noaa_wind_csv(r.content.decode(\"utf-8\", errors=\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "            else:\n",
        "                print(f\"NOAA wind chunk {cur:%Y-%m-%d}→{chunk_end:%Y-%m-%d} failed:\", r.status_code)\n",
        "        except Exception as e:\n",
        "            print(f\"NOAA wind chunk error {cur:%Y-%m-%d}→{chunk_end:%Y-%m-%d}:\", e)\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"time\",\"wind9435380_speed\",\"wind9435380_dir\",\"wind9435380_u\",\"wind9435380_v\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "df_noaa = fetch_noaa_wind_dataframe(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_STATION, days_per_chunk=NOAA_CHUNK_DAYS)\n",
        "print(\"9435380 NOAA coverage:\",\n",
        "      (df_noaa[\"time\"].min() if len(df_noaa) else None), \"→\",\n",
        "      (df_noaa[\"time\"].max() if len(df_noaa) else None), f\"(n={len(df_noaa):,})\")\n",
        "\n",
        "# -------------------\n",
        "# HMSC monthly .dat fetch + parse + PST->UTC + mph->m/s + u/v + hourly resample\n",
        "# -------------------\n",
        "def fetch_hmsc_month_text(year:int, month:int) -> str|None:\n",
        "    yyyymm = f\"{year}{month:02d}\"\n",
        "    url = HMSC_BASE_URL.format(yyyy=year, yyyymm=yyyymm)\n",
        "    try:\n",
        "        r = session.get(url, timeout=60)\n",
        "        if r.status_code == 200 and r.content:\n",
        "            return r.content.decode(\"utf-8\", errors=\"ignore\")\n",
        "        else:\n",
        "            return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_hmsc_dat(txt:str) -> pd.DataFrame:\n",
        "    # Find the header line that starts with TIMESTAMP\n",
        "    lines = txt.splitlines()\n",
        "    start_idx = None\n",
        "    for i, ln in enumerate(lines):\n",
        "        s = ln.strip().strip('\"')\n",
        "        if s.startswith(\"TIMESTAMP,\") or s.startswith(\"TIMESTAMP,RECORD\") or s.startswith(\"TIMESTAMP\"):\n",
        "            start_idx = i\n",
        "            break\n",
        "    if start_idx is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Reconstruct CSV content from header onward\n",
        "    content = \"\\n\".join(lines[start_idx:])\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "\n",
        "    # Drop the immediately following \"units/labels\" rows where TIMESTAMP is TS or empty\n",
        "    # Keep rows that look like actual datetimes.\n",
        "    def _try_time(x):\n",
        "        try:\n",
        "            return pd.to_datetime(x)\n",
        "        except Exception:\n",
        "            return pd.NaT\n",
        "    tt = df.iloc[:,0].apply(_try_time)\n",
        "    df = df.loc[tt.notna()].copy()\n",
        "    df.rename(columns={df.columns[0]:\"TIMESTAMP\"}, inplace=True)\n",
        "\n",
        "    # Keep AWS (avg wind speed, mph) and AWD (avg wind direction, deg)\n",
        "    # Columns names appear exactly as AWS / AWD in sample.\n",
        "    if \"AWS\" not in df.columns or \"AWD\" not in df.columns:\n",
        "        # Try case-insensitive match\n",
        "        cols_lower = {c.lower(): c for c in df.columns}\n",
        "        AWS_col = cols_lower.get(\"aws\")\n",
        "        AWD_col = cols_lower.get(\"awd\")\n",
        "    else:\n",
        "        AWS_col, AWD_col = \"AWS\", \"AWD\"\n",
        "\n",
        "    if AWS_col is None or AWD_col is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    out = pd.DataFrame()\n",
        "    # Local time (PST), convert to UTC by +8h (per user)\n",
        "    out[\"time\"] = pd.to_datetime(df[\"TIMESTAMP\"], errors=\"coerce\") + pd.Timedelta(hours=HMSC_UTC_OFFSET_H)\n",
        "\n",
        "    # Convert mph -> m/s\n",
        "    mph = pd.to_numeric(df[AWS_col], errors=\"coerce\")\n",
        "    spd_ms = mph * 0.44704\n",
        "\n",
        "    # Direction deg (meteorological FROM)\n",
        "    dir_deg = pd.to_numeric(df[AWD_col], errors=\"coerce\")\n",
        "    dir_deg[(dir_deg < 0) | (dir_deg > 360)] = np.nan\n",
        "\n",
        "    out[\"hmsc_speed_ms\"] = spd_ms.astype(\"float32\")\n",
        "    out[\"hmsc_dir_deg\"]  = dir_deg.astype(\"float32\")\n",
        "\n",
        "    # u/v components (meteorological FROM)\n",
        "    th = np.deg2rad(out[\"hmsc_dir_deg\"].to_numpy(dtype=float))\n",
        "    s  = out[\"hmsc_speed_ms\"].to_numpy(dtype=float)\n",
        "    u = -s * np.sin(th); v = -s * np.cos(th)\n",
        "    out[\"hmsc_u\"] = u.astype(np.float32)\n",
        "    out[\"hmsc_v\"] = v.astype(np.float32)\n",
        "\n",
        "    out = out.sort_values(\"time\").dropna(subset=[\"time\"]).drop_duplicates(\"time\")\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def month_iter(start:pd.Timestamp, end:pd.Timestamp):\n",
        "    y, m = start.year, start.month\n",
        "    while pd.Timestamp(year=y, month=m, day=1) <= end:\n",
        "        yield (y, m)\n",
        "        if m == 12: y, m = y+1, 1\n",
        "        else: m += 1\n",
        "\n",
        "def fetch_hmsc_range(start:pd.Timestamp, end:pd.Timestamp) -> pd.DataFrame:\n",
        "    frames=[]\n",
        "    for y,m in month_iter(start.normalize(), end.normalize()):\n",
        "        txt = fetch_hmsc_month_text(y, m)\n",
        "        if not txt: continue\n",
        "        dfm = parse_hmsc_dat(txt)\n",
        "        if not dfm.empty:\n",
        "            # Keep only within [start-1d, end+1d] to be safe\n",
        "            dfm = dfm[(dfm[\"time\"] >= start - pd.Timedelta(days=1)) &\n",
        "                      (dfm[\"time\"] <= end   + pd.Timedelta(days=1))]\n",
        "            if len(dfm): frames.append(dfm)\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"time\",\"hmsc_speed_ms\",\"hmsc_dir_deg\",\"hmsc_u\",\"hmsc_v\"])\n",
        "    df = pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# -------------------\n",
        "# Build hourly NOAA baseline and fill its gaps from HMSC\n",
        "# -------------------\n",
        "def resample_hmsc_hourly(df_hmsc:pd.DataFrame) -> pd.DataFrame:\n",
        "    if df_hmsc.empty:\n",
        "        return df_hmsc\n",
        "    g = df_hmsc.set_index(\"time\").sort_index()\n",
        "\n",
        "    # Vector mean per hour\n",
        "    hourly_u = g[\"hmsc_u\"].resample(\"H\").mean()\n",
        "    hourly_v = g[\"hmsc_v\"].resample(\"H\").mean()\n",
        "    # Direction-from in deg\n",
        "    th = np.degrees(np.arctan2(-hourly_u.to_numpy(), -hourly_v.to_numpy())) % 360.0\n",
        "    # Vector-mean speed\n",
        "    spd = np.sqrt(hourly_u.to_numpy()**2 + hourly_v.to_numpy()**2)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"time\": hourly_u.index,\n",
        "        \"hmsc_u\": hourly_u.values.astype(\"float32\"),\n",
        "        \"hmsc_v\": hourly_v.values.astype(\"float32\"),\n",
        "        \"hmsc_speed_ms\": spd.astype(\"float32\"),\n",
        "        \"hmsc_dir_deg\": th.astype(\"float32\"),\n",
        "    }).dropna(subset=[\"time\"])\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "# NOAA hourly baseline\n",
        "df_noaa_hourly = df_noaa.copy()\n",
        "# Ensure exact hourly index (some responses are already hourly)\n",
        "if not df_noaa_hourly.empty:\n",
        "    df_noaa_hourly[\"time\"] = pd.to_datetime(df_noaa_hourly[\"time\"])\n",
        "    df_noaa_hourly = df_noaa_hourly.sort_values(\"time\").drop_duplicates(\"time\")\n",
        "\n",
        "# Define the time span we need for filling: use df_noaa span; if empty, use df_seq/df_46050 overlap\n",
        "if len(df_noaa_hourly):\n",
        "    fill_start, fill_end = df_noaa_hourly[\"time\"].min(), df_noaa_hourly[\"time\"].max()\n",
        "else:\n",
        "    fill_start = max(df_seq[\"time\"].min(), df_46050[\"time\"].min())\n",
        "    fill_end   = min(df_seq[\"time\"].max(), df_46050[\"time\"].max())\n",
        "\n",
        "# Fetch HMSC for that span and build hourly series\n",
        "df_hmsc_raw = fetch_hmsc_range(fill_start, fill_end)\n",
        "print(\"HMSC raw coverage:\",\n",
        "      (df_hmsc_raw[\"time\"].min() if len(df_hmsc_raw) else None), \"→\",\n",
        "      (df_hmsc_raw[\"time\"].max() if len(df_hmsc_raw) else None), f\"(n={len(df_hmsc_raw):,})\")\n",
        "df_hmsc_hour = resample_hmsc_hourly(df_hmsc_raw)\n",
        "print(\"HMSC hourly coverage:\",\n",
        "      (df_hmsc_hour[\"time\"].min() if len(df_hmsc_hour) else None), \"→\",\n",
        "      (df_hmsc_hour[\"time\"].max() if len(df_hmsc_hour) else None), f\"(n={len(df_hmsc_hour):,})\")\n",
        "\n",
        "# Reindex NOAA to a perfect hourly grid over [fill_start..fill_end]\n",
        "hourly_grid = pd.date_range(fill_start.floor(\"H\"), fill_end.ceil(\"H\"), freq=\"H\")\n",
        "noaa = (df_noaa_hourly.set_index(\"time\")\n",
        "        .reindex(hourly_grid)\n",
        "        .rename_axis(\"time\")\n",
        "        .reset_index())\n",
        "noaa.rename(columns={\n",
        "    \"wind9435380_speed\":\"speed\",\n",
        "    \"wind9435380_dir\":\"dir\",\n",
        "    \"wind9435380_u\":\"u\",\n",
        "    \"wind9435380_v\":\"v\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Identify gaps\n",
        "gap_mask = noaa[\"speed\"].isna() | noaa[\"dir\"].isna() | noaa[\"u\"].isna() | noaa[\"v\"].isna()\n",
        "n_gaps = int(gap_mask.sum())\n",
        "print(f\"9435380 NOAA hourly NaN rows before fill: {n_gaps:,}\")\n",
        "\n",
        "# Build HMSC hourly aligned to same grid\n",
        "if len(df_hmsc_hour):\n",
        "    hmsc = (df_hmsc_hour.set_index(\"time\")\n",
        "            .reindex(hourly_grid)\n",
        "            .rename_axis(\"time\")\n",
        "            .reset_index())\n",
        "else:\n",
        "    hmsc = pd.DataFrame({\"time\": hourly_grid})\n",
        "    hmsc[\"hmsc_speed_ms\"] = np.nan\n",
        "    hmsc[\"hmsc_dir_deg\"]  = np.nan\n",
        "    hmsc[\"hmsc_u\"] = np.nan\n",
        "    hmsc[\"hmsc_v\"] = np.nan\n",
        "\n",
        "# Fill 9435380 gaps with HMSC (only where NOAA is NaN and HMSC has a value)\n",
        "fillable = gap_mask & hmsc[\"hmsc_speed_ms\"].notna() & hmsc[\"hmsc_dir_deg\"].notna() & hmsc[\"hmsc_u\"].notna() & hmsc[\"hmsc_v\"].notna()\n",
        "filled_rows = int(fillable.sum())\n",
        "noaa.loc[fillable, \"speed\"] = hmsc.loc[fillable, \"hmsc_speed_ms\"].values\n",
        "noaa.loc[fillable, \"dir\"]   = hmsc.loc[fillable, \"hmsc_dir_deg\"].values\n",
        "noaa.loc[fillable, \"u\"]     = hmsc.loc[fillable, \"hmsc_u\"].values\n",
        "noaa.loc[fillable, \"v\"]     = hmsc.loc[fillable, \"hmsc_v\"].values\n",
        "print(f\"Filled from HMSC: {filled_rows:,} hourly rows.\")\n",
        "print(f\"Remaining NOAA hourly NaN rows after fill: {int(noaa[['speed','dir','u','v']].isna().any(axis=1).sum()):,}\")\n",
        "\n",
        "# Final merged hourly wind series (named as 9435380 to keep downstream code simple)\n",
        "df_wind_filled = pd.DataFrame({\n",
        "    \"time\": noaa[\"time\"],\n",
        "    \"wind9435380_speed\": noaa[\"speed\"].astype(\"float32\"),\n",
        "    \"wind9435380_dir\":   noaa[\"dir\"].astype(\"float32\"),\n",
        "    \"wind9435380_u\":     noaa[\"u\"].astype(\"float32\"),\n",
        "    \"wind9435380_v\":     noaa[\"v\"].astype(\"float32\"),\n",
        "}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Merged hourly wind (NOAA+HMSC fill) coverage:\",\n",
        "      (df_wind_filled[\"time\"].min() if len(df_wind_filled) else None), \"→\",\n",
        "      (df_wind_filled[\"time\"].max() if len(df_wind_filled) else None), f\"(n={len(df_wind_filled):,})\")\n",
        "\n",
        "# -------------------\n",
        "# Align to 6-min grid (+ optional unified gap mask)\n",
        "# -------------------\n",
        "def interp_timegrid(t_src:pd.Series, x_src:np.ndarray, t_grid:pd.DatetimeIndex)->np.ndarray:\n",
        "    s = pd.Series(x_src, index=pd.to_datetime(t_src))\n",
        "    u = s.reindex(pd.to_datetime(sorted(set(s.index).union(set(t_grid)))))\n",
        "    u = u.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    return u.reindex(pd.to_datetime(t_grid)).to_numpy()\n",
        "\n",
        "# Define overlap grid across df_seq, 46050, and wind_filled\n",
        "t0 = max(\n",
        "    df_seq[\"time\"].min(),\n",
        "    df_46050[\"time\"].min(),\n",
        "    (df_wind_filled[\"time\"].min() if len(df_wind_filled) else df_seq[\"time\"].min())\n",
        ")\n",
        "t1 = min(\n",
        "    df_seq[\"time\"].max(),\n",
        "    df_46050[\"time\"].max(),\n",
        "    (df_wind_filled[\"time\"].max() if len(df_wind_filled) else df_seq[\"time\"].max())\n",
        ")\n",
        "t_grid = pd.date_range(t0, t1, freq=f\"{DT_MINUTES}min\")\n",
        "print(\"Common grid (pre-mask):\", t0, \"→\", t1, f\"(n={len(t_grid):,})\")\n",
        "\n",
        "# Build matrix for df_seq bases\n",
        "seq_cols = [c for c in df_seq.columns if c != \"time\"]\n",
        "seq_mat = {c: interp_timegrid(df_seq[\"time\"], df_seq[c].values, t_grid) for c in seq_cols}\n",
        "\n",
        "# Add ONLY wave params from 46050 (Hs/DPD/APD/MWD)\n",
        "for c in [\"Hs\",\"DPD\",\"APD\",\"MWD\"]:\n",
        "    seq_mat[c] = interp_timegrid(df_46050[\"time\"], df_46050[c].values, t_grid)\n",
        "\n",
        "# Add wind (no gust)\n",
        "if len(df_wind_filled):\n",
        "    for c in [\"wind9435380_speed\",\"wind9435380_dir\",\"wind9435380_u\",\"wind9435380_v\"]:\n",
        "        seq_mat[c] = interp_timegrid(df_wind_filled[\"time\"], df_wind_filled[c].values, t_grid)\n",
        "\n",
        "def gap_intervals(t:pd.Series, x:np.ndarray, min_gap_hours:float)->List[Tuple[pd.Timestamp,pd.Timestamp]]:\n",
        "    t = pd.to_datetime(t)\n",
        "    dt = np.diff(t.values).astype(\"timedelta64[s]\").astype(int)\n",
        "    jumps = np.where(dt > min_gap_hours*3600)[0]\n",
        "    iv = [(t[j], t[j+1]) for j in jumps]\n",
        "    s = pd.Series(x, index=t)\n",
        "    isn = s.isna().to_numpy()\n",
        "    if isn.any():\n",
        "        starts = np.where(np.diff(np.r_[False, isn])==1)[0]\n",
        "        ends   = np.where(np.diff(np.r_[isn, False])==-1)[0]\n",
        "        for a,b in zip(starts, ends):\n",
        "            if (t[b]-t[a]) >= pd.Timedelta(hours=min_gap_hours):\n",
        "                iv.append((t[a], t[b]))\n",
        "    return iv\n",
        "\n",
        "def merge_intervals(intervals):\n",
        "    if not intervals: return []\n",
        "    z = sorted(intervals, key=lambda k: k[0]); out = [list(z[0])]\n",
        "    for s,e in z[1:]:\n",
        "        if s <= out[-1][1]: out[-1][1] = max(out[-1][1], e)\n",
        "        else: out.append([s,e])\n",
        "    return [(pd.to_datetime(a), pd.to_datetime(b)) for a,b in out]\n",
        "\n",
        "if USE_UNIFIED_GAP_MASK:\n",
        "    all_intervals = []\n",
        "    for c in seq_cols:\n",
        "        all_intervals += gap_intervals(df_seq[\"time\"], df_seq[c].values, MAX_GAP_HOURS)\n",
        "    for c in [\"Hs\",\"DPD\",\"APD\",\"MWD\"]:\n",
        "        all_intervals += gap_intervals(df_46050[\"time\"], df_46050[c].values, MAX_GAP_HOURS)\n",
        "    if len(df_wind_filled):\n",
        "        for c in [\"wind9435380_speed\",\"wind9435380_dir\",\"wind9435380_u\",\"wind9435380_v\"]:\n",
        "            all_intervals += gap_intervals(df_wind_filled[\"time\"], df_wind_filled[c].values, MAX_GAP_HOURS)\n",
        "    merged_gaps = merge_intervals(all_intervals)\n",
        "    gap_mask = np.zeros(len(t_grid), dtype=bool)\n",
        "    for s,e in merged_gaps:\n",
        "        gap_mask |= (t_grid>=s) & (t_grid<=e)\n",
        "else:\n",
        "    gap_mask = np.zeros(len(t_grid), dtype=bool)\n",
        "\n",
        "df_all = pd.DataFrame({\"time\": t_grid})\n",
        "for c,v in seq_mat.items():\n",
        "    df_all[c] = v\n",
        "df_all = df_all.loc[~gap_mask].reset_index(drop=True)\n",
        "\n",
        "# Post-align cleaning + interpolation (guarantee no >50 in plots)\n",
        "for col, thresh in [(\"Hs\", 50.0), (\"DPD\", 50.0), (\"APD\", 50.0)]:\n",
        "    if col in df_all.columns:\n",
        "        m = df_all[col] > thresh\n",
        "        if m.any():\n",
        "            print(f\"Clean[post-align]: set {int(m.sum())} '{col}' values >{thresh} to NaN.\")\n",
        "            df_all.loc[m, col] = np.nan\n",
        "        df_all[col] = pd.Series(df_all[col].values, index=df_all[\"time\"]).interpolate(\n",
        "            method=\"time\", limit_area=\"inside\"\n",
        "        ).values\n",
        "\n",
        "if USE_UNIFIED_GAP_MASK:\n",
        "    full_grid_len = len(pd.date_range(t0, t1, freq=f\"{DT_MINUTES}min\"))\n",
        "    print(\"Aligned coverage (after unified mask):\",\n",
        "          df_all[\"time\"].min(), \"→\", df_all[\"time\"].max(), f\"(n={len(df_all):,})\")\n",
        "    print(\"Masked out grid points due to gaps:\",\n",
        "          f\"{full_grid_len - len(df_all):,}\")\n",
        "else:\n",
        "    print(\"Aligned coverage (mask disabled):\",\n",
        "          df_all[\"time\"].min(), \"→\", df_all[\"time\"].max(), f\"(n={len(df_all):,})\")\n",
        "\n",
        "# -------------------\n",
        "# Feature engineering (wl_gradient only)\n",
        "# -------------------\n",
        "if \"wl_low\" in df_all.columns:\n",
        "    wl = pd.Series(df_all[\"wl_low\"].values, index=df_all[\"time\"])\n",
        "    d  = wl.diff()\n",
        "    kph = int(60/DT_MINUTES)\n",
        "    df_all[\"wl_gradient\"] = (d * kph).astype(\"float32\").to_numpy()\n",
        "else:\n",
        "    df_all[\"wl_gradient\"] = np.nan\n",
        "\n",
        "# -------------------\n",
        "# Lag candidates\n",
        "# -------------------\n",
        "def is_eq_base(name: str) -> bool:\n",
        "    n = name.lower()\n",
        "    return n.startswith(\"eq\") or (\"eq_\" in n)\n",
        "\n",
        "def build_lag_candidates(df: pd.DataFrame,\n",
        "                         bases: List[str],\n",
        "                         dt_minutes: int) -> Tuple[pd.DataFrame, List[str], Dict[str,str], Dict[str,int]]:\n",
        "    lag_cols, series = [], {}\n",
        "    base_map, lag_minutes_map = {}, {}\n",
        "    for b in bases:\n",
        "        x = df[b].astype(\"float32\").to_numpy()\n",
        "        if not np.isfinite(x).any():  continue\n",
        "        max_lag_min = EQ_MAX_LAG_MIN if is_eq_base(b) else NON_EQ_MAX_LAG_MIN\n",
        "        grid_min = list(range(0, max_lag_min+1, int(30)))\n",
        "        for L in grid_min:\n",
        "            steps = int(round(L / dt_minutes))\n",
        "            nm = f\"{b}__lag_{L}min\"\n",
        "            lag_minutes_map[nm] = L\n",
        "            base_map[nm] = b\n",
        "            if steps == 0:\n",
        "                series[nm] = x\n",
        "            else:\n",
        "                s = pd.Series(x); series[nm] = s.shift(steps).to_numpy(dtype=\"float32\")\n",
        "            lag_cols.append(nm)\n",
        "    lag_df = pd.DataFrame(series).reindex(range(len(df))).reset_index(drop=True)\n",
        "    return lag_df, lag_cols, base_map, lag_minutes_map\n",
        "\n",
        "# Candidate bases: all numeric columns, excluding a few\n",
        "exclude_cols = set([\n",
        "    \"time\",\"var1h\",\"var1h_log1p\",\"wsp\",\"wdir\",\"gust\",\"atmvar1_boost\"  # gust excluded\n",
        "])\n",
        "cand_bases = [c for c in df_all.columns if (c not in exclude_cols and df_all[c].dtype != object)]\n",
        "lag_df, lag_cols, base_map, lag_minutes_map = build_lag_candidates(df_all, cand_bases, DT_MINUTES)\n",
        "\n",
        "# -------------------\n",
        "# XGB ranking (1 best lag per base)\n",
        "# -------------------\n",
        "from xgboost import XGBRegressor\n",
        "h_steps = max(1, int(round(PRIMARY_HORIZON/DT_MINUTES)))\n",
        "tgt = df_all[\"var1h\"].shift(-h_steps).reset_index(drop=True).astype(\"float32\")\n",
        "rank_data = pd.concat([lag_df, tgt.rename(\"target\")], axis=1).dropna().reset_index(drop=True)\n",
        "if rank_data.empty: raise RuntimeError(\"No data available for XGB ranking (after lagging and dropna).\")\n",
        "\n",
        "X_rank = rank_data[lag_cols].to_numpy(dtype=np.float32)\n",
        "y_rank = rank_data[\"target\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=800, learning_rate=0.03, max_depth=5,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "    objective=\"reg:squarederror\", n_jobs=-1, random_state=42\n",
        ")\n",
        "split = max(1, int(0.8*len(X_rank)))\n",
        "xgb.fit(X_rank[:split], y_rank[:split], verbose=False)\n",
        "importances = dict(zip(lag_cols, xgb.feature_importances_))\n",
        "\n",
        "best_lag_per_base = {}\n",
        "for nm, imp in importances.items():\n",
        "    b = base_map[nm]\n",
        "    prev = best_lag_per_base.get(b)\n",
        "    if (prev is None) or (imp > prev[1]): best_lag_per_base[b] = (nm, float(imp))\n",
        "best_list = [(b, nm, imp) for b,(nm,imp) in best_lag_per_base.items()]\n",
        "best_list.sort(key=lambda t: t[2], reverse=True)\n",
        "best_list = best_list[:TOP_LAG_FEATURES]\n",
        "\n",
        "selected_lag_cols = [nm for _, nm, _ in best_list]\n",
        "selected_bases    = [b  for b, _, _ in best_list]\n",
        "print(\"\\nSelected ONE best lag per base:\")\n",
        "for b, nm, imp in best_list:\n",
        "    print(f\"  {nm:<30s}  (base={b}, importance={imp:.6f}, lag={lag_minutes_map[nm]} min)\")\n",
        "\n",
        "df_lag_all = pd.concat(\n",
        "    [df_all[[\"time\",\"var1h\"]].reset_index(drop=True),\n",
        "     lag_df[selected_lag_cols].reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# Mini-TFT (seq-to-one)\n",
        "# -------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def _amp_ctx(): return nullcontext()\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.v = nn.Linear(d, d)\n",
        "        self.g = nn.Linear(d, d)\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.g(x)) * self.v(x)\n",
        "\n",
        "class MiniTFT(nn.Module):\n",
        "    def __init__(self, num_features:int, d_model:int=128, nhead:int=4, lstm_layers:int=1,\n",
        "                 ff_dim:int=256, dropout:float=0.2):\n",
        "        super().__init__()\n",
        "        self.F = num_features\n",
        "        self.d = d_model\n",
        "        self.feat_emb = nn.ModuleList([nn.Linear(1, d_model) for _ in range(num_features)])\n",
        "        self.vsn_w = nn.Linear(num_features, num_features)\n",
        "        self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model,\n",
        "                            num_layers=lstm_layers, batch_first=True, dropout=0.0, bidirectional=False)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(2*d_model, ff_dim), nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, d_model), GLU(d_model),\n",
        "            nn.Linear(d_model, 1)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.last_alpha = None\n",
        "    def forward(self, x):  # (B,T,F)\n",
        "        B,T,F = x.shape\n",
        "        x4 = x.unsqueeze(-1)  # (B,T,F,1)\n",
        "        embs = [ self.feat_emb[f](x4[:,:,f,:]) for f in range(self.F) ]  # each (B,T,d)\n",
        "        E = torch.stack(embs, dim=2)            # (B,T,F,d)\n",
        "        logits = self.vsn_w(x)                  # (B,T,F)\n",
        "        alpha = torch.softmax(logits, dim=2).unsqueeze(-1)  # (B,T,F,1)\n",
        "        self.last_alpha = alpha.detach()\n",
        "        Z = (alpha * E).sum(dim=2)              # (B,T,d)\n",
        "        Z = self.dropout(Z)\n",
        "        H, _ = self.lstm(Z)                     # (B,T,d)\n",
        "        last = H[:, -1:, :]\n",
        "        ctx, _ = self.attn(query=last, key=H, value=H, need_weights=False)\n",
        "        cat = torch.cat([last, ctx], dim=-1).squeeze(1)\n",
        "        out = self.proj(cat).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "def mean_vsn_weights(model:MiniTFT, X:np.ndarray, batch_size:int=4096)->np.ndarray:\n",
        "    if len(X)==0: return np.array([])\n",
        "    device = next(model.parameters()).device\n",
        "    ds = TensorDataset(torch.from_numpy(X))\n",
        "    dl = DataLoader(ds, batch_size=min(batch_size, len(ds)), shuffle=False, drop_last=False)\n",
        "    acc = None; n=0\n",
        "    model.eval()\n",
        "    with torch.no_grad(), _amp_ctx():\n",
        "        for (xb,) in dl:\n",
        "            xb = xb.to(device); _ = model(xb)\n",
        "            a = model.last_alpha.squeeze(-1)  # (B,T,F)\n",
        "            a = a.mean(dim=(0,1)).cpu().numpy()\n",
        "            if acc is None: acc = a\n",
        "            else: acc += a\n",
        "            n += 1\n",
        "    return acc / max(n,1)\n",
        "\n",
        "def rmse(a,b): return float(np.sqrt(np.mean((np.asarray(a)-np.asarray(b))**2)))\n",
        "\n",
        "# ---- Safe robust scaling ----\n",
        "def _safe_robust_params(X: np.ndarray, q_low=25.0, q_high=75.0, eps=1e-6):\n",
        "    Xf = X.reshape(-1, X.shape[2])\n",
        "    med = np.nanmedian(Xf, axis=0)\n",
        "    q1  = np.nanpercentile(Xf, q_low, axis=0)\n",
        "    q3  = np.nanpercentile(Xf, q_high, axis=0)\n",
        "    iqr = q3 - q1\n",
        "    scale = np.where(iqr <= eps, 1.0, iqr)\n",
        "    return med.astype(np.float32), scale.astype(np.float32)\n",
        "\n",
        "def fit_scaler_from_windows_safe(X_tr, X_va=None, X_te=None):\n",
        "    for X in (X_tr, X_va, X_te):\n",
        "        if X is not None and len(X) > 0:\n",
        "            med, scale = _safe_robust_params(X)\n",
        "            return {\"center\": med, \"scale\": scale}\n",
        "    F = X_tr.shape[2] if (X_tr is not None and X_tr.size) else (X_va.shape[2] if (X_va is not None and X_va.size) else (X_te.shape[2] if (X_te is not None and X_te.size) else 1))\n",
        "    return {\"center\": np.zeros(F, np.float32), \"scale\": np.ones(F, np.float32)}\n",
        "\n",
        "def norm_windows(X, rs, clip=20.0):\n",
        "    if len(X) == 0: return X\n",
        "    F = X.shape[2]\n",
        "    C = rs[\"center\"].reshape(1, 1, F)\n",
        "    S = rs[\"scale\" ].reshape(1, 1, F)\n",
        "    Z = (X - C) / S\n",
        "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
        "    if clip is not None: Z = np.clip(Z, -clip, clip, out=Z)\n",
        "    return Z\n",
        "\n",
        "def _assert_finite(name, *arrays):\n",
        "    for a in arrays:\n",
        "        if a is None or len(a) == 0: continue\n",
        "        if not np.isfinite(a).all(): raise ValueError(f\"{name}: found non-finite values.\")\n",
        "\n",
        "# ---- Multipliers (unchanged)\n",
        "WIND_STRESS_PATTERNS = [r\"(^|_)taux($|_)\", r\"(^|_)tauy($|_)\", r\"wind[_-]?stress\", r\"^Twind\", r\"_stress$\"]\n",
        "def is_wind_stress_base(base:str)->bool:\n",
        "    b = base if isinstance(base,str) else str(base)\n",
        "    return any(re.search(p, b, flags=re.IGNORECASE) for p in WIND_STRESS_PATTERNS)\n",
        "def compute_base_multipliers(feature_bases:List[str])->np.ndarray:\n",
        "    return np.array([0.8 if is_wind_stress_base(b) else 1.0 for b in feature_bases], dtype=np.float32)\n",
        "\n",
        "# -------------------\n",
        "# Utility: split summary printer\n",
        "# -------------------\n",
        "def _fmt(ts):\n",
        "    return ts.strftime(\"%Y-%m-%d %H:%M\") if (ts is not None and pd.notna(ts)) else \"—\"\n",
        "def print_split_summary(horizon, t_tr, t_va, t_te):\n",
        "    def rng(t):\n",
        "        if len(t)==0: return (None,None,0)\n",
        "        return (t.min(), t.max(), len(t))\n",
        "    tr0,tr1,ntr = rng(t_tr); va0,va1,nva = rng(t_va); te0,te1,nte = rng(t_te)\n",
        "    print(f\"[{horizon}m] Split ranges:\")\n",
        "    print(f\"  Train: {_fmt(tr0)} → {_fmt(tr1)}  (n={ntr})\")\n",
        "    print(f\"  Valid: {_fmt(va0)} → {_fmt(va1)}  (n={nva})\")\n",
        "    print(f\"  Test : {_fmt(te0)} → {_fmt(te1)}  (n={nte})\")\n",
        "    return (tr0,tr1),(va0,va1),(te0,te1)\n",
        "\n",
        "# -------------------\n",
        "# Windows + splits + training\n",
        "# -------------------\n",
        "def make_windows_from_lagged(df:pd.DataFrame, lag_cols:List[str],\n",
        "                             history_hours:int, horizon_min:int, dt_minutes:int,\n",
        "                             stride:int=1):\n",
        "    steps_hist  = int(round(history_hours*60/dt_minutes))\n",
        "    steps_ahead = int(round(horizon_min/dt_minutes))\n",
        "    tab = df[[\"time\",\"var1h\"] + lag_cols].copy()\n",
        "    tab = tab.dropna(subset=lag_cols + [\"var1h\"]).reset_index(drop=True)\n",
        "    if len(tab) < steps_hist + steps_ahead + 1:\n",
        "        return np.empty((0,steps_hist,len(lag_cols))), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    X_list, y_list, t_list = [], [], []\n",
        "    F = len(lag_cols); Ttot = len(tab)\n",
        "    for end_idx in range(steps_hist, Ttot - steps_ahead + 1, stride):\n",
        "        s = end_idx - steps_hist\n",
        "        k = end_idx - 1 + steps_ahead\n",
        "        xb = tab.iloc[s:end_idx][lag_cols].values\n",
        "        yv = tab.iloc[k][\"var1h\"]\n",
        "        if np.any(~np.isfinite(xb)) or not np.isfinite(yv): continue\n",
        "        X_list.append(xb.astype(np.float32))\n",
        "        y_list.append(float(yv))\n",
        "        t_list.append(pd.to_datetime(tab.iloc[k][\"time\"]))\n",
        "    if not X_list:\n",
        "        return np.empty((0,steps_hist,F)), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    return np.stack(X_list), np.array(y_list, dtype=np.float32), pd.DatetimeIndex(t_list)\n",
        "\n",
        "def adaptive_index_split(X, y, t_idx, train_frac=0.70, val_frac=0.15):\n",
        "    n = len(X)\n",
        "    if n < 10:\n",
        "        return (X[:0], y[:0], t_idx[:0]), (X[:0], y[:0], t_idx[:0]), (X, y, t_idx)\n",
        "    a = max(1, int(round(train_frac * n)))\n",
        "    b = max(a+1, int(round((train_frac + val_frac) * n)))\n",
        "    b = min(b, n-1)\n",
        "    return (X[:a], y[:a], t_idx[:a]), (X[a:b], y[a:b], t_idx[a:b]), (X[b:], y[b:], t_idx[b:])\n",
        "\n",
        "def cap_split(X_, y_, t_, cap=MAX_SAMPLES_SPLIT):\n",
        "    if len(X_) <= cap: return X_, y_, t_\n",
        "    idx = np.linspace(0, len(X_) - 1, cap, dtype=int)\n",
        "    return X_[idx], y_[idx], t_[idx]\n",
        "\n",
        "# Helper: plot raw inputs in window ordered by importance\n",
        "def plot_inputs_strip(df_all, best_list, start, end, title_prefix=\"\"):\n",
        "    win = df_all[(df_all[\"time\"] >= start) & (df_all[\"time\"] <= end)].copy()\n",
        "    if win.empty:\n",
        "        print(\"  [inputs strip] no data in window.\"); return\n",
        "    n = len(best_list)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(14, max(2.0*n, 4.0)), sharex=True)\n",
        "    if n == 1: axes = [axes]\n",
        "    for ax, (base, sel_name, imp) in zip(axes, best_list):\n",
        "        if base not in win.columns:\n",
        "            ax.text(0.5, 0.5, f\"{base} missing\", transform=ax.transAxes, ha=\"center\", va=\"center\")\n",
        "            ax.grid(alpha=0.3); continue\n",
        "        s = win[base].astype(float).copy()\n",
        "        if base in (\"Hs\",\"DPD\",\"APD\"):\n",
        "            s[s > 50] = np.nan\n",
        "        ax.plot(win[\"time\"], s, lw=1.0)\n",
        "        lag_min = re.search(r\"__lag_(\\d+)min\", sel_name)\n",
        "        lag_txt = f\"{lag_min.group(1)} min\" if lag_min else \"?\"\n",
        "        ax.set_ylabel(base)\n",
        "        ax.set_title(f\"{base}  (selected: {sel_name}, lag={lag_txt}, imp={imp:.3g})\", fontsize=9)\n",
        "        ax.grid(alpha=0.3)\n",
        "    axes[-1].set_xlabel(\"Time\")\n",
        "    fig.suptitle(f\"{title_prefix} raw inputs [{start:%Y-%m-%d} … {end:%Y-%m-%d}]\", y=0.995, fontsize=11)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "# -------------------\n",
        "# Main loop\n",
        "# -------------------\n",
        "results = {}\n",
        "for horizon in HORIZONS_MIN:\n",
        "    print(f\"\\n=== Horizon {horizon} min ===\")\n",
        "    X, y, t_idx = make_windows_from_lagged(df_lag_all, selected_lag_cols,\n",
        "                                           HISTORY_HOURS, horizon, DT_MINUTES, stride=1)\n",
        "\n",
        "    if len(t_idx):\n",
        "        print(f\"[{horizon}m] Windows available:\", t_idx.min(), \"→\", t_idx.max(), f\"(n={len(t_idx):,})\")\n",
        "    else:\n",
        "        print(f\"[{horizon}m] No windows after lagging/masking.\"); continue\n",
        "\n",
        "    # Time-based split\n",
        "    mtr = t_idx <= TRAIN_END\n",
        "    mva = (t_idx > TRAIN_END) & (t_idx <= VAL_END)\n",
        "    mte = t_idx > VAL_END\n",
        "\n",
        "    X_tr, y_tr, t_tr = X[mtr], y[mtr], t_idx[mtr]\n",
        "    X_va, y_va, t_va = X[mva], y[mva], t_idx[mva]\n",
        "    X_te, y_te, t_te = X[mte], y[mte], t_idx[mte]\n",
        "\n",
        "    if len(X_tr)==0 or len(X_va)==0 or len(X_te)==0:\n",
        "        print(\"  Time split produced an empty set; using adaptive 70/15/15 index split.\")\n",
        "        (X_tr, y_tr, t_tr), (X_va, y_va, t_va), (X_te, y_te, t_te) = adaptive_index_split(X, y, t_idx)\n",
        "\n",
        "    if len(X_te) == 0:\n",
        "        print(\"  Still no test after adaptive split; assigning last ~15% as test.\")\n",
        "        n = len(X); cut = max(1, int(0.85*n))\n",
        "        X_tr, y_tr, t_tr = X[:cut], y[:cut], t_idx[:cut]\n",
        "        X_va, y_va, t_va = X[:0], y[:0], t_idx[:0]\n",
        "        X_te, y_te, t_te = X[cut:], y[cut:], t_idx[cut:]\n",
        "\n",
        "    (_, _), (va_rng0, va_rng1), (te_rng0, te_rng1) = print_split_summary(horizon, t_tr, t_va, t_te)\n",
        "\n",
        "    X_tr, y_tr, t_tr = cap_split(X_tr, y_tr, t_tr)\n",
        "    X_va, y_va, t_va = cap_split(X_va, y_va, t_va)\n",
        "    X_te, y_te, t_te = cap_split(X_te, y_te, t_te)\n",
        "\n",
        "    rs     = fit_scaler_from_windows_safe(X_tr, X_va, X_te)\n",
        "    X_tr_n = norm_windows(X_tr, rs)\n",
        "    X_va_n = norm_windows(X_va, rs)\n",
        "    X_te_n = norm_windows(X_te, rs)\n",
        "    _assert_finite(\"X_tr_n\", X_tr_n); _assert_finite(\"X_va_n\", X_va_n); _assert_finite(\"X_te_n\", X_te_n)\n",
        "    _assert_finite(\"y_tr\", y_tr); _assert_finite(\"y_va\", y_va); _assert_finite(\"y_te\", y_te)\n",
        "\n",
        "    feature_bases = [base_map[c] for c in selected_lag_cols]\n",
        "    base_mult_vec = compute_base_multipliers(feature_bases)\n",
        "\n",
        "    def apply_base_mult(xnp):\n",
        "        if base_mult_vec is None or len(base_mult_vec)==0: return xnp\n",
        "        return (xnp * base_mult_vec.reshape(1,1,-1)).astype(np.float32)\n",
        "\n",
        "    # Train Mini-TFT\n",
        "    import torch\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "    model = MiniTFT(\n",
        "        num_features=X_tr_n.shape[2], d_model=D_MODEL, nhead=NHEAD,\n",
        "        lstm_layers=LSTM_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=min(LR, 1e-3))\n",
        "\n",
        "    ds_tr = TensorDataset(torch.from_numpy(apply_base_mult(X_tr_n)), torch.from_numpy(y_tr))\n",
        "    ds_va = TensorDataset(torch.from_numpy(apply_base_mult(X_va_n)), torch.from_numpy(y_va))\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=min(BATCH_SIZE, len(ds_tr)), shuffle=True, drop_last=False)\n",
        "    dl_va = DataLoader(ds_va, batch_size=min(BATCH_SIZE, len(ds_va)), shuffle=False, drop_last=False)\n",
        "\n",
        "    best = np.inf; best_state=None; patience=3; bad=0\n",
        "    q = np.quantile(y_tr, SPIKE_P90) if (WEIGHT_SPIKES and len(y_tr)>0) else None\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        model.train(); tr_loss=0.0\n",
        "        for xb,yb in dl_tr:\n",
        "            xb,yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            with _amp_ctx():\n",
        "                pred = model(xb)\n",
        "                l = (pred - yb) ** 2\n",
        "                if q is not None:\n",
        "                    q_t = torch.tensor(q, device=yb.device, dtype=yb.dtype)\n",
        "                    w = torch.where(yb >= q_t, torch.tensor(SPIKE_WEIGHT, device=yb.device, dtype=yb.dtype),\n",
        "                                              torch.tensor(1.0,         device=yb.device, dtype=yb.dtype))\n",
        "                    l = l * w\n",
        "                loss = l.mean()\n",
        "            if not torch.isfinite(loss):\n",
        "                raise RuntimeError(\"Non-finite training loss encountered.\")\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            opt.step()\n",
        "            tr_loss += loss.item() * len(xb)\n",
        "        tr_loss /= max(1, len(dl_tr.dataset))\n",
        "\n",
        "        model.eval(); va_loss=0.0\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for xb,yb in dl_va:\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                out = model(xb)\n",
        "                va_loss += nn.functional.mse_loss(out, yb, reduction=\"sum\").item()\n",
        "        va_loss /= max(1, len(dl_va.dataset))\n",
        "        print(f\"[TFT] epoch {ep+1:02d}  trainMSE={tr_loss:.5f}  valMSE={va_loss:.5f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best:\n",
        "            best = va_loss; best_state = model.state_dict(); bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                print(\"[TFT] early stop.\"); break\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "\n",
        "    # Predict on both val & test\n",
        "    def predict_tft(model, X):\n",
        "        ds = TensorDataset(torch.from_numpy(X))\n",
        "        dl = DataLoader(ds, batch_size=min(BATCH_SIZE, len(ds)), shuffle=False, drop_last=False)\n",
        "        out=[]; model.eval()\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for (xb,) in dl:\n",
        "                xb = xb.to(device)\n",
        "                xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                out.append(model(xb).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0) if len(out)>0 else np.array([])\n",
        "\n",
        "    yp_va = predict_tft(model, apply_base_mult(X_va_n)) if len(X_va_n)>0 else np.array([])\n",
        "    yp_te = predict_tft(model, apply_base_mult(X_te_n)) if len(X_te_n)>0 else np.array([])\n",
        "\n",
        "    if len(yp_te)>0 and len(y_te)>0:\n",
        "        R2 = r2_score(y_te, yp_te); RMSE = rmse(y_te, yp_te)\n",
        "    else:\n",
        "        R2 = float(\"nan\"); RMSE = float(\"nan\")\n",
        "    results[horizon] = dict(t_val=t_va, y_val=y_va, yp_val=yp_va,\n",
        "                            t=t_te, y=y_te, yp=yp_te, r2=R2, rmse=RMSE)\n",
        "    print(f\"[TFT | 1/base] {horizon}m  Test R²={R2:.3f}  RMSE={RMSE:.3f}\")\n",
        "\n",
        "    # Variable-selection weights (test)\n",
        "    try:\n",
        "        w_mean = mean_vsn_weights(model, apply_base_mult(X_te_n))\n",
        "        if w_mean.size:\n",
        "            print(\"  [TFT] mean variable-selection weights (test):\")\n",
        "            for f_idx, w in sorted(list(enumerate(w_mean)), key=lambda t: -t[1])[:min(12, len(w_mean))]:\n",
        "                base = selected_bases[f_idx] if f_idx < len(selected_bases) else f\"feat{f_idx}\"\n",
        "                print(f\"    {base:<24s}  {w:.4f}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---- Plot window\n",
        "    if PRED_SPLIT.lower() == \"val\":\n",
        "        t_plot, y_plot, yhat_plot = t_va, y_va, yp_va\n",
        "        split_name = \"Validation\"\n",
        "        avail0, avail1 = va_rng0, va_rng1\n",
        "    else:\n",
        "        t_plot, y_plot, yhat_plot = t_te, y_te, yp_te\n",
        "        split_name = \"Test\"\n",
        "        avail0, avail1 = te_rng0, te_rng1\n",
        "\n",
        "    if len(t_plot)==0:\n",
        "        print(f\"  [plot] No {split_name.lower()} points available to plot.\")\n",
        "        continue\n",
        "\n",
        "    mwin = (t_plot >= PRED_START) & (t_plot <= PRED_END)\n",
        "    if not np.any(mwin):\n",
        "        print(f\"  [plot] No {split_name.lower()} predictions in requested window \"\n",
        "              f\"{PRED_START:%Y-%m-%d} to {PRED_END:%Y-%m-%d}.\")\n",
        "        print(f\"         Available {split_name.lower()} range: {_fmt(avail0)} → {_fmt(avail1)}\")\n",
        "        end = t_plot[-1]; start = end - pd.Timedelta(days=PLOT_LAST_DAYS)\n",
        "        mlast = (t_plot >= start) & (t_plot <= end)\n",
        "        if np.any(mlast):\n",
        "            ts, yt, ypp = t_plot[mlast], y_plot[mlast], yhat_plot[mlast]\n",
        "            plt.figure(figsize=(14,4))\n",
        "            plt.plot(ts, yt, label=\"Measured\", lw=1.2)\n",
        "            plt.plot(ts, ypp, \"--\", label=f\"Predicted ({split_name})\", lw=1.2)\n",
        "            plt.title(f\"TFT — {horizon} min ({split_name}, last {PLOT_LAST_DAYS} days)\")\n",
        "            plt.ylabel(\"var1h\"); plt.xlabel(\"Time\"); plt.grid(alpha=0.3); plt.legend()\n",
        "            plt.tight_layout(); plt.show()\n",
        "            plot_inputs_strip(df_all, best_list, start=start, end=end,\n",
        "                              title_prefix=f\"Horizon {horizon} min — {split_name} —\")\n",
        "        continue\n",
        "\n",
        "    ts, yt, ypp = t_plot[mwin], y_plot[mwin], yhat_plot[mwin]\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.plot(ts, yt, label=\"Measured\", lw=1.2)\n",
        "    plt.plot(ts, ypp, \"--\", label=f\"Predicted ({split_name})\", lw=1.2)\n",
        "    plt.title(f\"TFT — {horizon} min ({split_name}) — {PRED_START:%b %Y}\")\n",
        "    plt.ylabel(\"var1h\"); plt.xlabel(\"Time\"); plt.grid(alpha=0.3); plt.legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    plot_inputs_strip(\n",
        "        df_all=df_all,\n",
        "        best_list=best_list,\n",
        "        start=PRED_START,\n",
        "        end=PRED_END,\n",
        "        title_prefix=f\"Horizon {horizon} min — {split_name} —\"\n",
        "    )\n",
        "\n",
        "print(\"\\nFinal selected lag channels (one per base):\")\n",
        "for nm in selected_lag_cols:\n",
        "    print(\"  \", nm)\n"
      ]
    }
  ]
}