{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3I/4bmaM5rG1xqATThRUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaysmerrill/Yaquina_Bay_Seiching/blob/main/seiche_prediction_rtdata_optimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuxRfnCZ22c7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Real-time forcings (no parquet) + var_wl (20–30 min bandpass → X-hour variance)\n",
        "# Sources: NOAA CO-OPS 9435380 (WL, wind, air pressure), NDBC 46050 (stdmet, swden), HMSC (baro gap-fill)\n",
        "# ============================================================\n",
        "import io, gzip, requests, re, json, os, math, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plotting + signal\n",
        "!pip -q install plotly scipy\n",
        "from scipy.signal import butter, filtfilt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# -------------------\n",
        "# USER KNOBS\n",
        "# -------------------\n",
        "YEARS                = list(range(2008, pd.Timestamp.utcnow().year + 1))\n",
        "DT_MINUTES           = 6  # common model grid\n",
        "WL_BAND_MIN_MIN      = 20.0\n",
        "WL_BAND_MAX_MIN      = 30.0\n",
        "VAR_WIN_HOURS        = 2.0     # <-- default window for variance of band-passed WL (tweakable)\n",
        "NOAA_STATION         = \"9435380\"  # Newport / South Beach\n",
        "NOAA_BEGIN_DATE      = \"20080101\"\n",
        "NOAA_END_DATE        = pd.Timestamp.utcnow().strftime(\"%Y%m%d\")\n",
        "NOAA_CHUNK_DAYS      = 365\n",
        "\n",
        "# HMSC monthly archive (baro gap fill for NOAA air_pressure ≥3h gaps; <3h: spline)\n",
        "HMSC_UTC_OFFSET_H    = +8\n",
        "INHG_TO_PA           = 3386.389\n",
        "\n",
        "# NDBC buoy 46050\n",
        "BUOY                 = \"46050\"\n",
        "\n",
        "# Optional preview window for plots\n",
        "PRED_START           = None  # e.g., pd.Timestamp(\"2022-11-01\")\n",
        "PRED_END             = None  # e.g., pd.Timestamp(\"2022-12-15\")\n",
        "\n",
        "# -------------------\n",
        "# HTTP session with retries\n",
        "# -------------------\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def _make_session():\n",
        "    s = requests.Session()\n",
        "    retry = Retry(total=4, backoff_factor=0.6, status_forcelist=(429,500,502,503,504),\n",
        "                  allowed_methods=[\"HEAD\",\"GET\",\"OPTIONS\"], raise_on_status=False)\n",
        "    s.headers.update({\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) RealtimeForcings/1.0\", \"Accept\":\"*/*\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
        "    return s\n",
        "session = _make_session()\n",
        "\n",
        "def _datestr(dt) -> str:\n",
        "    return pd.Timestamp(dt).strftime(\"%Y%m%d\")\n",
        "\n",
        "# -------------------\n",
        "# NOAA CO-OPS: Water Level (6-min) + bandpass(20–30min) + centered rolling variance -> var_wl\n",
        "# -------------------\n",
        "def parse_noaa_wl_csv(txt:str)->pd.DataFrame:\n",
        "    if txt.strip().lower().startswith(\"error\"):\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    if df.shape[1] < 2: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df.columns = [str(c).strip().lower().replace(\"  \",\" \") for c in df.columns]\n",
        "    tcol=None\n",
        "    for k in (\"date time\",\"date_time\",\"time\",\"t\",\"date\"):\n",
        "        if k in df.columns: tcol=k; break\n",
        "    if tcol is None: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df[\"time\"] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    vcol=None\n",
        "    for k in (\"water level\",\"water_level\",\"wl\",\"v\"):\n",
        "        if k in df.columns: vcol=k; break\n",
        "    if vcol is None: vcol = df.columns[1]\n",
        "    df[\"wl\"] = pd.to_numeric(df[vcol], errors=\"coerce\")\n",
        "    out = df[[\"time\",\"wl\"]].dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    out.loc[(out[\"wl\"]<-50)|(out[\"wl\"]>50),\"wl\"]=np.nan\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def fetch_noaa_wl(begin_date:str, end_date:str, station:str, days_per_chunk:int=31)->pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\"); end = pd.to_datetime(end_date, format=\"%Y%m%d\")\n",
        "    frames=[]\n",
        "    cur=start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(cur)}&end_date={_datestr(chunk_end)}\"\n",
        "               f\"&station={station}&product=water_level&datum=MLLW&time_zone=gmt&units=metric\"\n",
        "               f\"&interval=6&application=RealtimeForcings&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=90)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wl_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except: pass\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "def bandpass_then_var(series:pd.DataFrame, val_col:str, dt_minutes:int,\n",
        "                      band_min_min:float, band_max_min:float, var_win_h:float)->pd.DataFrame:\n",
        "    if series.empty: return pd.DataFrame(columns=[\"time\",\"var_wl\"])\n",
        "    t0,t1 = series[\"time\"].min(), series[\"time\"].max()\n",
        "    grid = pd.date_range(t0.floor(f\"{dt_minutes}min\"), t1.ceil(f\"{dt_minutes}min\"), freq=f\"{dt_minutes}min\")\n",
        "    s = series.set_index(\"time\")[val_col].reindex(grid)\n",
        "    s = s.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    dt_sec = dt_minutes*60.0; fs=1.0/dt_sec\n",
        "    f_lo = 1.0/(band_max_min*60.0)  # low cutoff = longer period edge\n",
        "    f_hi = 1.0/(band_min_min*60.0)  # high cutoff = shorter period edge\n",
        "    wn = [max(1e-6, f_lo/(fs/2.0)), min(0.999, f_hi/(fs/2.0))]\n",
        "    b,a = butter(6, wn, btype=\"bandpass\")\n",
        "    x = s.to_numpy(dtype=float)\n",
        "    if np.isnan(x).any():\n",
        "        idx = np.arange(len(x)); good = np.isfinite(x);\n",
        "        if good.any(): x[~good] = np.interp(idx[~good], idx[good], x[good])\n",
        "    x_bp = filtfilt(b,a,x,method=\"gust\")  # zero-phase\n",
        "    win = max(3, int(round((var_win_h*60.0)/dt_minutes)))\n",
        "    minp = max(2, int(0.8*win))\n",
        "    var_series = pd.Series(x_bp, index=grid).rolling(window=win, center=True, min_periods=minp).var()\n",
        "    return pd.DataFrame({\"time\":grid, \"var_wl\":var_series.astype(\"float32\")}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "df_wl6  = fetch_noaa_wl(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_STATION, days_per_chunk=31)\n",
        "df_varWL = bandpass_then_var(df_wl6, \"wl\", DT_MINUTES, WL_BAND_MIN_MIN, WL_BAND_MAX_MIN, VAR_WIN_HOURS)\n",
        "print(\"WL coverage:\", (df_wl6[\"time\"].min() if len(df_wl6) else None), \"→\", (df_wl6[\"time\"].max() if len(df_wl6) else None), f\"(n={len(df_wl6):,})\")\n",
        "\n",
        "# -------------------\n",
        "# NOAA CO-OPS: Wind (hourly)\n",
        "# -------------------\n",
        "def parse_noaa_wind_csv(txt:str)->pd.DataFrame:\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    tcol = [c for c in df.columns if str(c).strip().lower() in (\"t\",\"time\",\"date time\",\"date_time\",\"date\")]\n",
        "    tcol = tcol[0] if tcol else df.columns[0]\n",
        "    df[\"time\"] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    def _find(keys):\n",
        "        keys = [k.lower() for k in keys]\n",
        "        for c in df.columns:\n",
        "            s = str(c).lower()\n",
        "            if any(k == s or k in s for k in keys): return c\n",
        "        return None\n",
        "    sp_col = _find([\"s\",\"speed\"])\n",
        "    dir_col= _find([\"d\",\"dir\",\"direction\"])\n",
        "    out = pd.DataFrame({\"time\":df[\"time\"]})\n",
        "    out[\"wind9435380_speed\"] = pd.to_numeric(df[sp_col], errors=\"coerce\") if sp_col else np.nan\n",
        "    out[\"wind9435380_dir\"]   = pd.to_numeric(df[dir_col], errors=\"coerce\") if dir_col else np.nan\n",
        "    out.loc[(out[\"wind9435380_dir\"]<0)|(out[\"wind9435380_dir\"]>360),\"wind9435380_dir\"]=np.nan\n",
        "    out.loc[(out[\"wind9435380_speed\"]<0)|(out[\"wind9435380_speed\"]>100),\"wind9435380_speed\"]=np.nan\n",
        "    return (out.sort_values(\"time\").dropna(subset=[\"time\"])\n",
        "            .drop_duplicates(\"time\").reset_index(drop=True))\n",
        "\n",
        "def fetch_noaa_wind_dataframe(begin_date:str, end_date:str, station:str, days_per_chunk:int=365)->pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\"); end = pd.to_datetime(end_date, format=\"%Y%m%d\")\n",
        "    frames=[]; cur=start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(cur)}&end_date={_datestr(chunk_end)}\"\n",
        "               f\"&station={station}&product=wind&time_zone=gmt&interval=h&units=metric\"\n",
        "               \"&application=RealtimeForcings&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=60)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wind_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except: pass\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"wind9435380_speed\",\"wind9435380_dir\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "df_wind = fetch_noaa_wind_dataframe(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_STATION, days_per_chunk=NOAA_CHUNK_DAYS)\n",
        "print(\"Wind coverage:\", (df_wind[\"time\"].min() if len(df_wind) else None), \"→\", (df_wind[\"time\"].max() if len(df_wind) else None), f\"(n={len(df_wind):,})\")\n",
        "\n",
        "# -------------------\n",
        "# NOAA CO-OPS: Air Pressure (hourly) + HMSC gap-fill logic\n",
        "# -------------------\n",
        "def parse_noaa_air_pressure_csv(txt: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    # time col\n",
        "    tcol = None\n",
        "    for k in (\"t\",\"time\",\"date time\",\"date_time\",\"date\"):\n",
        "        m = [c for c in df.columns if str(c).strip().lower()==k]\n",
        "        if m: tcol = m[0]; break\n",
        "    if tcol is None: tcol = df.columns[0]\n",
        "    # value col\n",
        "    vcol = None\n",
        "    for key in (\"air_pressure\",\"air pressure\",\"barometric pressure\",\"baro\",\"pressure\"):\n",
        "        m = [c for c in df.columns if key in str(c).strip().lower()]\n",
        "        if m: vcol = m[0]; break\n",
        "    if vcol is None: vcol = df.columns[1] if df.shape[1] > 1 else df.columns[0]\n",
        "    out = pd.DataFrame({\n",
        "        \"time\": pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None),\n",
        "        \"air_pressure_hpa\": pd.to_numeric(df[vcol], errors=\"coerce\")\n",
        "    }).dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "    out.loc[(out[\"air_pressure_hpa\"] < 850) | (out[\"air_pressure_hpa\"] > 1100), \"air_pressure_hpa\"] = np.nan\n",
        "    return out\n",
        "\n",
        "def fetch_noaa_air_pressure_dataframe(begin_date: str, end_date: str, station: str,\n",
        "                                      days_per_chunk: int = 365) -> pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\"); end = pd.to_datetime(end_date, format=\"%Y%m%d\")\n",
        "    frames=[]; cur = start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(cur)}&end_date={_datestr(chunk_end)}\"\n",
        "               f\"&station={station}&product=air_pressure&time_zone=gmt&interval=h&units=metric\"\n",
        "               \"&application=RealtimeForcings&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=60)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_air_pressure_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except: pass\n",
        "        cur = chunk_end + pd.Timedelta(days=1)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"air_pressure_hpa\"])\n",
        "    return (pd.concat(frames, ignore_index=True)\n",
        "            .sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True))\n",
        "\n",
        "# HMSC baro (AvgBP in inHg) -> Pa for gap fill\n",
        "def fetch_hmsc_month_text(year:int, month:int) -> str|None:\n",
        "    url = f\"http://weather.hmsc.oregonstate.edu/weather/weatherproject/archive/{year}/HMSC_{year}{month:02d}.dat\"\n",
        "    try:\n",
        "        r = session.get(url, timeout=60)\n",
        "        if r.status_code==200 and r.content:\n",
        "            return r.content.decode(\"utf-8\", \"ignore\")\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "def parse_hmsc_dat_avgbp_pa(txt: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse HMSC monthly .dat text to dataframe with UTC 'time' and pressure in Pa.\n",
        "\n",
        "    - Accepts files where the first CSV header line starts with TIMESTAMP\n",
        "    - Filters out the two metadata/unit rows that appear after the header\n",
        "    - Parses local (America/Los_Angeles) times and converts to UTC (naive)\n",
        "    - Converts AvgBP (inHg) to Pascals\n",
        "    \"\"\"\n",
        "    # 1) Find the CSV header (TIMESTAMP,...)\n",
        "    lines = txt.splitlines()\n",
        "    start_idx = None\n",
        "    for i, ln in enumerate(lines):\n",
        "        if ln.strip().strip('\"').startswith(\"TIMESTAMP\"):\n",
        "            start_idx = i\n",
        "            break\n",
        "    if start_idx is None:\n",
        "        return pd.DataFrame(columns=[\"time\", \"bp_pa\"])\n",
        "\n",
        "    content = \"\\n\".join(lines[start_idx:])\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "\n",
        "    # 2) First column is time text; strip quotes and whitespace\n",
        "    raw_t = df.iloc[:, 0].astype(str).str.strip().str.replace('\"', '', regex=False)\n",
        "\n",
        "    # Filter out the two metadata/unit rows (keep only rows that look like YYYY-mm-dd HH:MM:SS[.fff])\n",
        "    time_like = raw_t.str.match(r\"\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?$\")\n",
        "    raw_t = raw_t.where(time_like)  # non-matching rows -> NaN\n",
        "\n",
        "    # 3) Fast explicit parse, then gentle fallbacks\n",
        "    t = pd.to_datetime(raw_t, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
        "    if t.isna().any():\n",
        "        t_ms = pd.to_datetime(raw_t, format=\"%Y-%m-%d %H:%M:%S.%f\", errors=\"coerce\")\n",
        "        t = t.fillna(t_ms)\n",
        "    if t.isna().any():\n",
        "        # Last resort (rare)\n",
        "        t = t.fillna(pd.to_datetime(raw_t, errors=\"coerce\"))\n",
        "\n",
        "    # Local (America/Los_Angeles) -> UTC (naive)\n",
        "    try:\n",
        "        t = (\n",
        "            t.dt.tz_localize(\"America/Los_Angeles\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
        "             .dt.tz_convert(\"UTC\")\n",
        "             .dt.tz_localize(None)\n",
        "        )\n",
        "    except Exception:\n",
        "        # If tz database isn't available in the environment, proceed as naive\n",
        "        pass\n",
        "\n",
        "    # 4) Find AvgBP (inHg), convert to float\n",
        "    target = None\n",
        "    for c in df.columns:\n",
        "        if \"avgbp\" in str(c).lower():\n",
        "            target = c\n",
        "            break\n",
        "    if target is None:\n",
        "        return pd.DataFrame(columns=[\"time\", \"bp_pa\"])\n",
        "\n",
        "    avgbp_inhg = pd.to_numeric(df[target], errors=\"coerce\")\n",
        "\n",
        "    # Avoid the invalid value warnings: use between() with inclusive bounds\n",
        "    good = avgbp_inhg.between(26.0, 33, inclusive=\"both\")\n",
        "    avgbp_inhg = avgbp_inhg.where(good)\n",
        "\n",
        "    INHG_TO_PA = 3386.389  # Pa per inHg\n",
        "    bp_pa = avgbp_inhg * INHG_TO_PA\n",
        "\n",
        "    out = (\n",
        "        pd.DataFrame({\"time\": t, \"bp_pa\": bp_pa})\n",
        "        .dropna(subset=[\"time\"])\n",
        "        .astype({\"bp_pa\": \"float64\"})\n",
        "        .sort_values(\"time\")           # now safe: all datetimes\n",
        "        .drop_duplicates(\"time\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return out\n",
        "\n",
        "\n",
        "def month_iter(start:pd.Timestamp, end:pd.Timestamp):\n",
        "    y,m = start.year, start.month\n",
        "    while pd.Timestamp(year=y, month=m, day=1) <= end:\n",
        "        yield (y,m)\n",
        "        if m==12: y,m=y+1,1\n",
        "        else: m+=1\n",
        "\n",
        "# Fetch + merge air pressure with HMSC fill\n",
        "df_air = fetch_noaa_air_pressure_dataframe(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_STATION, days_per_chunk=NOAA_CHUNK_DAYS)\n",
        "if len(df_air):\n",
        "    df_air[\"air_pressure_pa\"] = (df_air[\"air_pressure_hpa\"].astype(float) * 100.0).astype(\"float64\")\n",
        "    ap_start, ap_end = df_air[\"time\"].min(), df_air[\"time\"].max()\n",
        "    hourly_grid = pd.date_range(ap_start.floor(\"h\"), ap_end.ceil(\"h\"), freq=\"h\")\n",
        "    s_noaa = df_air.set_index(\"time\")[\"air_pressure_pa\"].reindex(hourly_grid).astype(\"float64\")\n",
        "\n",
        "    # HMSC coverage padded\n",
        "    start_pad = (ap_start - pd.Timedelta(days=31)).normalize()\n",
        "    end_pad   = (ap_end   + pd.Timedelta(days=31)).normalize()\n",
        "    hmsc_frames=[]\n",
        "    for y,m in month_iter(start_pad, end_pad):\n",
        "        txt = fetch_hmsc_month_text(y,m)\n",
        "        if not txt: continue\n",
        "        dfm = parse_hmsc_dat_avgbp_pa(txt)\n",
        "        if not dfm.empty:\n",
        "            mask = (dfm[\"time\"]>=start_pad) & (dfm[\"time\"]<=end_pad)\n",
        "            hmsc_frames.append(dfm.loc[mask])\n",
        "    df_hmsc_bp = (pd.concat(hmsc_frames, ignore_index=True)\n",
        "                  .sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)) if hmsc_frames else pd.DataFrame(columns=[\"time\",\"bp_pa\"])\n",
        "    if len(df_hmsc_bp):\n",
        "        s_hmsc = (df_hmsc_bp.set_index(\"time\")[\"bp_pa\"].reindex(hourly_grid)\n",
        "                  .interpolate(method=\"time\", limit_area=\"inside\").astype(\"float64\"))\n",
        "    else:\n",
        "        s_hmsc = pd.Series(index=hourly_grid, dtype=\"float64\")\n",
        "\n",
        "    # fill rules\n",
        "    filled = s_noaa.copy()\n",
        "    nan_mask = filled.isna().to_numpy()\n",
        "    if nan_mask.any():\n",
        "        starts = np.where(~nan_mask[:-1] & nan_mask[1:])[0] + 1 if len(nan_mask)>1 else np.array([],dtype=int)\n",
        "        ends   = np.where(nan_mask[:-1] & ~nan_mask[1:])[0] + 1 if len(nan_mask)>1 else np.array([],dtype=int)\n",
        "        if nan_mask[0]:  starts = np.r_[0, starts]\n",
        "        if nan_mask[-1]: ends   = np.r_[ends, len(nan_mask)]\n",
        "        for s,e in zip(starts, ends):\n",
        "            run_len = e - s\n",
        "            if run_len >= 3:  # >=3h: fill from HMSC\n",
        "                seg_idx = hourly_grid[s:e]\n",
        "                filled.iloc[s:e] = s_hmsc.loc[seg_idx].values\n",
        "        if filled.isna().any():\n",
        "            try:\n",
        "                filled = filled.interpolate(method=\"spline\", order=3, limit_area=\"inside\")\n",
        "            except Exception:\n",
        "                filled = filled.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    df_air_merged = pd.DataFrame({\"time\": hourly_grid, \"air_pressure_pa\": filled.to_numpy()})\n",
        "else:\n",
        "    df_air_merged = pd.DataFrame(columns=[\"time\",\"air_pressure_pa\"])\n",
        "\n",
        "print(\"Air pressure coverage:\", (df_air_merged[\"time\"].min() if len(df_air_merged) else None),\n",
        "      \"→\", (df_air_merged[\"time\"].max() if len(df_air_merged) else None),\n",
        "      f\"(n={len(df_air_merged):,})\")\n",
        "\n",
        "# -------------------\n",
        "# NDBC 46050: stdmet (Hs, DPD, APD, MWD) + SWDEN bands (high_HIG, Hswell, Hsea)\n",
        "# -------------------\n",
        "def _ndbc_to_float(tok:str) -> float:\n",
        "    s = str(tok).strip()\n",
        "    if s == \"\" or s.upper() == \"MM\": return np.nan\n",
        "    if re.fullmatch(r\"-?9+(\\.0+)?\", s): return np.nan\n",
        "    try: return float(s)\n",
        "    except: return np.nan\n",
        "\n",
        "STDMET_PHP_PAT = f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}h\"+\"{year}.txt.gz&dir=data/historical/stdmet/\"\n",
        "\n",
        "def fetch_stdmet_year(year:int)->str|None:\n",
        "    for url in (STDMET_PHP_PAT.format(year=year),):\n",
        "        try:\n",
        "            r = session.get(url, timeout=40)\n",
        "            if r.status_code==200 and r.text and \"Page Not Found\" not in r.text:\n",
        "                return r.text\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def parse_stdmet_text(txt:str)->pd.DataFrame:\n",
        "    rows=[]\n",
        "    for ln in txt.splitlines():\n",
        "        if not ln or ln.lstrip().startswith(\"#\"): continue\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 6: continue\n",
        "        y,mo,dy,hh,mi = parts[:5]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh); mi=int(mi)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, minute=mi, tz=\"UTC\").tz_localize(None)\n",
        "        except: continue\n",
        "        vals = parts[5:]\n",
        "        if len(vals) < 13: vals = (vals + [\"MM\"]*13)[:13]\n",
        "        vals = [_ndbc_to_float(v) for v in vals]\n",
        "        rows.append([ts] + vals)\n",
        "    if not rows: return pd.DataFrame()\n",
        "    cols = [\"time\",\"WDIR\",\"WSPD\",\"GST\",\"WVHT\",\"DPD\",\"APD\",\"MWD\",\"PRES\",\"ATMP\",\"WTMP\",\"DEWP\",\"VIS\",\"TIDE\"]\n",
        "    df = pd.DataFrame(rows, columns=cols).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    df = df.rename(columns={\"WVHT\":\"Hs\"}).reset_index(drop=True)\n",
        "    for c,thr in [(\"Hs\",50.0),(\"DPD\",50.0),(\"APD\",50.0)]:\n",
        "        if c in df.columns: df.loc[df[c]>thr, c]=np.nan\n",
        "    df.loc[(df[\"MWD\"]<0)|(df[\"MWD\"]>360),\"MWD\"]=np.nan\n",
        "    keep = [\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"]\n",
        "    for k in keep:\n",
        "        if k not in df.columns: df[k]=np.nan\n",
        "    return df[keep]\n",
        "\n",
        "def build_stdmet(years: list[int]) -> pd.DataFrame:\n",
        "    frames=[]\n",
        "    for y in years:\n",
        "        txt = fetch_stdmet_year(y)\n",
        "        if not txt: continue\n",
        "        dfy = parse_stdmet_text(txt)\n",
        "        if not dfy.empty: frames.append(dfy)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "# SWDEN\n",
        "SWDEN_PHP_PATS = [\n",
        "    f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}w\"+\"{year}.txt.gz&dir=data/historical/swden/\",\n",
        "    f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}w\"+\"{year}.txt.gz&dir=data/swden/\"+BUOY+\"/\",\n",
        "]\n",
        "def fetch_swden_year(year:int)->str|None:\n",
        "    for url in SWDEN_PHP_PATS:\n",
        "        try:\n",
        "            r = session.get(url.format(year=year), timeout=40)\n",
        "            if r.status_code==200 and r.text and \"Page Not Found\" not in r.text:\n",
        "                return r.text\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def split_swden_blocks(lines:list[str]):\n",
        "    header=[]; i=0\n",
        "    while i<len(lines) and lines[i].startswith(\"#\"): header.append(lines[i]); i+=1\n",
        "    while i<len(lines) and not lines[i].strip(): i+=1\n",
        "    freq_line = lines[i].rstrip(\"\\n\") if i<len(lines) else None\n",
        "    i += 1 if i<len(lines) else 0\n",
        "    data=[ln for ln in lines[i:] if ln.strip()]\n",
        "    return header, freq_line, data\n",
        "\n",
        "def parse_swden_year(txt:str)->pd.DataFrame:\n",
        "    lines = txt.splitlines()\n",
        "    _, freq_line, data_lines = split_swden_blocks(lines)\n",
        "    if not freq_line: return pd.DataFrame()\n",
        "    freq_tokens = [t for t in re.split(r\"\\s+\", freq_line.strip()) if t]\n",
        "    if any(k in freq_tokens[:6] for k in (\"YY\",\"MM\",\"DD\",\"hh\")):\n",
        "        idx = lines.index(freq_line)\n",
        "        j=idx+1\n",
        "        while j<len(lines) and not lines[j].strip(): j+=1\n",
        "        if j<len(lines):\n",
        "            freq_tokens = [t for t in re.split(r\"\\s+\", lines[j].strip()) if t]\n",
        "            data_lines  = [ln for ln in lines[j+1:] if ln.strip()]\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "    def _num(tok):\n",
        "        try: return float(tok)\n",
        "        except: return np.nan\n",
        "    freqs = np.array([_num(t) for t in freq_tokens], dtype=float)\n",
        "    freqs = freqs[np.isfinite(freqs)]\n",
        "    if freqs.size==0: return pd.DataFrame()\n",
        "    dfw = np.empty_like(freqs)\n",
        "    if freqs.size==1:\n",
        "        dfw[:] = 0.0\n",
        "    else:\n",
        "        dfw[1:-1] = 0.5*(freqs[2:] - freqs[:-2])\n",
        "        dfw[0]    = freqs[1] - freqs[0]\n",
        "        dfw[-1]   = freqs[-1] - freqs[-2]\n",
        "        dfw = np.where(dfw<0, np.nan, dfw)\n",
        "    mask_hig   = (freqs < 0.05)\n",
        "    mask_swell = (freqs >= 0.05) & (freqs <= 0.14)\n",
        "    mask_sea   = (freqs > 0.14) & (freqs <= 0.30)\n",
        "    rows=[]\n",
        "    for ln in data_lines:\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 4: continue\n",
        "        y,mo,dy,hh = parts[:4]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, tz=\"UTC\").tz_localize(None)\n",
        "        except: continue\n",
        "        vals = np.array([_ndbc_to_float(v) for v in parts[4:]], dtype=float)\n",
        "        if vals.size != freqs.size:\n",
        "            if vals.size < freqs.size: vals = np.pad(vals, (0, freqs.size - vals.size), constant_values=np.nan)\n",
        "            else: vals = vals[:freqs.size]\n",
        "        def m0(mask): return np.nansum(vals[mask] * dfw[mask]) if mask.any() else np.nan\n",
        "        high_HIG = 4.0*np.sqrt(m0(mask_hig))   if mask_hig.any()   else np.nan\n",
        "        Hswell   = 4.0*np.sqrt(m0(mask_swell)) if mask_swell.any() else np.nan\n",
        "        Hsea     = 4.0*np.sqrt(m0(mask_sea))   if mask_sea.any()   else np.nan\n",
        "        rows.append((ts, high_HIG, Hswell, Hsea))\n",
        "    if not rows: return pd.DataFrame()\n",
        "    return (pd.DataFrame(rows, columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "            .sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True))\n",
        "\n",
        "df_stdmet = build_stdmet(YEARS)\n",
        "df_swden  = pd.concat([parse_swden_year(txt) for y in YEARS if (txt:=fetch_swden_year(y))], ignore_index=True) \\\n",
        "               if YEARS else pd.DataFrame(columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "\n",
        "# -------------------\n",
        "# Common 6-min grid (based on available sources)\n",
        "# -------------------\n",
        "def _interp_timegrid(t_src:pd.Series, x_src:np.ndarray, t_grid:pd.DatetimeIndex)->np.ndarray:\n",
        "    s = pd.Series(x_src, index=pd.to_datetime(t_src))\n",
        "    u = s.reindex(pd.to_datetime(sorted(set(s.index).union(set(t_grid)))))\n",
        "    u = u.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    return u.reindex(pd.to_datetime(t_grid)).to_numpy()\n",
        "\n",
        "t0_candidates=[]; t1_candidates=[]\n",
        "for d,col in [(df_varWL,\"time\"), (df_wl6,\"time\"), (df_wind,\"time\"), (df_air_merged,\"time\"),\n",
        "              (df_stdmet,\"time\"), (df_swden,\"time\")]:\n",
        "    if len(d):\n",
        "        t0_candidates.append(d[col].min()); t1_candidates.append(d[col].max())\n",
        "if not t0_candidates:\n",
        "    raise RuntimeError(\"No sources available; check API windows.\")\n",
        "t0 = max(t0_candidates); t1 = min(t1_candidates)\n",
        "t_grid = pd.date_range(t0, t1, freq=f\"{DT_MINUTES}min\")\n",
        "\n",
        "# -------------------\n",
        "# Assemble seq_mat on grid (NO parquet; all online)\n",
        "# -------------------\n",
        "seq_mat = {}\n",
        "\n",
        "# target: var_wl (requested)\n",
        "seq_mat[\"var_wl\"] = _interp_timegrid(df_varWL[\"time\"], df_varWL[\"var_wl\"].values, t_grid)\n",
        "\n",
        "# WL raw (optional for QC)\n",
        "seq_mat[\"wl\"] = _interp_timegrid(df_wl6[\"time\"], df_wl6[\"wl\"].values, t_grid) if len(df_wl6) else np.full(len(t_grid), np.nan)\n",
        "\n",
        "# wind to 6-min grid\n",
        "if len(df_wind):\n",
        "    seq_mat[\"wind9435380_speed\"] = _interp_timegrid(df_wind[\"time\"], df_wind[\"wind9435380_speed\"].values, t_grid)\n",
        "    seq_mat[\"wind9435380_dir\"]   = _interp_timegrid(df_wind[\"time\"], df_wind[\"wind9435380_dir\"].values,   t_grid)\n",
        "\n",
        "# air pressure (Pa) on 6-min grid\n",
        "if len(df_air_merged):\n",
        "    seq_mat[\"air_pressure_pa\"] = _interp_timegrid(df_air_merged[\"time\"], df_air_merged[\"air_pressure_pa\"].values, t_grid)\n",
        "\n",
        "# 46050 stdmet (EXCLUDE Hs from features, but keep for QC if you want)\n",
        "if len(df_stdmet):\n",
        "    for c in [\"DPD\",\"APD\",\"MWD\",\"Hs\"]:\n",
        "        seq_mat[c] = _interp_timegrid(df_stdmet[\"time\"], df_stdmet[c].values, t_grid)\n",
        "\n",
        "# 46050 swden bands\n",
        "if len(df_swden):\n",
        "    for c in [\"high_HIG\",\"Hswell\",\"Hsea\"]:\n",
        "        seq_mat[c] = _interp_timegrid(df_swden[\"time\"], df_swden[c].values, t_grid)\n",
        "\n",
        "# Final dataframe\n",
        "df_all = pd.DataFrame({\"time\": t_grid})\n",
        "for k,v in seq_mat.items():\n",
        "    df_all[k] = v\n",
        "\n",
        "# -------------------\n",
        "# Interactive, linked plots (QC)\n",
        "# -------------------\n",
        "def _col_in(df, name): return (name in df.columns) and pd.api.types.is_numeric_dtype(df[name])\n",
        "def _pick(df, names): return [c for c in names if _col_in(df, c)]\n",
        "\n",
        "def _stacked_subplot(df, cols, title, start=None, end=None):\n",
        "    if not cols:\n",
        "        fig = go.FigureWidget(); fig.update_layout(title=f\"{title} (no series)\", height=150); return fig\n",
        "    d = df.sort_values(\"time\")\n",
        "    if start is not None: d = d[d[\"time\"] >= pd.Timestamp(start)]\n",
        "    if end   is not None: d = d[d[\"time\"] <= pd.Timestamp(end)]\n",
        "    rows=len(cols)\n",
        "    fig = make_subplots(rows=rows, cols=1, shared_xaxes=True, vertical_spacing=0.007,\n",
        "                        row_heights=[1.0/rows]*rows)\n",
        "    fig = go.FigureWidget(fig)\n",
        "    for i,c in enumerate(cols, start=1):\n",
        "        y = d[c]\n",
        "        fig.add_trace(go.Scattergl(x=d[\"time\"], y=y, name=c, mode=\"lines\",\n",
        "                                   line=dict(width=1),\n",
        "                                   hovertemplate=\"%{x|%Y-%m-%d %H:%M}<br>\"+c+\": %{y:.4g}<extra></extra>\"),\n",
        "                      row=i, col=1)\n",
        "        fig.update_yaxes(title_text=c, row=i, col=1)\n",
        "    fig.update_layout(title=title, height=max(280, rows*85), hovermode=\"x unified\",\n",
        "                      margin=dict(t=40,b=10,l=60,r=15))\n",
        "    fig.update_xaxes(rangeslider=dict(visible=True), row=rows, col=1)\n",
        "    return fig\n",
        "\n",
        "def plot_forcings_linked(df_all, start=None, end=None,\n",
        "                         group_layout=(\"Target\",\"Waves\",\"Wind\",\"Pressure\",\"Other\")):\n",
        "    assert \"time\" in df_all.columns\n",
        "    candidates = [c for c in df_all.columns if c!=\"time\" and pd.api.types.is_numeric_dtype(df_all[c])]\n",
        "    # Buckets\n",
        "    target   = _pick(df_all, [\"var_wl\",\"wl\"])\n",
        "    waves    = _pick(df_all, [\"high_HIG\",\"Hswell\",\"Hsea\",\"DPD\",\"APD\",\"MWD\",\"Hs\"])\n",
        "    wind     = [c for c in candidates if c.startswith(\"wind9435380\")]\n",
        "    pressure = _pick(df_all, [\"air_pressure_pa\"])\n",
        "    other    = [c for c in candidates if c not in set(target)|set(waves)|set(wind)|set(pressure)]\n",
        "    groups = {\"Target\":target, \"Waves\":waves, \"Wind\":wind, \"Pressure\":pressure, \"Other\":other}\n",
        "\n",
        "    figs=[]\n",
        "    for g in group_layout:\n",
        "        cols = groups.get(g, [])\n",
        "        figs.append(_stacked_subplot(df_all, cols, f\"{g}\", start=start, end=end))\n",
        "\n",
        "    # link x across figures\n",
        "    syncing={\"busy\":False}\n",
        "    def _link(src,*others):\n",
        "        def _on_relayout(_, relayout_data):\n",
        "            if syncing[\"busy\"]: return\n",
        "            if not any(k.startswith(\"xaxis.range\") or k==\"xaxis.autorange\" for k in relayout_data.keys()):\n",
        "                return\n",
        "            x0 = relayout_data.get(\"xaxis.range[0]\"); x1 = relayout_data.get(\"xaxis.range[1]\")\n",
        "            autor = relayout_data.get(\"xaxis.autorange\")\n",
        "            syncing[\"busy\"]=True\n",
        "            try:\n",
        "                for f in others:\n",
        "                    if autor is True: f.update_xaxes(autorange=True)\n",
        "                    elif x0 is not None and x1 is not None: f.update_xaxes(range=[x0,x1])\n",
        "            finally:\n",
        "                syncing[\"busy\"]=False\n",
        "        src.on_relayout(_on_relayout)\n",
        "    for i,src in enumerate(figs): _link(src, *[f for j,f in enumerate(figs) if j!=i])\n",
        "    for f in figs: display(f)\n",
        "    return figs\n",
        "\n",
        "_ = plot_forcings_linked(df_all, start=PRED_START, end=PRED_END,\n",
        "                         group_layout=(\"Target\",\"Waves\",\"Wind\",\"Pressure\"))\n",
        "\n",
        "# ------------------------------\n",
        "# ⛔ Hard stop (QC before any ML)\n",
        "# ------------------------------\n",
        "STOP_BEFORE_ML = True\n",
        "if STOP_BEFORE_ML:\n",
        "    print(\"Stopping here so you can verify forcings & var_wl before any modeling.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# Colab: Seiche forecast with PatchTST (XGB feature pick, forced baro_var4h)\n",
        "# ================================================\n",
        "\n",
        "# -- If needed (Colab usually has torch already):\n",
        "!pip -q install xgboost\n",
        "\n",
        "import io, gzip, re, json, os, time, math, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "from contextlib import nullcontext\n",
        "from sklearn.metrics import r2_score\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# --------------------------------\n",
        "# USER KNOBS\n",
        "# --------------------------------\n",
        "YEARS                = list(range(2008, 2026))\n",
        "DT_MINUTES           = 6                     # base grid\n",
        "HISTORY_HOURS        = 12\n",
        "HORIZONS_MIN         = [60, 120, 240]\n",
        "PRIMARY_HORIZON      = 60\n",
        "MAX_GAP_HOURS        = 5\n",
        "TOP_LAG_FEATURES     = 16                   # keep a bit larger; we’ll still force baro features\n",
        "\n",
        "# WL bandpass + variance\n",
        "VAR_WIN_HOURS        = 2.0                  # centered rolling variance window (hours)\n",
        "WL_BAND_MIN_MIN      = 20.0                 # minutes\n",
        "WL_BAND_MAX_MIN      = 30.0                 # minutes\n",
        "\n",
        "# NOAA stations + ranges\n",
        "NOAA_WIND_STATION    = \"9435380\"            # Newport / South Beach (CO-OPS)\n",
        "NOAA_BEGIN_DATE      = \"20080101\"\n",
        "NOAA_END_DATE        = \"20251001\"\n",
        "NOAA_CHUNK_DAYS      = 365\n",
        "NOAA_WL_STATION      = \"9435380\"\n",
        "WL_BEGIN_DATE        = \"20080101\"\n",
        "WL_END_DATE          = \"20251001\"\n",
        "\n",
        "# NDBC buoy 46050 spectra/stdmet\n",
        "BUOY                 = \"46050\"\n",
        "\n",
        "# HMSC monthly archive\n",
        "HMSC_BASE_URL        = \"http://weather.hmsc.oregonstate.edu/weather/weatherproject/archive/{yyyy}/HMSC_{yyyymm}.dat\"\n",
        "HMSC_UTC_OFFSET_H    = +8\n",
        "\n",
        "# Barometric pressure high-pass + variance\n",
        "BARO_HP_CUTOFF_H     = 4.0                  # high-pass cutoff (hours)\n",
        "BARO_VAR_WIN_H       = 2.0                  # centered rolling variance (hours)\n",
        "FORCE_BARO_IN_XGB    = True                 # force include baro features\n",
        "BARO_FORCE_LAGS_MIN  = [0, 240]             # lags to try/force (minutes)\n",
        "\n",
        "# Earthquake events (UTC) — add/edit as needed\n",
        "EQ_EVENTS = [\n",
        "    (\"2011-03-11 05:46:00\", 9.1),   # Tohoku\n",
        "    (\"2015-09-16 22:54:00\", 8.3),   # Illapel, Chile\n",
        "]\n",
        "# EQ amplitude ~10^(M-7); decay tau (hours) ≈ 6.67*M - 30 (M9 ≈ 30h, M7.5 ≈ 20h)\n",
        "EQ_DECAY_TAU_FUNC = lambda M: max(8.0, 6.6667 * M - 30.0)\n",
        "EQ_AMPLITUDE_FUNC = lambda M: 10.0**(M - 7.0)\n",
        "\n",
        "# Spike emphasis (targets) for PatchTST training\n",
        "WEIGHT_SPIKES       = True\n",
        "SPIKE_P90           = 0.90\n",
        "SPIKE_WEIGHT        = 2.0\n",
        "\n",
        "# PatchTST model knobs\n",
        "PATCH_LEN           = 24      # 24*6min = 144 min patches\n",
        "PATCH_STRIDE        = 6       # slide by 36 min\n",
        "D_MODEL             = 192\n",
        "NHEAD               = 4\n",
        "NLAYERS             = 3\n",
        "FF_DIM              = 384\n",
        "DROPOUT             = 0.15\n",
        "LR                  = 2e-3\n",
        "EPOCHS              = 20\n",
        "BATCH_SIZE          = 512\n",
        "\n",
        "# Plot window\n",
        "PRED_SPLIT          = \"test\"  # \"test\" or \"val\"\n",
        "PRED_START          = pd.Timestamp(\"2022-11-01 00:00:00\")\n",
        "PRED_END            = pd.Timestamp(\"2022-12-20 23:59:59\")\n",
        "PLOT_LAST_DAYS      = 40\n",
        "\n",
        "# Time splits\n",
        "TRAIN_END           = pd.Timestamp(\"2018-12-31 23:59:59\")\n",
        "VAL_END             = pd.Timestamp(\"2022-12-31 23:59:59\")\n",
        "MAX_SAMPLES_SPLIT   = 60_000\n",
        "\n",
        "# --------------------------------\n",
        "# HTTP session with retries\n",
        "# --------------------------------\n",
        "def _make_session():\n",
        "    s = requests.Session()\n",
        "    retry = Retry(total=4, backoff_factor=0.6, status_forcelist=(429,500,502,503,504),\n",
        "                  allowed_methods=[\"HEAD\",\"GET\",\"OPTIONS\"], raise_on_status=False)\n",
        "    s.headers.update({\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) SeichePatchTST/1.0\", \"Accept\":\"*/*\"})\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
        "    return s\n",
        "session = _make_session()\n",
        "\n",
        "def _ndbc_to_float(tok:str) -> float:\n",
        "    s = str(tok).strip()\n",
        "    if s == \"\" or s.upper() == \"MM\": return np.nan\n",
        "    if re.fullmatch(r\"-?9+(\\.0+)?\", s): return np.nan\n",
        "    try: return float(s)\n",
        "    except: return np.nan\n",
        "\n",
        "def _datestr(dt) -> str:\n",
        "    return pd.Timestamp(dt).strftime(\"%Y%m%d\")\n",
        "\n",
        "# --------------------------------\n",
        "# NDBC 46050 STDMET (Hs/DPD/APD/MWD) — we will EXCLUDE Hs later\n",
        "# --------------------------------\n",
        "STDMET_PHP_PAT = f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}h\"+\"{year}.txt.gz&dir=data/historical/stdmet/\"\n",
        "\n",
        "def fetch_stdmet_year(year:int)->str|None:\n",
        "    for url in (STDMET_PHP_PAT.format(year=year),):\n",
        "        try:\n",
        "            r = session.get(url, timeout=40)\n",
        "            if r.status_code==200 and r.text and \"Page Not Found\" not in r.text:\n",
        "                return r.text\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def parse_stdmet_text(txt:str)->pd.DataFrame:\n",
        "    rows=[]\n",
        "    for ln in txt.splitlines():\n",
        "        if not ln or ln.lstrip().startswith(\"#\"): continue\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 6: continue\n",
        "        y,mo,dy,hh,mi = parts[:5]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh); mi=int(mi)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, minute=mi, tz=\"UTC\").tz_localize(None)\n",
        "        except: continue\n",
        "        vals = parts[5:]\n",
        "        if len(vals) < 13: vals = (vals + [\"MM\"]*13)[:13]\n",
        "        vals = [_ndbc_to_float(v) for v in vals]\n",
        "        rows.append([ts] + vals)\n",
        "    if not rows: return pd.DataFrame()\n",
        "    cols = [\"time\",\"WDIR\",\"WSPD\",\"GST\",\"WVHT\",\"DPD\",\"APD\",\"MWD\",\"PRES\",\"ATMP\",\"WTMP\",\"DEWP\",\"VIS\",\"TIDE\"]\n",
        "    df = pd.DataFrame(rows, columns=cols).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    df = df.rename(columns={\"WVHT\":\"Hs\"}).reset_index(drop=True)\n",
        "    for c,thr in [(\"Hs\",50.0),(\"DPD\",50.0),(\"APD\",50.0)]:\n",
        "        if c in df.columns: df.loc[df[c]>thr, c]=np.nan\n",
        "    df.loc[(df[\"MWD\"]<0)|(df[\"MWD\"]>360),\"MWD\"]=np.nan\n",
        "    keep = [\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"]\n",
        "    for k in keep:\n",
        "        if k not in df.columns: df[k]=np.nan\n",
        "    return df[keep]\n",
        "\n",
        "def build_stdmet(years: List[int]) -> pd.DataFrame:\n",
        "    frames=[]\n",
        "    for y in years:\n",
        "        txt = fetch_stdmet_year(y)\n",
        "        if not txt: continue\n",
        "        dfy = parse_stdmet_text(txt)\n",
        "        if not dfy.empty: frames.append(dfy)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"Hs\",\"DPD\",\"APD\",\"MWD\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "df_stdmet = build_stdmet(YEARS)\n",
        "\n",
        "# --------------------------------\n",
        "# NDBC 46050 SWDEN → spectral bands high_HIG (<0.05Hz), Hswell (0.05–0.14Hz), Hsea (0.14–0.30Hz)\n",
        "# --------------------------------\n",
        "SWDEN_PHP_PATS = [\n",
        "    f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}w\"+\"{year}.txt.gz&dir=data/historical/swden/\",\n",
        "    f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={BUOY}w\"+\"{year}.txt.gz&dir=data/swden/\"+BUOY+\"/\",\n",
        "]\n",
        "\n",
        "def fetch_swden_year(year:int)->str|None:\n",
        "    for url in SWDEN_PHP_PATS:\n",
        "        try:\n",
        "            r = session.get(url.format(year=year), timeout=40)\n",
        "            if r.status_code==200 and r.text and \"Page Not Found\" not in r.text:\n",
        "                return r.text\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def split_swden_blocks(lines:List[str]):\n",
        "    header=[]; i=0\n",
        "    while i<len(lines) and lines[i].startswith(\"#\"):\n",
        "        header.append(lines[i]); i+=1\n",
        "    while i<len(lines) and not lines[i].strip():\n",
        "        i+=1\n",
        "    freq_line = lines[i].rstrip(\"\\n\") if i<len(lines) else None\n",
        "    i += 1 if i<len(lines) else 0\n",
        "    data=[ln for ln in lines[i:] if ln.strip()]\n",
        "    return header, freq_line, data\n",
        "\n",
        "def parse_swden_year(txt:str)->pd.DataFrame:\n",
        "    lines = txt.splitlines()\n",
        "    _, freq_line, data_lines = split_swden_blocks(lines)\n",
        "    if not freq_line: return pd.DataFrame()\n",
        "    freq_tokens = [t for t in re.split(r\"\\s+\", freq_line.strip()) if t]\n",
        "    if any(k in freq_tokens[:6] for k in (\"YY\",\"MM\",\"DD\",\"hh\")):\n",
        "        idx = lines.index(freq_line)\n",
        "        j=idx+1\n",
        "        while j<len(lines) and not lines[j].strip(): j+=1\n",
        "        if j<len(lines):\n",
        "            freq_tokens = [t for t in re.split(r\"\\s+\", lines[j].strip()) if t]\n",
        "            data_lines  = [ln for ln in lines[j+1:] if ln.strip()]\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _num(tok):\n",
        "        try: return float(tok)\n",
        "        except: return np.nan\n",
        "    freqs = np.array([_num(t) for t in freq_tokens], dtype=float)\n",
        "    freqs = freqs[np.isfinite(freqs)]\n",
        "    if freqs.size==0: return pd.DataFrame()\n",
        "\n",
        "    dfw = np.empty_like(freqs)\n",
        "    if freqs.size==1:\n",
        "        dfw[:] = 0.0\n",
        "    else:\n",
        "        dfw[1:-1] = 0.5*(freqs[2:] - freqs[:-2])\n",
        "        dfw[0]    = freqs[1] - freqs[0]\n",
        "        dfw[-1]   = freqs[-1] - freqs[-2]\n",
        "        dfw = np.where(dfw<0, np.nan, dfw)\n",
        "\n",
        "    mask_hig   = (freqs < 0.05)\n",
        "    mask_swell = (freqs >= 0.05) & (freqs <= 0.14)\n",
        "    mask_sea   = (freqs > 0.14) & (freqs <= 0.30)\n",
        "\n",
        "    rows=[]\n",
        "    for ln in data_lines:\n",
        "        parts = re.split(r\"\\s+\", ln.strip())\n",
        "        if len(parts) < 4: continue\n",
        "        y,mo,dy,hh = parts[:4]\n",
        "        try:\n",
        "            y=int(y); mo=int(mo); dy=int(dy); hh=int(hh)\n",
        "            year = 2000+y if y<100 else y\n",
        "            ts = pd.Timestamp(year=year, month=mo, day=dy, hour=hh, tz=\"UTC\").tz_localize(None)\n",
        "        except: continue\n",
        "\n",
        "        vals = np.array([_ndbc_to_float(v) for v in parts[4:]], dtype=float)\n",
        "        if vals.size != freqs.size:\n",
        "            if vals.size < freqs.size: vals = np.pad(vals, (0, freqs.size - vals.size), constant_values=np.nan)\n",
        "            else: vals = vals[:freqs.size]\n",
        "\n",
        "        def m0(mask): return np.nansum(vals[mask] * dfw[mask]) if mask.any() else np.nan\n",
        "        high_HIG = 4.0*np.sqrt(m0(mask_hig))   if mask_hig.any()   else np.nan\n",
        "        Hswell   = 4.0*np.sqrt(m0(mask_swell)) if mask_swell.any() else np.nan\n",
        "        Hsea     = 4.0*np.sqrt(m0(mask_sea))   if mask_sea.any()   else np.nan\n",
        "        rows.append((ts, high_HIG, Hswell, Hsea))\n",
        "\n",
        "    if not rows: return pd.DataFrame()\n",
        "    return (pd.DataFrame(rows, columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "            .sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True))\n",
        "\n",
        "def build_swden(years: List[int]) -> pd.DataFrame:\n",
        "    frames=[]\n",
        "    for y in years:\n",
        "        txt = fetch_swden_year(y)\n",
        "        if not txt: continue\n",
        "        dfy = parse_swden_year(txt)\n",
        "        if not dfy.empty: frames.append(dfy)\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"high_HIG\",\"Hswell\",\"Hsea\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "df_swden = build_swden(YEARS)\n",
        "\n",
        "# --------------------------------\n",
        "# NOAA CO-OPS wind (hourly) → speed, dir (no u/v in features)\n",
        "# --------------------------------\n",
        "def parse_noaa_wind_csv(txt:str)->pd.DataFrame:\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    tcol = [c for c in df.columns if str(c).strip().lower() in (\"t\",\"time\",\"date time\",\"date_time\",\"date\")]\n",
        "    tcol = tcol[0] if tcol else df.columns[0]\n",
        "    df[\"time\"] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    def _find(keys):\n",
        "        keys = [k.lower() for k in keys]\n",
        "        for c in df.columns:\n",
        "            s = str(c).lower()\n",
        "            if any(k == s or k in s for k in keys): return c\n",
        "        return None\n",
        "    sp_col = _find([\"s\",\"speed\"])\n",
        "    dir_col= _find([\"d\",\"dir\",\"direction\"])\n",
        "    out = pd.DataFrame({\"time\":df[\"time\"]})\n",
        "    out[\"wind_speed\"] = pd.to_numeric(df[sp_col], errors=\"coerce\") if sp_col else np.nan\n",
        "    out[\"wind_dir\"]   = pd.to_numeric(df[dir_col], errors=\"coerce\") if dir_col else np.nan\n",
        "    out.loc[(out[\"wind_dir\"]<0)|(out[\"wind_dir\"]>360),\"wind_dir\"]=np.nan\n",
        "    out.loc[(out[\"wind_speed\"]<0)|(out[\"wind_speed\"]>100),\"wind_speed\"]=np.nan\n",
        "    return (out.sort_values(\"time\").dropna(subset=[\"time\"])\n",
        "            .drop_duplicates(\"time\").reset_index(drop=True))\n",
        "\n",
        "def fetch_noaa_wind_dataframe(begin_date:str, end_date:str, station:str, days_per_chunk:int=365)->pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\"); end = pd.to_datetime(end_date, format=\"%Y%m%d\")\n",
        "    chunks=[]; cur=start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        chunks.append((cur, chunk_end)); cur = chunk_end + pd.Timedelta(days=1)\n",
        "    frames=[]\n",
        "    for a,b in chunks:\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(a)}&end_date={_datestr(b)}\"\n",
        "               f\"&station={station}&product=wind&time_zone=gmt&interval=h&units=metric\"\n",
        "               \"&application=SeichePatchTST&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=60)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wind_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except: pass\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"wind_speed\",\"wind_dir\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "df_noaa_wind = fetch_noaa_wind_dataframe(NOAA_BEGIN_DATE, NOAA_END_DATE, NOAA_WIND_STATION, days_per_chunk=NOAA_CHUNK_DAYS)\n",
        "\n",
        "# ================= NOAA + HMSC baro merge rules =================\n",
        "# - NOAA air_pressure (hourly, metric) -> hPa -> Pa\n",
        "# - HMSC AvgBP (inHg, 5-min) -> Pa\n",
        "# - Fill NOAA gaps:\n",
        "#     * runs >= 3 hours: fill from HMSC (on hourly grid)\n",
        "#     * runs < 3 hours: spline interpolation (NOAA only)\n",
        "# - Then interpolate merged Pa to t_grid and compute rolling variance\n",
        "\n",
        "INHG_TO_PA = 3386.389  # 1 inch Hg ≈ 3386.389 Pa\n",
        "\n",
        "# --- 1) Ensure df_air from NOAA exists (hourly), convert to Pa ---\n",
        "if 'df_air' not in globals():\n",
        "    raise RuntimeError(\"df_air (NOAA air pressure) is not defined. Fetch it first.\")\n",
        "df_air = df_air.dropna(subset=[\"time\"]).drop_duplicates(\"time\").sort_values(\"time\").reset_index(drop=True)\n",
        "df_air[\"air_pressure_pa\"] = (df_air[\"air_pressure_hpa\"].astype(float) * 100.0).astype(\"float64\")\n",
        "\n",
        "# Hourly base grid for merging/filling\n",
        "ap_start = df_air[\"time\"].min()\n",
        "ap_end   = df_air[\"time\"].max()\n",
        "hourly_grid = pd.date_range(ap_start.floor(\"h\"), ap_end.ceil(\"h\"), freq=\"h\")\n",
        "\n",
        "s_noaa_pa = (df_air.set_index(\"time\")[\"air_pressure_pa\"]\n",
        "             .reindex(hourly_grid)  # align to hourly\n",
        "             .astype(\"float64\"))\n",
        "\n",
        "# --- 2) Build HMSC AvgBP (inHg) -> Pa over same window ---\n",
        "def fetch_hmsc_month_text(year:int, month:int) -> str|None:\n",
        "    url = f\"http://weather.hmsc.oregonstate.edu/weather/weatherproject/archive/{year}/HMSC_{year}{month:02d}.dat\"\n",
        "    try:\n",
        "        r = session.get(url, timeout=60)\n",
        "        if r.status_code==200 and r.content:\n",
        "            return r.content.decode(\"utf-8\", \"ignore\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def parse_hmsc_dat_avgbp_pa(txt:str) -> pd.DataFrame:\n",
        "    # Find the header line that starts the CSV (TIMESTAMP,...)\n",
        "    lines = txt.splitlines()\n",
        "    start_idx = None\n",
        "    for i,ln in enumerate(lines):\n",
        "        s = ln.strip().strip('\"')\n",
        "        if s.startswith(\"TIMESTAMP\"):\n",
        "            start_idx = i\n",
        "            break\n",
        "    if start_idx is None:\n",
        "        return pd.DataFrame(columns=[\"time\",\"bp_pa\"])\n",
        "\n",
        "    content = \"\\n\".join(lines[start_idx:])\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "    # Time\n",
        "    t = pd.to_datetime(df.iloc[:,0], errors=\"coerce\")\n",
        "    # Treat timestamps as America/Los_Angeles local, convert to UTC naive for merging with NOAA GMT\n",
        "    try:\n",
        "        t = t.dt.tz_localize(\"America/Los_Angeles\", nonexistent=\"shift_forward\", ambiguous=\"NaT\") \\\n",
        "             .dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
        "    except Exception:\n",
        "        # Fallback: assume already UTC if tz db not available\n",
        "        pass\n",
        "\n",
        "    df.rename(columns={df.columns[0]:\"TIMESTAMP\"}, inplace=True)\n",
        "\n",
        "    # Find AvgBP column (case-insensitive match)\n",
        "    target = None\n",
        "    for c in df.columns:\n",
        "        if \"avgbp\" in str(c).strip().lower():\n",
        "            target = c; break\n",
        "    if target is None:\n",
        "        return pd.DataFrame(columns=[\"time\",\"bp_pa\"])\n",
        "\n",
        "    avgbp_inhg = pd.to_numeric(df[target], errors=\"coerce\").astype(\"float64\")\n",
        "    # sanity: realistic sea-level pressure in inches Hg (~27–32)\n",
        "    avgbp_inhg = avgbp_inhg.where(avgbp_inhg.between(26.0, 33, inclusive=\"both\"))\n",
        "\n",
        "    out = pd.DataFrame({\"time\": t, \"bp_pa\": avgbp_inhg * INHG_TO_PA})\n",
        "    out = (out.dropna(subset=[\"time\"])\n",
        "              .sort_values(\"time\")\n",
        "              .drop_duplicates(\"time\")\n",
        "              .reset_index(drop=True))\n",
        "    return out\n",
        "\n",
        "def month_iter(start:pd.Timestamp, end:pd.Timestamp):\n",
        "    y,m = start.year, start.month\n",
        "    while pd.Timestamp(year=y, month=m, day=1) <= end:\n",
        "        yield (y,m)\n",
        "        if m==12: y,m=y+1,1\n",
        "        else: m+=1\n",
        "\n",
        "# Fetch months that cover NOAA window (pad one month on each side)\n",
        "start_pad = (ap_start - pd.Timedelta(days=31)).normalize()\n",
        "end_pad   = (ap_end   + pd.Timedelta(days=31)).normalize()\n",
        "\n",
        "hmsc_frames = []\n",
        "for y,m in month_iter(start_pad, end_pad):\n",
        "    txt = fetch_hmsc_month_text(y,m)\n",
        "    if not txt: continue\n",
        "    dfm = parse_hmsc_dat_avgbp_pa(txt)\n",
        "    if not dfm.empty:\n",
        "        # keep only within padded window to reduce size\n",
        "        mask = (dfm[\"time\"] >= start_pad) & (dfm[\"time\"] <= end_pad)\n",
        "        hmsc_frames.append(dfm.loc[mask])\n",
        "\n",
        "df_hmsc_bp = (pd.concat(hmsc_frames, ignore_index=True)\n",
        "              .sort_values(\"time\")\n",
        "              .drop_duplicates(\"time\")\n",
        "              .reset_index(drop=True)) if hmsc_frames else pd.DataFrame(columns=[\"time\",\"bp_pa\"])\n",
        "\n",
        "# Resample HMSC bp to hourly with time interpolation (so we can drop into NOAA hourly gaps)\n",
        "if len(df_hmsc_bp):\n",
        "    s_hmsc_pa_hourly = (df_hmsc_bp.set_index(\"time\")[\"bp_pa\"]\n",
        "                        .reindex(hourly_grid)\n",
        "                        .interpolate(method=\"time\", limit_area=\"inside\")\n",
        "                        .astype(\"float64\"))\n",
        "else:\n",
        "    s_hmsc_pa_hourly = pd.Series(index=hourly_grid, dtype=\"float64\")\n",
        "\n",
        "# --- 3) Fill rules on hourly grid ---\n",
        "filled = s_noaa_pa.copy()\n",
        "\n",
        "# Identify contiguous NaN runs\n",
        "nan_mask = filled.isna().to_numpy()\n",
        "if nan_mask.any():\n",
        "    # group NaN runs\n",
        "    idx = np.arange(len(nan_mask))\n",
        "    # start/end indices of each NaN block\n",
        "    starts = np.where(~nan_mask[:-1] & nan_mask[1:])[0] + 1 if len(nan_mask) > 1 else np.array([], dtype=int)\n",
        "    ends   = np.where(nan_mask[:-1] & ~nan_mask[1:])[0] + 1 if len(nan_mask) > 1 else np.array([], dtype=int)\n",
        "    # edge cases\n",
        "    if nan_mask[0]:  starts = np.r_[0, starts]\n",
        "    if nan_mask[-1]: ends   = np.r_[ends, len(nan_mask)]\n",
        "\n",
        "    for s,e in zip(starts, ends):\n",
        "        run_len_hours = e - s  # hourly freq\n",
        "        if run_len_hours >= 3:\n",
        "            # >= 3h: fill from HMSC hourly if available\n",
        "            seg_idx = hourly_grid[s:e]\n",
        "            hmsc_vals = s_hmsc_pa_hourly.loc[seg_idx]\n",
        "            filled.iloc[s:e] = hmsc_vals.values\n",
        "    # For any remaining NaNs (i.e., runs < 3h or where HMSC missing), do spline on NOAA only\n",
        "    if filled.isna().any():\n",
        "        # spline requires some finite points; fall back to 'time' if sparse\n",
        "        try:\n",
        "            filled = filled.interpolate(method=\"spline\", order=3, limit_area=\"inside\")\n",
        "        except Exception:\n",
        "            filled = filled.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "\n",
        "# --- 4) Push merged Pa to your 6-min model grid and compute rolling variance ---\n",
        "def _interp_to_grid_series(times: pd.DatetimeIndex, values: np.ndarray, grid: pd.DatetimeIndex) -> np.ndarray:\n",
        "    s = pd.Series(values, index=times)\n",
        "    u = s.reindex(pd.to_datetime(sorted(set(s.index).union(set(grid)))))\n",
        "    u = u.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    return u.reindex(pd.to_datetime(grid)).to_numpy()\n",
        "\n",
        "if 't_grid' not in globals():\n",
        "    if 'df_varWL' in globals() and len(df_varWL):\n",
        "        t_grid = pd.date_range(df_varWL[\"time\"].min(), df_varWL[\"time\"].max(), freq=f\"{DT_MINUTES}min\")\n",
        "    else:\n",
        "        raise RuntimeError(\"t_grid is required to map air pressure to model grid.\")\n",
        "\n",
        "air_pa_on_grid = _interp_to_grid_series(hourly_grid, filled.to_numpy(dtype=\"float64\"), t_grid)\n",
        "\n",
        "# centered rolling variance in Pa^2\n",
        "win = max(3, int(round((BARO_VAR_WIN_H*60.0)/DT_MINUTES)))  # BARO_VAR_WIN_H hours\n",
        "minp = max(2, int(0.8*win))\n",
        "baro_var = (pd.Series(air_pa_on_grid, index=t_grid)\n",
        "            .rolling(window=win, center=True, min_periods=minp).var()\n",
        "            .astype(\"float32\"))\n",
        "\n",
        "# --- 5) Inject into seq_mat and df_all ---\n",
        "if 'seq_mat' not in globals():\n",
        "    seq_mat = {}\n",
        "seq_mat[\"air_pressure_pa\"] = air_pa_on_grid.astype(\"float32\")\n",
        "seq_mat[\"baro_var4h\"]      = baro_var.to_numpy(dtype=\"float32\")  # keep name for downstream code\n",
        "\n",
        "# Merge into df_all\n",
        "base = pd.DataFrame({\"time\": t_grid, \"air_pressure_pa\": seq_mat[\"air_pressure_pa\"], \"baro_var4h\": seq_mat[\"baro_var4h\"]})\n",
        "if 'df_all' in globals() and len(df_all):\n",
        "    df_all = df_all.merge(base, on=\"time\", how=\"left\")\n",
        "else:\n",
        "    df_all = base.copy()\n",
        "\n",
        "# Quick diagnostics\n",
        "print(\"Air pressure (Pa) non-NaN on t_grid:\", np.isfinite(seq_mat[\"air_pressure_pa\"]).sum(), \"/\", len(t_grid))\n",
        "print(\"baro_var4h non-NaN on t_grid:\", np.isfinite(seq_mat[\"baro_var4h\"]).sum(), \"/\", len(t_grid))\n",
        "\n",
        "# --------------------------------\n",
        "# WL (6-min) → bandpass(20–30 min) → centered variance (2 h)\n",
        "# --------------------------------\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def parse_noaa_wl_csv(txt:str)->pd.DataFrame:\n",
        "    if txt.strip().lower().startswith(\"error\"):\n",
        "        return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df = pd.read_csv(io.StringIO(txt))\n",
        "    if df.shape[1] < 2: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df.columns = [str(c).strip().lower().replace(\"  \",\" \") for c in df.columns]\n",
        "    tcol=None\n",
        "    for k in (\"date time\",\"date_time\",\"time\",\"t\",\"date\"):\n",
        "        if k in df.columns: tcol=k; break\n",
        "    if tcol is None: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    df[\"time\"] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    vcol=None\n",
        "    for k in (\"water level\",\"water_level\",\"wl\",\"v\"):\n",
        "        if k in df.columns: vcol=k; break\n",
        "    if vcol is None: vcol = df.columns[1]\n",
        "    df[\"wl\"] = pd.to_numeric(df[vcol], errors=\"coerce\")\n",
        "    out = df[[\"time\",\"wl\"]].dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    out.loc[(out[\"wl\"]<-50)|(out[\"wl\"]>50),\"wl\"]=np.nan\n",
        "    if out[\"wl\"].notna().sum()==0: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def fetch_noaa_wl(begin_date:str, end_date:str, station:str, days_per_chunk:int=31)->pd.DataFrame:\n",
        "    start = pd.to_datetime(begin_date, format=\"%Y%m%d\"); end = pd.to_datetime(end_date, format=\"%Y%m%d\")\n",
        "    chunks=[]; cur=start\n",
        "    while cur <= end:\n",
        "        chunk_end = min(end, cur + pd.Timedelta(days=days_per_chunk-1))\n",
        "        chunks.append((cur, chunk_end)); cur = chunk_end + pd.Timedelta(days=1)\n",
        "    frames=[]\n",
        "    for a,b in chunks:\n",
        "        url = (\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "               f\"?begin_date={_datestr(a)}&end_date={_datestr(b)}\"\n",
        "               f\"&station={station}&product=water_level&datum=MLLW&time_zone=gmt&units=metric\"\n",
        "               f\"&interval=6&application=SeichePatchTST&format=csv\")\n",
        "        try:\n",
        "            r = session.get(url, timeout=90)\n",
        "            if r.status_code==200 and r.content:\n",
        "                dfc = parse_noaa_wl_csv(r.content.decode(\"utf-8\",\"ignore\"))\n",
        "                if not dfc.empty: frames.append(dfc)\n",
        "        except: pass\n",
        "    if not frames: return pd.DataFrame(columns=[\"time\",\"wl\"])\n",
        "    return pd.concat(frames, ignore_index=True).sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "def bandpass_then_var(series:pd.DataFrame, val_col:str, dt_minutes:int,\n",
        "                      band_min_min:float, band_max_min:float, var_win_h:float)->pd.DataFrame:\n",
        "    if series.empty: return pd.DataFrame(columns=[\"time\",\"var\"])\n",
        "    t0,t1 = series[\"time\"].min(), series[\"time\"].max()\n",
        "    grid = pd.date_range(t0.floor(f\"{dt_minutes}min\"), t1.ceil(f\"{dt_minutes}min\"), freq=f\"{dt_minutes}min\")\n",
        "    s = series.set_index(\"time\")[val_col].reindex(grid)\n",
        "    s = s.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    dt_sec = dt_minutes*60.0; fs=1.0/dt_sec\n",
        "    f_lo = 1.0/(band_max_min*60.0)\n",
        "    f_hi = 1.0/(band_min_min*60.0)\n",
        "    wn = [max(1e-6, f_lo/(fs/2.0)), min(0.999, f_hi/(fs/2.0))]\n",
        "    b,a = butter(6, wn, btype=\"bandpass\")\n",
        "    x = s.to_numpy(dtype=float)\n",
        "    if np.isnan(x).any():\n",
        "        idx = np.arange(len(x)); good = np.isfinite(x); x[~good] = np.interp(idx[~good], idx[good], x[good])\n",
        "    x_bp = filtfilt(b,a,x,method=\"gust\")\n",
        "    win = max(3, int(round((var_win_h*60.0)/dt_minutes))); minp = max(2, int(0.8*win))\n",
        "    var_series = pd.Series(x_bp, index=grid).rolling(window=win, center=True, min_periods=minp).var()\n",
        "    return pd.DataFrame({\"time\":grid, \"var\":var_series.astype(\"float32\")}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "def highpass_then_var(series:pd.DataFrame, val_col:str, dt_minutes:int,\n",
        "                      hp_cutoff_h:float, var_win_h:float)->pd.DataFrame:\n",
        "    \"\"\"1st: high-pass (remove periods longer than hp_cutoff_h), then centered rolling variance.\"\"\"\n",
        "    if series.empty: return pd.DataFrame(columns=[\"time\",\"var\"])\n",
        "    t0,t1 = series[\"time\"].min(), series[\"time\"].max()\n",
        "    grid = pd.date_range(t0.floor(f\"{dt_minutes}min\"), t1.ceil(f\"{dt_minutes}min\"), freq=f\"{dt_minutes}min\")\n",
        "    s = series.set_index(\"time\")[val_col].reindex(grid)\n",
        "    s = s.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    dt_sec = dt_minutes*60.0; fs=1.0/dt_sec\n",
        "    f_c = 1.0/(hp_cutoff_h*3600.0)  # Hz\n",
        "    wn = max(1e-6, min(0.999, f_c/(fs/2.0)))\n",
        "    b,a = butter(4, wn, btype=\"highpass\")   # 4th order is plenty here\n",
        "    x = s.to_numpy(dtype=float)\n",
        "    if np.isnan(x).any():\n",
        "        idx = np.arange(len(x)); good = np.isfinite(x); x[~good] = np.interp(idx[~good], idx[good], x[good])\n",
        "    x_hp = filtfilt(b,a,x,method=\"gust\")\n",
        "    win = max(3, int(round((var_win_h*60.0)/dt_minutes))); minp = max(2, int(0.8*win))\n",
        "    var_series = pd.Series(x_hp, index=grid).rolling(window=win, center=True, min_periods=minp).var()\n",
        "    return pd.DataFrame({\"time\":grid, \"var\":var_series.astype(\"float32\")}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "# --------------------------------\n",
        "# Build all sources\n",
        "# --------------------------------\n",
        "df_wl6   = fetch_noaa_wl(WL_BEGIN_DATE, WL_END_DATE, NOAA_WL_STATION, days_per_chunk=31)\n",
        "df_varWL = bandpass_then_var(df_wl6, \"wl\", DT_MINUTES, WL_BAND_MIN_MIN, WL_BAND_MAX_MIN, VAR_WIN_HOURS)\n",
        "\n",
        "df_46050 = df_stdmet.copy()\n",
        "df_swd   = df_swden.copy()\n",
        "\n",
        "# Wind: choose coverage window from wind if present, else WL\n",
        "if len(df_noaa_wind):\n",
        "    fill_start, fill_end = df_noaa_wind[\"time\"].min(), df_noaa_wind[\"time\"].max()\n",
        "else:\n",
        "    candidates=[]\n",
        "    if len(df_swd): candidates.append((df_swd[\"time\"].min(), df_swd[\"time\"].max()))\n",
        "    if len(df_46050): candidates.append((df_46050[\"time\"].min(), df_46050[\"time\"].max()))\n",
        "    if len(df_wl6): candidates.append((df_wl6[\"time\"].min(), df_wl6[\"time\"].max()))\n",
        "    if candidates:\n",
        "        fill_start = min(a for a,_ in candidates); fill_end = max(b for _,b in candidates)\n",
        "    else:\n",
        "        fill_start = pd.Timestamp(\"2008-01-01\"); fill_end=pd.Timestamp(\"2025-10-01\")\n",
        "\n",
        "# HMSC wind/dir/baro hourlies\n",
        "df_hmsc = fetch_hmsc_range(fill_start, fill_end)\n",
        "\n",
        "# Build unified hourly grid for wind fill\n",
        "hourly_grid = pd.date_range(fill_start.floor(\"h\"), fill_end.ceil(\"h\"), freq=\"h\")\n",
        "noaa = (df_noaa_wind.set_index(\"time\").reindex(hourly_grid).rename_axis(\"time\").reset_index()) if len(df_noaa_wind) else pd.DataFrame({\"time\":hourly_grid})\n",
        "noaa.rename(columns={\"wind_speed\":\"speed\",\"wind_dir\":\"dir\"}, inplace=True)\n",
        "\n",
        "if len(df_hmsc):\n",
        "    hmsc = (df_hmsc.set_index(\"time\").reindex(hourly_grid).rename_axis(\"time\").reset_index())\n",
        "else:\n",
        "    hmsc = pd.DataFrame({\"time\":hourly_grid, \"hmsc_speed_ms\":np.nan,\"hmsc_dir_deg\":np.nan,\"hmsc_baro_hpa\":np.nan})\n",
        "\n",
        "# fill wind speed/dir gaps from HMSC if both present\n",
        "gap_mask = noaa[[\"speed\",\"dir\"]].isna().any(axis=1) if set([\"speed\",\"dir\"]).issubset(noaa.columns) else pd.Series(True, index=noaa.index)\n",
        "fillable = gap_mask & hmsc[[\"hmsc_speed_ms\",\"hmsc_dir_deg\"]].notna().all(axis=1)\n",
        "noaa.loc[fillable, \"speed\"] = hmsc.loc[fillable, \"hmsc_speed_ms\"].values\n",
        "noaa.loc[fillable, \"dir\"]   = hmsc.loc[fillable, \"hmsc_dir_deg\"].values\n",
        "\n",
        "df_wind = pd.DataFrame({\n",
        "    \"time\": noaa[\"time\"],\n",
        "    \"wind9435380_speed\": noaa.get(\"speed\", pd.Series(index=noaa.index, dtype=float)).astype(\"float32\"),\n",
        "    \"wind9435380_dir\":   noaa.get(\"dir\",   pd.Series(index=noaa.index, dtype=float)).astype(\"float32\"),\n",
        "}).dropna(subset=[\"time\"]).reset_index(drop=True)\n",
        "\n",
        "# Baro (hourly) from HMSC if present → highpass + variance\n",
        "if \"hmsc_baro_hpa\" in hmsc.columns and hmsc[\"hmsc_baro_hpa\"].notna().any():\n",
        "    df_baro = hmsc[[\"time\",\"hmsc_baro_hpa\"]].copy()\n",
        "    df_baro = df_baro.dropna(subset=[\"time\"]).drop_duplicates(\"time\").sort_values(\"time\").reset_index(drop=True)\n",
        "    df_baro_var = highpass_then_var(df_baro, \"hmsc_baro_hpa\", 60, BARO_HP_CUTOFF_H, BARO_VAR_WIN_H)\n",
        "else:\n",
        "    df_baro = pd.DataFrame(columns=[\"time\",\"hmsc_baro_hpa\"])\n",
        "    df_baro_var = pd.DataFrame(columns=[\"time\",\"var\"])\n",
        "\n",
        "# --------------------------------\n",
        "# Build common DT grid + assemble inputs\n",
        "# --------------------------------\n",
        "def interp_timegrid(t_src:pd.Series, x_src:np.ndarray, t_grid:pd.DatetimeIndex)->np.ndarray:\n",
        "    s = pd.Series(x_src, index=pd.to_datetime(t_src))\n",
        "    u = s.reindex(pd.to_datetime(sorted(set(s.index).union(set(t_grid)))))\n",
        "    u = u.interpolate(method=\"time\", limit_area=\"inside\")\n",
        "    return u.reindex(pd.to_datetime(t_grid)).to_numpy()\n",
        "\n",
        "# overlap\n",
        "t0_candidates = [df_varWL[\"time\"].min()] if len(df_varWL) else []\n",
        "t1_candidates = [df_varWL[\"time\"].max()] if len(df_varWL) else []\n",
        "for d, col in [(df_swd,\"time\"), (df_46050,\"time\"), (df_wind,\"time\"), (df_baro,\"time\"), (df_baro_var,\"time\")]:\n",
        "    if len(d):\n",
        "        t0_candidates.append(d[col].min()); t1_candidates.append(d[col].max())\n",
        "t0 = max(t0_candidates); t1 = min(t1_candidates)\n",
        "t_grid = pd.date_range(t0, t1, freq=f\"{DT_MINUTES}min\")\n",
        "\n",
        "# target\n",
        "var_on_grid = interp_timegrid(df_varWL[\"time\"], df_varWL[\"var\"].values, t_grid)\n",
        "\n",
        "# inputs\n",
        "seq_mat = {}\n",
        "if len(df_swd):\n",
        "    seq_mat[\"high_HIG\"] = interp_timegrid(df_swd[\"time\"], df_swd[\"high_HIG\"].values, t_grid)\n",
        "    seq_mat[\"Hswell\"]   = interp_timegrid(df_swd[\"time\"], df_swd[\"Hswell\"].values, t_grid)\n",
        "    seq_mat[\"Hsea\"]     = interp_timegrid(df_swd[\"time\"], df_swd[\"Hsea\"].values, t_grid)\n",
        "\n",
        "for c in [\"DPD\",\"APD\",\"MWD\"]:  # EXCLUDE Hs\n",
        "    if len(df_46050):\n",
        "        seq_mat[c] = interp_timegrid(df_46050[\"time\"], df_46050[c].values, t_grid)\n",
        "\n",
        "if len(df_wind):\n",
        "    seq_mat[\"wind9435380_speed\"] = interp_timegrid(df_wind[\"time\"], df_wind[\"wind9435380_speed\"].values, t_grid)\n",
        "    seq_mat[\"wind9435380_dir\"]   = interp_timegrid(df_wind[\"time\"], df_wind[\"wind9435380_dir\"].values,   t_grid)\n",
        "\n",
        "if len(df_baro):\n",
        "    seq_mat[\"hmsc_baro_hpa\"] = interp_timegrid(df_baro[\"time\"], df_baro[\"hmsc_baro_hpa\"].values, t_grid)\n",
        "if len(df_baro_var):\n",
        "    seq_mat[\"baro_var4h\"] = interp_timegrid(df_baro_var[\"time\"], df_baro_var[\"var\"].values, t_grid)\n",
        "else:\n",
        "    seq_mat[\"baro_var4h\"] = np.full(len(t_grid), np.nan, dtype=float)\n",
        "\n",
        "# EQ_decay\n",
        "def build_eq_decay_series(t_grid: pd.DatetimeIndex, eq_events: list[tuple[str|pd.Timestamp, float]]) -> np.ndarray:\n",
        "    if not eq_events or len(t_grid)==0:\n",
        "        return np.zeros(len(t_grid), dtype=float)\n",
        "    out = np.zeros(len(t_grid), dtype=float)\n",
        "    for t0_evt, M in eq_events:\n",
        "        t0_evt = pd.Timestamp(t0_evt)\n",
        "        amp = float(EQ_AMPLITUDE_FUNC(float(M)))\n",
        "        tau_h = float(EQ_DECAY_TAU_FUNC(float(M))); tau_s = max(1.0, tau_h*3600.0)\n",
        "        dt = (t_grid - t0_evt).total_seconds().astype(float)\n",
        "        m = dt >= 0.0\n",
        "        out[m] += amp * np.exp(-dt[m] / tau_s)\n",
        "    return out\n",
        "\n",
        "seq_mat[\"EQ_decay\"] = build_eq_decay_series(t_grid, EQ_EVENTS)\n",
        "\n",
        "# Unified gap mask (use the major channels)\n",
        "def gap_intervals(t:pd.Series, x:np.ndarray, min_gap_hours:float)->List[Tuple[pd.Timestamp,pd.Timestamp]]:\n",
        "    t = pd.to_datetime(t)\n",
        "    if len(t) < 2: return []\n",
        "    dt = np.diff(t.values).astype(\"timedelta64[s]\").astype(int)\n",
        "    jumps = np.where(dt > min_gap_hours*3600)[0]\n",
        "    iv = [(t[j], t[j+1]) for j in jumps]\n",
        "    s = pd.Series(x, index=t)\n",
        "    isn = s.isna().to_numpy()\n",
        "    if isn.any():\n",
        "        starts = np.where(np.diff(np.r_[False, isn])==1)[0]\n",
        "        ends   = np.where(np.diff(np.r_[isn, False])==-1)[0]\n",
        "        for a,b in zip(starts, ends):\n",
        "            if (t[b]-t[a]) >= pd.Timedelta(hours=min_gap_hours):\n",
        "                iv.append((t[a], t[b]))\n",
        "    return iv\n",
        "\n",
        "def merge_intervals(intervals):\n",
        "    if not intervals: return []\n",
        "    z = sorted(intervals, key=lambda k: k[0]); out = [list(z[0])]\n",
        "    for s,e in z[1:]:\n",
        "        if s <= out[-1][1]: out[-1][1] = max(out[-1][1], e)\n",
        "        else: out.append([s,e])\n",
        "    return [(pd.to_datetime(a), pd.to_datetime(b)) for a,b in out]\n",
        "\n",
        "all_intervals=[]\n",
        "if len(df_swd):\n",
        "    for c in [\"high_HIG\",\"Hswell\",\"Hsea\"]:\n",
        "        all_intervals += gap_intervals(df_swd[\"time\"], df_swd[c].values, MAX_GAP_HOURS)\n",
        "if len(df_46050):\n",
        "    for c in [\"DPD\",\"APD\",\"MWD\"]:\n",
        "        all_intervals += gap_intervals(df_46050[\"time\"], df_46050[c].values, MAX_GAP_HOURS)\n",
        "if len(df_wind):\n",
        "    for c in [\"wind9435380_speed\",\"wind9435380_dir\"]:\n",
        "        all_intervals += gap_intervals(df_wind[\"time\"], df_wind[c].values, MAX_GAP_HOURS)\n",
        "if len(df_baro):\n",
        "    all_intervals += gap_intervals(df_baro[\"time\"], df_baro[\"hmsc_baro_hpa\"].values, MAX_GAP_HOURS)\n",
        "\n",
        "merged_gaps = merge_intervals(all_intervals)\n",
        "gap_mask = np.zeros(len(t_grid), dtype=bool)\n",
        "for s,e in merged_gaps:\n",
        "    gap_mask |= (t_grid>=s) & (t_grid<=e)\n",
        "\n",
        "# Assemble df_all (target + inputs)\n",
        "df_all = pd.DataFrame({\"time\": t_grid, \"var1h\": var_on_grid})\n",
        "for c,v in seq_mat.items():\n",
        "    df_all[c] = v\n",
        "df_all = df_all.loc[~gap_mask].reset_index(drop=True)\n",
        "\n",
        "# Post-align cleaning for stdmet heads\n",
        "for col, thresh in [(\"DPD\",50.0),(\"APD\",50.0)]:\n",
        "    if col in df_all.columns:\n",
        "        m = df_all[col] > thresh\n",
        "        if m.any(): df_all.loc[m, col] = np.nan\n",
        "        df_all[col] = pd.Series(df_all[col].values, index=df_all[\"time\"]).interpolate(\"time\", limit_area=\"inside\").values\n",
        "\n",
        "# --------------------------------\n",
        "# Lag features + XGB pick (force baro)\n",
        "# --------------------------------\n",
        "NON_EQ_MAX_LAG_MIN  = 4 * 60\n",
        "EQ_MAX_LAG_MIN      = 24 * 60\n",
        "\n",
        "def is_eq_base(name: str) -> bool:\n",
        "    n = name.lower()\n",
        "    return n.startswith(\"eq\") or (\"eq_\" in n)\n",
        "\n",
        "def build_lag_candidates(df: pd.DataFrame, bases: List[str], dt_minutes: int,\n",
        "                         step_minutes: int = 30) -> Tuple[pd.DataFrame, List[str], Dict[str, str], Dict[str, int]]:\n",
        "    lag_cols, series = [], {}\n",
        "    base_map, lag_minutes_map = {}, {}\n",
        "    for b in bases:\n",
        "        x = df[b].astype(\"float32\").to_numpy()\n",
        "        if not np.isfinite(x).any(): continue\n",
        "        max_lag_min = EQ_MAX_LAG_MIN if is_eq_base(b) else NON_EQ_MAX_LAG_MIN\n",
        "        grid_min = list(range(0, max_lag_min + 1, int(step_minutes)))\n",
        "        for L in grid_min:\n",
        "            steps = int(round(L / dt_minutes))\n",
        "            nm = f\"{b}__lag_{L}min\"\n",
        "            lag_minutes_map[nm] = L; base_map[nm] = b\n",
        "            if steps == 0:\n",
        "                series[nm] = x\n",
        "            else:\n",
        "                s = pd.Series(x); series[nm] = s.shift(steps).to_numpy(dtype=\"float32\")\n",
        "            lag_cols.append(nm)\n",
        "    lag_df = pd.DataFrame(series).reindex(range(len(df))).reset_index(drop=True)\n",
        "    return lag_df, lag_cols, base_map, lag_minutes_map\n",
        "\n",
        "# Exclude Hs explicitly; include everything else numeric\n",
        "exclude_cols = set([\"time\",\"var1h\",\"Hs\"])  # drop Hs — redundant with spectral bands\n",
        "cand_bases = [c for c in df_all.columns if (c not in exclude_cols and df_all[c].dtype != object)]\n",
        "\n",
        "lag_df, lag_cols, base_map, lag_minutes_map = build_lag_candidates(df_all, cand_bases, DT_MINUTES)\n",
        "\n",
        "# XGB rank one best per base\n",
        "from xgboost import XGBRegressor\n",
        "h_steps = max(1, int(round(PRIMARY_HORIZON / DT_MINUTES)))\n",
        "tgt = df_all[\"var1h\"].shift(-h_steps).reset_index(drop=True).astype(\"float32\")\n",
        "rank_data = pd.concat([lag_df, tgt.rename(\"target\")], axis=1).dropna().reset_index(drop=True)\n",
        "if rank_data.empty: raise RuntimeError(\"No data for XGB ranking; check coverage.\")\n",
        "\n",
        "X_rank = rank_data[lag_cols].to_numpy(dtype=np.float32)\n",
        "y_rank = rank_data[\"target\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "# existing code\n",
        "X_rank = rank_data[lag_cols].to_numpy(dtype=np.float32)\n",
        "y_rank = rank_data[\"target\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "# === start Optuna integration ===\n",
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# split ranking data (80/20) for tuning\n",
        "split_idx = max(1, int(0.8 * len(X_rank)))\n",
        "X_train, X_val = X_rank[:split_idx], X_rank[split_idx:]\n",
        "y_train, y_val = y_rank[:split_idx], y_rank[split_idx:]\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10.0),\n",
        "    }\n",
        "    tree_method = \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
        "    model = XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        tree_method=tree_method,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        **params\n",
        "    )\n",
        "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
        "              eval_metric=\"rmse\", verbose=False, early_stopping_rounds=50)\n",
        "    preds = model.predict(X_val)\n",
        "    return mean_squared_error(y_val, preds)\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "best_params = study.best_params\n",
        "best_params.update({\"objective\": \"reg:squarederror\",\n",
        "                    \"tree_method\": (\"gpu_hist\" if torch.cuda.is_available() else \"hist\"),\n",
        "                    \"random_state\": 42,\n",
        "                    \"n_jobs\": -1})\n",
        "xgb_opt = XGBRegressor(**best_params)\n",
        "xgb_opt.fit(X_rank, y_rank, verbose=False)\n",
        "\n",
        "importances = dict(zip(lag_cols, xgb_opt.feature_importances_))\n",
        "\n",
        "best_lag_per_base = {}\n",
        "for nm, imp in importances.items():\n",
        "    b = base_map[nm]\n",
        "    prev = best_lag_per_base.get(b)\n",
        "    if (prev is None) or (imp > prev[1]): best_lag_per_base[b] = (nm, float(imp))\n",
        "best_list = [(b, nm, imp) for b,(nm,imp) in best_lag_per_base.items()]\n",
        "best_list.sort(key=lambda t: t[2], reverse=True)\n",
        "best_list = best_list[:TOP_LAG_FEATURES]\n",
        "\n",
        "selected_lag_cols = [nm for _, nm, _ in best_list]\n",
        "selected_bases    = [b  for b, _, _ in best_list]\n",
        "\n",
        "# Force include baro features (and EQ if you want to guarantee)\n",
        "def _prefer_lags(base_name:str, pref_lags:list[int])->str|None:\n",
        "    for L in pref_lags:\n",
        "        nm = f\"{base_name}__lag_{L}min\"\n",
        "        if nm in lag_cols: return nm\n",
        "    cands = [nm for nm in lag_cols if base_map.get(nm)==base_name]\n",
        "    if not cands: return None\n",
        "    return max(cands, key=lambda z: importances.get(z, 0.0))\n",
        "\n",
        "for base in [\"baro_var4h\", \"hmsc_baro_hpa\", \"EQ_decay\"]:\n",
        "    nm = _prefer_lags(base, BARO_FORCE_LAGS_MIN if base!=\"EQ_decay\" else [0,240])\n",
        "    if nm and nm not in selected_lag_cols:\n",
        "        selected_lag_cols.append(nm); selected_bases.append(base)\n",
        "\n",
        "print(\"\\nSelected ONE best lag per base (forced baro/EQ included if available):\")\n",
        "for nm in selected_lag_cols:\n",
        "    b = base_map[nm]\n",
        "    imp = importances.get(nm, 0.0)\n",
        "    print(f\"  {nm:<32s} base={b:<16s} imp={imp:.6f} lag={lag_minutes_map[nm]}\")\n",
        "\n",
        "# Assemble minimal lag table\n",
        "df_lag_all = pd.concat(\n",
        "    [df_all[[\"time\", \"var1h\"]].reset_index(drop=True),\n",
        "     lag_df[selected_lag_cols].reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# --------------------------------\n",
        "# PatchTST model & training\n",
        "# --------------------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "def _amp_ctx(): return nullcontext()\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_features:int, patch_len:int, stride:int, d_model:int):\n",
        "        super().__init__()\n",
        "        self.patch_len = patch_len\n",
        "        self.stride    = stride\n",
        "        self.proj      = nn.Linear(in_features*patch_len, d_model)\n",
        "    def forward(self, x):\n",
        "        B,T,F = x.shape\n",
        "        N = 1 + (T - self.patch_len)//self.stride\n",
        "        patches = []\n",
        "        for i in range(N):\n",
        "            s = i*self.stride; e = s + self.patch_len\n",
        "            seg = x[:, s:e, :].reshape(B, -1)\n",
        "            patches.append(seg)\n",
        "        P = torch.stack(patches, dim=1)  # (B,N,F*patch_len)\n",
        "        return self.proj(P)              # (B,N,d)\n",
        "\n",
        "class PatchTST(nn.Module):\n",
        "    def __init__(self, in_features, patch_len, stride, d_model=128, nhead=4, nlayers=3, ff_dim=256, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = PatchEmbedding(in_features, patch_len, stride, d_model)\n",
        "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff_dim,\n",
        "                                           dropout=drop, batch_first=True, norm_first=True)\n",
        "        self.enc  = nn.TransformerEncoder(layer, num_layers=nlayers)\n",
        "        self.head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.ReLU(), nn.Linear(d_model//2, 1))\n",
        "    def forward(self, x):\n",
        "        z = self.embed(x); z = self.enc(z)\n",
        "        return self.head(z[:, -1, :]).squeeze(-1)\n",
        "\n",
        "def rmse(a,b): return float(np.sqrt(np.mean((np.asarray(a)-np.asarray(b))**2)))\n",
        "\n",
        "# scaling helpers\n",
        "def _safe_robust_params(X: np.ndarray, q_low=25.0, q_high=75.0, eps=1e-6):\n",
        "    Xf = X.reshape(-1, X.shape[2])\n",
        "    med = np.nanmedian(Xf, axis=0)\n",
        "    q1  = np.nanpercentile(Xf, q_low, axis=0)\n",
        "    q3  = np.nanpercentile(Xf, q_high, axis=0)\n",
        "    iqr = q3 - q1\n",
        "    scale = np.where(iqr <= eps, 1.0, iqr)\n",
        "    return med.astype(np.float32), scale.astype(np.float32)\n",
        "\n",
        "def fit_scaler_from_windows_safe(X_tr, X_va=None, X_te=None):\n",
        "    for X in (X_tr, X_va, X_te):\n",
        "        if X is not None and len(X) > 0:\n",
        "            med, scale = _safe_robust_params(X)\n",
        "            return {\"center\": med, \"scale\": scale}\n",
        "    F = X_tr.shape[2] if (X_tr is not None and X_tr.size) else (\n",
        "        X_va.shape[2] if (X_va is not None and X_va.size) else (\n",
        "        X_te.shape[2] if (X_te is not None and X_te.size) else 1))\n",
        "    return {\"center\": np.zeros(F, np.float32), \"scale\": np.ones(F, np.float32)}\n",
        "\n",
        "def norm_windows(X, rs, clip=20.0):\n",
        "    if len(X) == 0: return X\n",
        "    F = X.shape[2]\n",
        "    C = rs[\"center\"].reshape(1, 1, F)\n",
        "    S = rs[\"scale\" ].reshape(1, 1, F)\n",
        "    Z = (X - C) / S\n",
        "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
        "    if clip is not None: Z = np.clip(Z, -clip, clip, out=Z)\n",
        "    return Z\n",
        "\n",
        "def make_windows_from_lagged(df:pd.DataFrame, lag_cols:List[str],\n",
        "                             history_hours:int, horizon_min:int, dt_minutes:int,\n",
        "                             stride:int=1):\n",
        "    steps_hist  = int(round(history_hours*60/dt_minutes))\n",
        "    steps_ahead = int(round(horizon_min/dt_minutes))\n",
        "    tab = df[[\"time\",\"var1h\"] + lag_cols].copy()\n",
        "    tab = tab.dropna(subset=lag_cols + [\"var1h\"]).reset_index(drop=True)\n",
        "    if len(tab) < steps_hist + steps_ahead + 1:\n",
        "        return np.empty((0,steps_hist,len(lag_cols))), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    X_list, y_list, t_list = [], [], []\n",
        "    F = len(lag_cols); Ttot = len(tab)\n",
        "    for end_idx in range(steps_hist, Ttot - steps_ahead + 1, stride):\n",
        "        s = end_idx - steps_hist\n",
        "        k = end_idx - 1 + steps_ahead\n",
        "        xb = tab.iloc[s:end_idx][lag_cols].values\n",
        "        yv = tab.iloc[k][\"var1h\"]\n",
        "        if np.any(~np.isfinite(xb)) or not np.isfinite(yv): continue\n",
        "        X_list.append(xb.astype(np.float32))\n",
        "        y_list.append(float(yv))\n",
        "        t_list.append(pd.to_datetime(tab.iloc[k][\"time\"]))\n",
        "    if not X_list:\n",
        "        return np.empty((0,steps_hist,F)), np.empty((0,)), pd.DatetimeIndex([])\n",
        "    return np.stack(X_list), np.array(y_list, dtype=np.float32), pd.DatetimeIndex(t_list)\n",
        "\n",
        "def adaptive_index_split(X, y, t_idx, train_frac=0.70, val_frac=0.15):\n",
        "    n = len(X)\n",
        "    if n < 10:\n",
        "        return (X[:0], y[:0], t_idx[:0]), (X[:0], y[:0], t_idx[:0]), (X, y, t_idx)\n",
        "    a = max(1, int(round(train_frac * n)))\n",
        "    b = max(a+1, int(round((train_frac + val_frac) * n)))\n",
        "    b = min(b, n-1)\n",
        "    return (X[:a], y[:a], t_idx[:a]), (X[a:b], y[a:b], t_idx[a:b]), (X[b:], y[b:], t_idx[b:])\n",
        "\n",
        "def cap_split(X_, y_, t_, cap=MAX_SAMPLES_SPLIT):\n",
        "    if len(X_) <= cap: return X_, y_, t_\n",
        "    idx = np.linspace(0, len(X_) - 1, cap, dtype=int)\n",
        "    return X_[idx], y_[idx], t_[idx]\n",
        "\n",
        "def _fmt(ts):\n",
        "    return ts.strftime(\"%Y-%m-%d %H:%M\") if (ts is not None and pd.notna(ts)) else \"—\"\n",
        "\n",
        "def print_split_summary(horizon, t_tr, t_va, t_te):\n",
        "    def rng(t):\n",
        "        if len(t)==0: return (None,None,0)\n",
        "        return (t.min(), t.max(), len(t))\n",
        "    tr0,tr1,ntr = rng(t_tr); va0,va1,nva = rng(t_va); te0,te1,nte = rng(t_te)\n",
        "    print(f\"[{horizon}m] Split ranges:\")\n",
        "    print(f\"  Train: {_fmt(tr0)} → {_fmt(tr1)}  (n={ntr})\")\n",
        "    print(f\"  Valid: {_fmt(va0)} → {_fmt(va1)}  (n={nva})\")\n",
        "    print(f\"  Test : {_fmt(te0)} → {_fmt(te1)}  (n={nte})\")\n",
        "    return (tr0,tr1),(va0,va1),(te0,te1)\n",
        "\n",
        "# --------------------------------\n",
        "# Train PatchTST across horizons\n",
        "# --------------------------------\n",
        "results = {}\n",
        "for horizon in HORIZONS_MIN:\n",
        "    print(f\"\\n=== Horizon {horizon} min ===\")\n",
        "    X, y, t_idx = make_windows_from_lagged(df_lag_all, selected_lag_cols,\n",
        "                                           HISTORY_HOURS, horizon, DT_MINUTES, stride=1)\n",
        "\n",
        "    if len(t_idx):\n",
        "        print(f\"[{horizon}m] Windows available:\", t_idx.min(), \"→\", t_idx.max(), f\"(n={len(t_idx):,})\")\n",
        "    else:\n",
        "        print(f\"[{horizon}m] No windows after lagging/masking.\"); continue\n",
        "\n",
        "    mtr = t_idx <= TRAIN_END\n",
        "    mva = (t_idx > TRAIN_END) & (t_idx <= VAL_END)\n",
        "    mte = t_idx > VAL_END\n",
        "\n",
        "    X_tr, y_tr, t_tr = X[mtr], y[mtr], t_idx[mtr]\n",
        "    X_va, y_va, t_va = X[mva], y[mva], t_idx[mva]\n",
        "    X_te, y_te, t_te = X[mte], y[mte], t_idx[mte]\n",
        "\n",
        "    if len(X_tr)==0 or len(X_va)==0 or len(X_te)==0:\n",
        "        print(\"  Time split empty; using adaptive 70/15/15.\")\n",
        "        (X_tr, y_tr, t_tr), (X_va, y_va, t_va), (X_te, y_te, t_te) = adaptive_index_split(X, y, t_idx)\n",
        "    if len(X_te)==0:\n",
        "        n=len(X); cut=max(1,int(0.85*n))\n",
        "        X_tr,y_tr,t_tr = X[:cut],y[:cut],t_idx[:cut]\n",
        "        X_va,y_va,t_va = X[:0],y[:0],t_idx[:0]\n",
        "        X_te,y_te,t_te = X[cut:],y[cut:],t_idx[cut:]\n",
        "\n",
        "    _ = print_split_summary(horizon, t_tr, t_va, t_te)\n",
        "\n",
        "    # scale\n",
        "    rs     = fit_scaler_from_windows_safe(X_tr, X_va, X_te)\n",
        "    X_tr_n = norm_windows(X_tr, rs); X_va_n = norm_windows(X_va, rs); X_te_n = norm_windows(X_te, rs)\n",
        "\n",
        "    # model\n",
        "    device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "    model = PatchTST(\n",
        "        in_features=X_tr_n.shape[2], patch_len=PATCH_LEN, stride=PATCH_STRIDE,\n",
        "        d_model=D_MODEL, nhead=NHEAD, nlayers=NLAYERS, ff_dim=FF_DIM, drop=DROPOUT\n",
        "    ).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
        "\n",
        "    ds_tr = TensorDataset(torch.from_numpy(X_tr_n), torch.from_numpy(y_tr))\n",
        "    ds_va = TensorDataset(torch.from_numpy(X_va_n), torch.from_numpy(y_va))\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=min(BATCH_SIZE, len(ds_tr)), shuffle=True, drop_last=False)\n",
        "    dl_va = DataLoader(ds_va, batch_size=min(BATCH_SIZE, len(ds_va)), shuffle=False, drop_last=False)\n",
        "\n",
        "    best = np.inf; best_state=None; patience=3; bad=0\n",
        "    q = np.quantile(y_tr, SPIKE_P90) if (WEIGHT_SPIKES and len(y_tr)>0) else None\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        model.train(); tr_loss=0.0\n",
        "        for xb,yb in dl_tr:\n",
        "            xb,yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with _amp_ctx():\n",
        "                pred = model(xb)\n",
        "                l = (pred - yb) ** 2\n",
        "                if q is not None:\n",
        "                    w = torch.where(yb >= torch.tensor(q, device=yb.device, dtype=yb.dtype),\n",
        "                                    torch.tensor(SPIKE_WEIGHT, device=yb.device, dtype=yb.dtype),\n",
        "                                    torch.tensor(1.0,         device=yb.device, dtype=yb.dtype))\n",
        "                    l = l * w\n",
        "                loss = l.mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            opt.step()\n",
        "            tr_loss += loss.item() * len(xb)\n",
        "        tr_loss /= max(1, len(dl_tr.dataset))\n",
        "\n",
        "        model.eval(); va_loss=0.0\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for xb,yb in dl_va:\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb)\n",
        "                va_loss += nn.functional.mse_loss(out, yb, reduction=\"sum\").item()\n",
        "        va_loss /= max(1, len(dl_va.dataset))\n",
        "        print(f\"[PatchTST] epoch {ep+1:02d}  trainMSE={tr_loss:.5f}  valMSE={va_loss:.5f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best:\n",
        "            best = va_loss; best_state = model.state_dict(); bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                print(\"[PatchTST] early stop.\"); break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # predict\n",
        "    def predict_patchtst(model, X):\n",
        "        ds = TensorDataset(torch.from_numpy(X))\n",
        "        dl = DataLoader(ds, batch_size=min(BATCH_SIZE, len(ds)), shuffle=False, drop_last=False)\n",
        "        out=[]; model.eval()\n",
        "        with torch.no_grad(), _amp_ctx():\n",
        "            for (xb,) in dl:\n",
        "                xb = xb.to(device)\n",
        "                out.append(model(xb).cpu().numpy())\n",
        "        return np.concatenate(out, axis=0) if len(out)>0 else np.array([])\n",
        "\n",
        "    yp_va = predict_patchtst(model, X_va_n) if len(X_va_n)>0 else np.array([])\n",
        "    yp_te = predict_patchtst(model, X_te_n) if len(X_te_n)>0 else np.array([])\n",
        "\n",
        "    def _rmse(a,b):\n",
        "        return float(np.sqrt(np.mean((np.asarray(a)-np.asarray(b))**2))) if len(a)==len(b) and len(a)>0 else float(\"nan\")\n",
        "\n",
        "    R2 = r2_score(y_te, yp_te) if len(yp_te)>0 and len(y_te)>0 else float(\"nan\")\n",
        "    RMSE = _rmse(y_te, yp_te)\n",
        "    results[horizon] = dict(t_val=t_va, y_val=y_va, yp_val=yp_va,\n",
        "                            t=t_te, y=y_te, yp=yp_te, r2=R2, rmse=RMSE)\n",
        "    print(f\"[PatchTST] {horizon}m  Test R²={R2:.3f}  RMSE={RMSE:.3f}\")\n",
        "\n",
        "    # plots\n",
        "    if PRED_SPLIT.lower() == \"val\":\n",
        "        t_plot, y_plot, yhat_plot = t_va, y_va, yp_va; split_name=\"Validation\"\n",
        "    else:\n",
        "        t_plot, y_plot, yhat_plot = t_te, y_te, yp_te; split_name=\"Test\"\n",
        "    if len(t_plot)==0: continue\n",
        "\n",
        "    mwin = (t_plot >= PRED_START) & (t_plot <= PRED_END)\n",
        "    if not np.any(mwin):\n",
        "        end = t_plot[-1]; start = end - pd.Timedelta(days=PLOT_LAST_DAYS)\n",
        "        mwin = (t_plot >= start) & (t_plot <= end)\n",
        "\n",
        "    if np.any(mwin):\n",
        "        ts, yt, ypp = t_plot[mwin], y_plot[mwin], yhat_plot[mwin]\n",
        "        plt.figure(figsize=(14,4))\n",
        "        plt.plot(ts, yt, label=\"Measured\", lw=1.2)\n",
        "        plt.plot(ts, ypp, \"--\", label=f\"Predicted ({split_name})\", lw=1.2)\n",
        "        plt.title(f\"PatchTST — {horizon} min ({split_name})\")\n",
        "        plt.ylabel(\"var1h\"); plt.xlabel(\"Time\"); plt.grid(alpha=0.3); plt.legend()\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"\\nFinal selected lag channels (one per base):\")\n",
        "for nm in selected_lag_cols:\n",
        "    print(\"  \", nm)\n"
      ]
    }
  ]
}